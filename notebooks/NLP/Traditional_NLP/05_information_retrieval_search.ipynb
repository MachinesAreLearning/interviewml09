{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval & Search - Traditional NLP\n",
    "## Interview Preparation Notebook for Senior Applied AI Scientist (Retail Banking)\n",
    "\n",
    "---\n",
    "\n",
    "**Goal**: Demonstrate mastery of traditional search and retrieval techniques (TF-IDF, BM25) that form the foundation for modern RAG systems.\n",
    "\n",
    "**Interview Signal**: This notebook shows you understand the fundamentals that underpin vector search and RAG - critical for anyone building AI systems that need to retrieve relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Context (Banking Lens)\n",
    "\n",
    "### Why Information Retrieval Exists in Retail Banking\n",
    "\n",
    "Banks accumulate vast knowledge bases that employees and customers need to search effectively.\n",
    "\n",
    "| Use Case | Content | Users | Search Challenge |\n",
    "|----------|---------|-------|------------------|\n",
    "| **Policy Search** | 10,000+ internal policies | All employees | Find relevant policy quickly |\n",
    "| **FAQ Matching** | Customer service FAQ | Call center agents | Match customer question to answer |\n",
    "| **Document Discovery** | Contracts, agreements | Legal/compliance | Find all related documents |\n",
    "| **Knowledge Base** | Product documentation | Customers, advisors | Self-service answers |\n",
    "| **Regulatory Search** | Compliance regulations | Compliance officers | Find applicable rules |\n",
    "\n",
    "### The Business Problem\n",
    "\n",
    "> \"Our call center agents spend 3 minutes searching for the right policy to answer customer questions. How do we reduce this to 10 seconds?\"\n",
    "\n",
    "**Without good search**: Agents guess keywords, browse hierarchies, call colleagues  \n",
    "**With good search**: Type natural question, get ranked relevant policies instantly\n",
    "\n",
    "### Real Banking Example\n",
    "\n",
    "**Query**: \"What is the daily ATM withdrawal limit for premium checking accounts?\"\n",
    "\n",
    "**Expected Result**: Policy document section on withdrawal limits, ranked by relevance\n",
    "\n",
    "### This is Pre-RAG Foundation\n",
    "\n",
    "Modern RAG systems combine:\n",
    "1. **Retrieval** (this notebook) - Find relevant documents\n",
    "2. **Generation** (LLM) - Generate answer from retrieved docs\n",
    "\n",
    "Understanding traditional retrieval is essential for building and debugging RAG systems.\n",
    "\n",
    "### Interview Framing\n",
    "\n",
    "```\n",
    "\"Before jumping to vector search and embeddings, I always establish a BM25 baseline. \n",
    "It's fast, interpretable, and surprisingly competitive. In many banking use cases, \n",
    "BM25 actually outperforms dense retrieval because banking queries often contain \n",
    "specific terms like 'Regulation E' or 'wire transfer' that benefit from exact matching.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem Definition\n",
    "\n",
    "### Task Type: Ranking/Retrieval\n",
    "\n",
    "| Aspect | Description |\n",
    "|--------|-------------|\n",
    "| **Input** | Query + Document corpus |\n",
    "| **Output** | Ranked list of documents |\n",
    "| **Core Challenge** | Define \"relevance\" mathematically |\n",
    "| **Constraint** | Must be fast (sub-second for large corpora) |\n",
    "\n",
    "### Sparse vs Dense Retrieval\n",
    "\n",
    "| Approach | Representation | Similarity | Strengths |\n",
    "|----------|---------------|------------|----------|\n",
    "| **Sparse (BM25)** | Term frequencies | Lexical overlap | Exact match, fast, interpretable |\n",
    "| **Dense (Embeddings)** | Dense vectors | Semantic similarity | Handles synonyms, paraphrase |\n",
    "\n",
    "### Why Traditional Retrieval Before Dense Methods\n",
    "\n",
    "1. **Exact keyword matching**: \"Regulation E\" must match \"Regulation E\"\n",
    "2. **Interpretable**: Can explain why document was retrieved (shared terms)\n",
    "3. **Fast at scale**: Inverted indices enable sub-ms retrieval over millions of docs\n",
    "4. **No training required**: Works out of the box\n",
    "5. **Strong baseline**: Often hard to beat significantly with dense methods\n",
    "\n",
    "### Mathematical Foundation: TF-IDF\n",
    "\n",
    "$$\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)$$\n",
    "\n",
    "Where:\n",
    "- $\\text{TF}(t, d)$ = Term frequency of term $t$ in document $d$\n",
    "- $\\text{IDF}(t, D) = \\log\\frac{|D|}{|\\{d \\in D : t \\in d\\}|}$ = Inverse document frequency\n",
    "\n",
    "### BM25 (Best Match 25)\n",
    "\n",
    "$$\\text{BM25}(q, d) = \\sum_{t \\in q} \\text{IDF}(t) \\cdot \\frac{f(t, d) \\cdot (k_1 + 1)}{f(t, d) + k_1 \\cdot (1 - b + b \\cdot \\frac{|d|}{\\text{avgdl}})}$$\n",
    "\n",
    "Key improvements over TF-IDF:\n",
    "- Term frequency saturation (diminishing returns)\n",
    "- Document length normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset\n",
    "\n",
    "### Dataset: Banking Policy Documents (Simulated)\n",
    "\n",
    "We'll create a simulated banking knowledge base with policy documents and FAQs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install scikit-learn nltk rank-bm25 numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Sklearn for TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create banking knowledge base\n",
    "banking_documents = [\n",
    "    {\n",
    "        \"id\": \"POL-001\",\n",
    "        \"title\": \"ATM Withdrawal Limits Policy\",\n",
    "        \"content\": \"\"\"Daily ATM withdrawal limits vary by account type. Standard checking accounts have a daily limit of $500. \n",
    "        Premium checking accounts have an increased daily limit of $1,000. Private banking clients can withdraw up to $2,500 per day. \n",
    "        Limits reset at midnight Eastern Time. Customers may request temporary limit increases by contacting customer service \n",
    "        at least 24 hours in advance. International ATM withdrawals are subject to additional daily limits of $300.\"\"\",\n",
    "        \"category\": \"Limits\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"POL-002\",\n",
    "        \"title\": \"Wire Transfer Procedures\",\n",
    "        \"content\": \"\"\"Domestic wire transfers can be initiated online, by phone, or in branch. The cutoff time for same-day \n",
    "        processing is 4:00 PM Eastern Time. International wire transfers require additional verification and have a \n",
    "        cutoff of 2:00 PM Eastern Time. Wire transfer fees are $25 for domestic and $45 for international transfers. \n",
    "        SWIFT code and IBAN are required for international transfers. Maximum wire transfer limit is $100,000 per day \n",
    "        without additional approval.\"\"\",\n",
    "        \"category\": \"Transfers\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"POL-003\",\n",
    "        \"title\": \"Overdraft Protection Policy\",\n",
    "        \"content\": \"\"\"Overdraft protection links your checking account to a savings account or line of credit. When your \n",
    "        checking balance is insufficient, funds are automatically transferred. There is a $10 transfer fee per \n",
    "        overdraft occurrence. Standard overdraft coverage charges $35 per item. Customers can opt out of overdraft \n",
    "        coverage for ATM and debit card transactions. Overdraft fees are limited to 4 per day maximum.\"\"\",\n",
    "        \"category\": \"Fees\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"POL-004\",\n",
    "        \"title\": \"Mobile Check Deposit Policy\",\n",
    "        \"content\": \"\"\"Mobile check deposit allows customers to deposit checks using the mobile app. Daily deposit limits \n",
    "        are $5,000 for standard accounts and $10,000 for premium accounts. Funds availability is typically next \n",
    "        business day for checks under $500 and 2 business days for larger amounts. Checks must be endorsed with \n",
    "        'For Mobile Deposit Only' and account number. Eligible checks include personal, business, and government checks.\"\"\",\n",
    "        \"category\": \"Deposits\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"POL-005\",\n",
    "        \"title\": \"Account Closure Procedures\",\n",
    "        \"content\": \"\"\"Customers may close their account at any time by visiting a branch or calling customer service. \n",
    "        All pending transactions must clear before closure. Outstanding loans or credit cards linked to the account \n",
    "        must be paid off or transferred. Early closure fee of $25 applies if account is closed within 90 days of opening. \n",
    "        Remaining balance will be mailed via check within 7-10 business days.\"\"\",\n",
    "        \"category\": \"Account Management\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"POL-006\",\n",
    "        \"title\": \"Fraud Dispute Resolution\",\n",
    "        \"content\": \"\"\"Customers should report suspected fraud immediately by calling 1-800-FRAUD or through online banking. \n",
    "        Provisional credit is typically issued within 10 business days for disputed transactions. Investigation may take \n",
    "        up to 45 days for domestic transactions and 90 days for international. Under Regulation E, customer liability is \n",
    "        limited to $50 if reported within 2 business days. Zero liability protection applies to credit card transactions.\"\"\",\n",
    "        \"category\": \"Fraud\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"POL-007\",\n",
    "        \"title\": \"Interest Rate Disclosure\",\n",
    "        \"content\": \"\"\"Savings account interest rates are variable and subject to change. Current APY for standard savings \n",
    "        is 0.50%. High-yield savings accounts earn 4.25% APY for balances over $10,000. Interest is compounded daily \n",
    "        and credited monthly. Certificate of Deposit rates are fixed for the term duration. Early withdrawal penalty \n",
    "        for CDs is 90 days of interest for terms under 1 year.\"\"\",\n",
    "        \"category\": \"Rates\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"POL-008\",\n",
    "        \"title\": \"Zelle Payment Limits and Policies\",\n",
    "        \"content\": \"\"\"Zelle enables instant person-to-person payments using email or phone number. Daily sending limit \n",
    "        is $2,500 and monthly limit is $10,000 for standard accounts. Premium accounts have $5,000 daily limits. \n",
    "        Zelle payments cannot be cancelled once sent. Recipient must be enrolled in Zelle to receive funds. \n",
    "        Business accounts have separate limits and terms.\"\"\",\n",
    "        \"category\": \"Transfers\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"POL-009\",\n",
    "        \"title\": \"Safe Deposit Box Policy\",\n",
    "        \"content\": \"\"\"Safe deposit boxes are available at select branches. Annual rental fees range from $50 for \n",
    "        small boxes to $300 for large boxes. Two keys are provided; replacement keys cost $25 each. Access is \n",
    "        available during regular banking hours. Contents are not insured by the bank; customers should obtain \n",
    "        separate insurance. Drilling fee of $150 applies if keys are lost.\"\"\",\n",
    "        \"category\": \"Services\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"POL-010\",\n",
    "        \"title\": \"Foreign Currency Exchange\",\n",
    "        \"content\": \"\"\"Foreign currency exchange is available at main branches. Exchange rates are updated daily and \n",
    "        include a spread. No commission for amounts under $1,000. Orders over $5,000 require 2 business days notice. \n",
    "        Available currencies include EUR, GBP, JPY, CAD, and 40 other currencies. Buyback rates are typically less \n",
    "        favorable than sell rates.\"\"\",\n",
    "        \"category\": \"Services\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"FAQ-001\",\n",
    "        \"title\": \"How do I reset my online banking password?\",\n",
    "        \"content\": \"\"\"To reset your online banking password, click 'Forgot Password' on the login page. Enter your \n",
    "        username and verify your identity using your registered phone number or email. You will receive a one-time \n",
    "        code to create a new password. Passwords must be 8-20 characters with at least one uppercase letter, one \n",
    "        number, and one special character.\"\"\",\n",
    "        \"category\": \"FAQ\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"FAQ-002\",\n",
    "        \"title\": \"What is the routing number for wire transfers?\",\n",
    "        \"content\": \"\"\"Our routing number for domestic wire transfers is 021000089. For ACH transfers and direct \n",
    "        deposits, use routing number 021000021. The routing number is also printed on the bottom left of your checks. \n",
    "        International wire transfers require our SWIFT code: BOFAUS3N. Always verify the routing number before \n",
    "        initiating transfers.\"\"\",\n",
    "        \"category\": \"FAQ\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "docs_df = pd.DataFrame(banking_documents)\n",
    "print(f\"Knowledge base: {len(docs_df)} documents\")\n",
    "print(f\"\\nCategories: {docs_df['category'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample queries for testing\n",
    "test_queries = [\n",
    "    \"What is the ATM withdrawal limit for premium accounts?\",\n",
    "    \"How do I send a wire transfer?\",\n",
    "    \"What are the overdraft fees?\",\n",
    "    \"How long does mobile deposit take?\",\n",
    "    \"Report fraud on my account\",\n",
    "    \"What is the savings account interest rate?\",\n",
    "    \"Zelle payment limits\",\n",
    "    \"routing number for direct deposit\"\n",
    "]\n",
    "\n",
    "print(\"Test queries:\")\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    print(f\"  {i}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Traditional NLP Pipeline\n",
    "\n",
    "### 4.1 Text Preprocessing for Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocessor for search/retrieval tasks.\n",
    "    \n",
    "    Key considerations:\n",
    "    - Stemming helps match 'withdrawal' with 'withdrawals'\n",
    "    - Remove stopwords to focus on content words\n",
    "    - Lowercase for case-insensitive matching\n",
    "    - BUT: Keep important banking terms intact\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_stemming=True):\n",
    "        self.use_stemming = use_stemming\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Banking terms to preserve (don't stem)\n",
    "        self.preserve_terms = {\n",
    "            'atm', 'apy', 'ach', 'iban', 'swift', 'zelle', 'cd', 'cds',\n",
    "            'regulation', 'fdic', 'overdraft'\n",
    "        }\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize and normalize text.\"\"\"\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Handle special patterns (keep numbers with $)\n",
    "        text = re.sub(r'\\$([\\d,]+)', r'DOLLAR\\1', text)\n",
    "        \n",
    "        # Remove punctuation except hyphens in compound words\n",
    "        text = re.sub(r'[^\\w\\s-]', ' ', text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Process tokens\n",
    "        processed = []\n",
    "        for token in tokens:\n",
    "            # Skip stopwords\n",
    "            if token in self.stop_words:\n",
    "                continue\n",
    "            \n",
    "            # Skip very short tokens\n",
    "            if len(token) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Preserve banking terms\n",
    "            if token in self.preserve_terms:\n",
    "                processed.append(token)\n",
    "            elif self.use_stemming:\n",
    "                processed.append(self.stemmer.stem(token))\n",
    "            else:\n",
    "                processed.append(token)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Return preprocessed text as string.\"\"\"\n",
    "        return ' '.join(self.tokenize(text))\n",
    "\n",
    "preprocessor = SearchPreprocessor(use_stemming=True)\n",
    "\n",
    "# Demo\n",
    "sample = \"What is the daily ATM withdrawal limit for premium checking accounts?\"\n",
    "print(f\"Original: {sample}\")\n",
    "print(f\"Tokens: {preprocessor.tokenize(sample)}\")\n",
    "print(f\"Processed: {preprocessor.preprocess(sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 TF-IDF Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFRetriever:\n",
    "    \"\"\"\n",
    "    TF-IDF based document retriever.\n",
    "    \n",
    "    Uses cosine similarity between query and document TF-IDF vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "            max_df=0.9,\n",
    "            min_df=1\n",
    "        )\n",
    "        self.doc_vectors = None\n",
    "        self.documents = None\n",
    "    \n",
    "    def index(self, documents):\n",
    "        \"\"\"Build TF-IDF index from documents.\"\"\"\n",
    "        self.documents = documents\n",
    "        \n",
    "        # Combine title and content for indexing\n",
    "        texts = [f\"{doc['title']} {doc['content']}\" for doc in documents]\n",
    "        \n",
    "        # Build TF-IDF matrix\n",
    "        self.doc_vectors = self.vectorizer.fit_transform(texts)\n",
    "        \n",
    "        print(f\"Indexed {len(documents)} documents\")\n",
    "        print(f\"Vocabulary size: {len(self.vectorizer.vocabulary_)}\")\n",
    "    \n",
    "    def search(self, query, top_k=5):\n",
    "        \"\"\"Search for relevant documents.\"\"\"\n",
    "        # Transform query\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarities = cosine_similarity(query_vector, self.doc_vectors).flatten()\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = similarities.argsort()[::-1][:top_k]\n",
    "        \n",
    "        # Build results\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] > 0:  # Only include if there's some match\n",
    "                results.append({\n",
    "                    'rank': len(results) + 1,\n",
    "                    'doc_id': self.documents[idx]['id'],\n",
    "                    'title': self.documents[idx]['title'],\n",
    "                    'score': float(similarities[idx]),\n",
    "                    'content_preview': self.documents[idx]['content'][:200] + '...'\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Build TF-IDF index\n",
    "tfidf_retriever = TFIDFRetriever()\n",
    "tfidf_retriever.index(banking_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test TF-IDF retrieval\n",
    "query = \"What is the ATM withdrawal limit for premium accounts?\"\n",
    "\n",
    "results = tfidf_retriever.search(query, top_k=3)\n",
    "\n",
    "print(f\"TF-IDF RETRIEVAL\")\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\nRank {result['rank']}: {result['title']}\")\n",
    "    print(f\"  Score: {result['score']:.4f}\")\n",
    "    print(f\"  Preview: {result['content_preview'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 BM25 Retrieval\n",
    "\n",
    "BM25 improves on TF-IDF with:\n",
    "- Term frequency saturation (diminishing returns)\n",
    "- Document length normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    \"\"\"\n",
    "    BM25 (Okapi BM25) retriever - the gold standard for sparse retrieval.\n",
    "    \n",
    "    Parameters:\n",
    "    - k1: Term frequency saturation (typical: 1.2-2.0)\n",
    "    - b: Document length normalization (typical: 0.75)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k1=1.5, b=0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.preprocessor = SearchPreprocessor(use_stemming=True)\n",
    "        \n",
    "        # Index structures\n",
    "        self.documents = None\n",
    "        self.doc_tokens = None\n",
    "        self.doc_lengths = None\n",
    "        self.avgdl = 0\n",
    "        self.idf = {}\n",
    "        self.inverted_index = defaultdict(list)\n",
    "    \n",
    "    def index(self, documents):\n",
    "        \"\"\"Build BM25 index.\"\"\"\n",
    "        self.documents = documents\n",
    "        n_docs = len(documents)\n",
    "        \n",
    "        # Tokenize all documents\n",
    "        self.doc_tokens = []\n",
    "        self.doc_lengths = []\n",
    "        doc_freqs = Counter()\n",
    "        \n",
    "        for doc in documents:\n",
    "            text = f\"{doc['title']} {doc['content']}\"\n",
    "            tokens = self.preprocessor.tokenize(text)\n",
    "            self.doc_tokens.append(tokens)\n",
    "            self.doc_lengths.append(len(tokens))\n",
    "            \n",
    "            # Count document frequencies\n",
    "            unique_tokens = set(tokens)\n",
    "            for token in unique_tokens:\n",
    "                doc_freqs[token] += 1\n",
    "        \n",
    "        # Calculate average document length\n",
    "        self.avgdl = sum(self.doc_lengths) / len(self.doc_lengths)\n",
    "        \n",
    "        # Calculate IDF for each term\n",
    "        for term, df in doc_freqs.items():\n",
    "            # BM25 IDF formula\n",
    "            self.idf[term] = math.log((n_docs - df + 0.5) / (df + 0.5) + 1)\n",
    "        \n",
    "        # Build inverted index with term frequencies\n",
    "        for doc_idx, tokens in enumerate(self.doc_tokens):\n",
    "            term_freqs = Counter(tokens)\n",
    "            for term, freq in term_freqs.items():\n",
    "                self.inverted_index[term].append((doc_idx, freq))\n",
    "        \n",
    "        print(f\"Indexed {n_docs} documents\")\n",
    "        print(f\"Vocabulary size: {len(self.idf)}\")\n",
    "        print(f\"Average document length: {self.avgdl:.1f} tokens\")\n",
    "    \n",
    "    def score(self, query_tokens, doc_idx):\n",
    "        \"\"\"Calculate BM25 score for a document.\"\"\"\n",
    "        doc_tokens = self.doc_tokens[doc_idx]\n",
    "        doc_len = self.doc_lengths[doc_idx]\n",
    "        term_freqs = Counter(doc_tokens)\n",
    "        \n",
    "        score = 0.0\n",
    "        for term in query_tokens:\n",
    "            if term not in self.idf:\n",
    "                continue\n",
    "            \n",
    "            tf = term_freqs.get(term, 0)\n",
    "            idf = self.idf[term]\n",
    "            \n",
    "            # BM25 scoring formula\n",
    "            numerator = tf * (self.k1 + 1)\n",
    "            denominator = tf + self.k1 * (1 - self.b + self.b * (doc_len / self.avgdl))\n",
    "            score += idf * (numerator / denominator)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def search(self, query, top_k=5):\n",
    "        \"\"\"Search for relevant documents.\"\"\"\n",
    "        query_tokens = self.preprocessor.tokenize(query)\n",
    "        \n",
    "        # Find candidate documents (those containing at least one query term)\n",
    "        candidate_docs = set()\n",
    "        for token in query_tokens:\n",
    "            if token in self.inverted_index:\n",
    "                for doc_idx, _ in self.inverted_index[token]:\n",
    "                    candidate_docs.add(doc_idx)\n",
    "        \n",
    "        # Score candidates\n",
    "        scores = []\n",
    "        for doc_idx in candidate_docs:\n",
    "            score = self.score(query_tokens, doc_idx)\n",
    "            scores.append((doc_idx, score))\n",
    "        \n",
    "        # Sort by score\n",
    "        scores.sort(key=lambda x: -x[1])\n",
    "        \n",
    "        # Build results\n",
    "        results = []\n",
    "        for rank, (doc_idx, score) in enumerate(scores[:top_k], 1):\n",
    "            results.append({\n",
    "                'rank': rank,\n",
    "                'doc_id': self.documents[doc_idx]['id'],\n",
    "                'title': self.documents[doc_idx]['title'],\n",
    "                'score': score,\n",
    "                'content_preview': self.documents[doc_idx]['content'][:200] + '...'\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Build BM25 index\n",
    "bm25_retriever = BM25Retriever(k1=1.5, b=0.75)\n",
    "bm25_retriever.index(banking_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BM25 retrieval\n",
    "query = \"What is the ATM withdrawal limit for premium accounts?\"\n",
    "\n",
    "results = bm25_retriever.search(query, top_k=3)\n",
    "\n",
    "print(f\"BM25 RETRIEVAL\")\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\nRank {result['rank']}: {result['title']}\")\n",
    "    print(f\"  Score: {result['score']:.4f}\")\n",
    "    print(f\"  Preview: {result['content_preview'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare TF-IDF vs BM25 on all test queries\n",
    "def compare_retrievers(queries, tfidf_ret, bm25_ret, top_k=3):\n",
    "    \"\"\"Compare retrieval results from different methods.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for query in queries:\n",
    "        tfidf_results = tfidf_ret.search(query, top_k=top_k)\n",
    "        bm25_results = bm25_ret.search(query, top_k=top_k)\n",
    "        \n",
    "        tfidf_docs = [r['doc_id'] for r in tfidf_results]\n",
    "        bm25_docs = [r['doc_id'] for r in bm25_results]\n",
    "        \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'tfidf_top1': tfidf_docs[0] if tfidf_docs else None,\n",
    "            'bm25_top1': bm25_docs[0] if bm25_docs else None,\n",
    "            'agreement': tfidf_docs[0] == bm25_docs[0] if tfidf_docs and bm25_docs else False\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "comparison = compare_retrievers(test_queries, tfidf_retriever, bm25_retriever)\n",
    "\n",
    "print(\"RETRIEVER COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "agreement_rate = comparison['agreement'].mean()\n",
    "print(f\"\\nTop-1 Agreement Rate: {agreement_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production search interface\n",
    "class ProductionSearchEngine:\n",
    "    \"\"\"\n",
    "    Production-ready search engine with multiple retrieval methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, documents, default_method='bm25'):\n",
    "        self.documents = {doc['id']: doc for doc in documents}\n",
    "        self.default_method = default_method\n",
    "        \n",
    "        # Initialize retrievers\n",
    "        self.tfidf = TFIDFRetriever()\n",
    "        self.bm25 = BM25Retriever()\n",
    "        \n",
    "        # Build indices\n",
    "        print(\"Building search indices...\")\n",
    "        self.tfidf.index(documents)\n",
    "        self.bm25.index(documents)\n",
    "        print(\"Search engine ready.\")\n",
    "    \n",
    "    def search(self, query, top_k=5, method=None, min_score=0.0):\n",
    "        \"\"\"\n",
    "        Search for documents matching query.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query string\n",
    "            top_k: Number of results to return\n",
    "            method: 'tfidf' or 'bm25' (default: bm25)\n",
    "            min_score: Minimum relevance score to include\n",
    "        \"\"\"\n",
    "        method = method or self.default_method\n",
    "        \n",
    "        # Input validation\n",
    "        if not query or len(query.strip()) < 2:\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'message': 'Query too short',\n",
    "                'results': []\n",
    "            }\n",
    "        \n",
    "        # Retrieve\n",
    "        if method == 'tfidf':\n",
    "            results = self.tfidf.search(query, top_k=top_k)\n",
    "        else:\n",
    "            results = self.bm25.search(query, top_k=top_k)\n",
    "        \n",
    "        # Filter by minimum score\n",
    "        results = [r for r in results if r['score'] >= min_score]\n",
    "        \n",
    "        # Add full document content for top results\n",
    "        for result in results:\n",
    "            doc = self.documents[result['doc_id']]\n",
    "            result['full_content'] = doc['content']\n",
    "            result['category'] = doc['category']\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'query': query,\n",
    "            'method': method,\n",
    "            'total_results': len(results),\n",
    "            'results': results\n",
    "        }\n",
    "    \n",
    "    def hybrid_search(self, query, top_k=5, tfidf_weight=0.3):\n",
    "        \"\"\"\n",
    "        Combine TF-IDF and BM25 results with weighted scoring.\n",
    "        \"\"\"\n",
    "        tfidf_results = self.tfidf.search(query, top_k=top_k*2)\n",
    "        bm25_results = self.bm25.search(query, top_k=top_k*2)\n",
    "        \n",
    "        # Normalize scores\n",
    "        def normalize(results):\n",
    "            if not results:\n",
    "                return {}\n",
    "            max_score = max(r['score'] for r in results)\n",
    "            return {r['doc_id']: r['score']/max_score if max_score > 0 else 0 \n",
    "                   for r in results}\n",
    "        \n",
    "        tfidf_scores = normalize(tfidf_results)\n",
    "        bm25_scores = normalize(bm25_results)\n",
    "        \n",
    "        # Combine scores\n",
    "        all_docs = set(tfidf_scores.keys()) | set(bm25_scores.keys())\n",
    "        combined = []\n",
    "        \n",
    "        for doc_id in all_docs:\n",
    "            score = (tfidf_weight * tfidf_scores.get(doc_id, 0) + \n",
    "                    (1 - tfidf_weight) * bm25_scores.get(doc_id, 0))\n",
    "            combined.append((doc_id, score))\n",
    "        \n",
    "        # Sort and return top-k\n",
    "        combined.sort(key=lambda x: -x[1])\n",
    "        \n",
    "        results = []\n",
    "        for rank, (doc_id, score) in enumerate(combined[:top_k], 1):\n",
    "            doc = self.documents[doc_id]\n",
    "            results.append({\n",
    "                'rank': rank,\n",
    "                'doc_id': doc_id,\n",
    "                'title': doc['title'],\n",
    "                'score': score,\n",
    "                'content_preview': doc['content'][:200] + '...'\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'method': 'hybrid',\n",
    "            'results': results\n",
    "        }\n",
    "\n",
    "# Initialize production search\n",
    "search_engine = ProductionSearchEngine(banking_documents, default_method='bm25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test production search\n",
    "query = \"How do I report fraud on my account?\"\n",
    "\n",
    "result = search_engine.search(query, top_k=3)\n",
    "\n",
    "print(\"PRODUCTION SEARCH RESULT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: {result['query']}\")\n",
    "print(f\"Method: {result['method']}\")\n",
    "print(f\"Results found: {result['total_results']}\")\n",
    "\n",
    "for r in result['results']:\n",
    "    print(f\"\\n{r['rank']}. [{r['doc_id']}] {r['title']}\")\n",
    "    print(f\"   Category: {r['category']}\")\n",
    "    print(f\"   Score: {r['score']:.4f}\")\n",
    "    print(f\"   Preview: {r['content_preview'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Strategy\n",
    "\n",
    "### Key Retrieval Metrics\n",
    "\n",
    "| Metric | Formula | Interpretation |\n",
    "|--------|---------|----------------|\n",
    "| **Precision@K** | Relevant in top K / K | Quality of top results |\n",
    "| **Recall@K** | Relevant in top K / Total relevant | Coverage |\n",
    "| **MRR** | 1 / rank of first relevant | How quickly we find something relevant |\n",
    "| **NDCG@K** | DCG@K / IDCG@K | Position-weighted relevance |\n",
    "| **MAP** | Mean of Average Precision | Overall ranking quality |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_retrieval_metrics(results, relevant_docs, k=5):\n",
    "    \"\"\"\n",
    "    Calculate standard retrieval metrics.\n",
    "    \n",
    "    Args:\n",
    "        results: List of retrieved doc_ids in rank order\n",
    "        relevant_docs: Set of relevant doc_ids (ground truth)\n",
    "        k: Cutoff for @K metrics\n",
    "    \"\"\"\n",
    "    results_at_k = results[:k]\n",
    "    \n",
    "    # Precision@K\n",
    "    relevant_retrieved = sum(1 for doc in results_at_k if doc in relevant_docs)\n",
    "    precision_at_k = relevant_retrieved / k if k > 0 else 0\n",
    "    \n",
    "    # Recall@K\n",
    "    recall_at_k = relevant_retrieved / len(relevant_docs) if relevant_docs else 0\n",
    "    \n",
    "    # MRR (Mean Reciprocal Rank)\n",
    "    mrr = 0\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        if doc in relevant_docs:\n",
    "            mrr = 1 / i\n",
    "            break\n",
    "    \n",
    "    # NDCG@K\n",
    "    def dcg(relevances):\n",
    "        return sum(rel / math.log2(i + 2) for i, rel in enumerate(relevances))\n",
    "    \n",
    "    relevances = [1 if doc in relevant_docs else 0 for doc in results_at_k]\n",
    "    ideal_relevances = sorted(relevances, reverse=True)\n",
    "    \n",
    "    dcg_score = dcg(relevances)\n",
    "    idcg_score = dcg(ideal_relevances)\n",
    "    ndcg_at_k = dcg_score / idcg_score if idcg_score > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        f'Precision@{k}': precision_at_k,\n",
    "        f'Recall@{k}': recall_at_k,\n",
    "        'MRR': mrr,\n",
    "        f'NDCG@{k}': ndcg_at_k\n",
    "    }\n",
    "\n",
    "# Example evaluation\n",
    "# Simulated ground truth: which documents are relevant for each query\n",
    "ground_truth = {\n",
    "    \"What is the ATM withdrawal limit for premium accounts?\": {\"POL-001\"},\n",
    "    \"How do I send a wire transfer?\": {\"POL-002\", \"FAQ-002\"},\n",
    "    \"What are the overdraft fees?\": {\"POL-003\"},\n",
    "    \"How long does mobile deposit take?\": {\"POL-004\"},\n",
    "    \"Report fraud on my account\": {\"POL-006\"},\n",
    "    \"What is the savings account interest rate?\": {\"POL-007\"},\n",
    "    \"Zelle payment limits\": {\"POL-008\"},\n",
    "    \"routing number for direct deposit\": {\"FAQ-002\"}\n",
    "}\n",
    "\n",
    "# Evaluate BM25\n",
    "print(\"RETRIEVAL EVALUATION (BM25)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_metrics = []\n",
    "for query, relevant in ground_truth.items():\n",
    "    results = bm25_retriever.search(query, top_k=5)\n",
    "    retrieved_ids = [r['doc_id'] for r in results]\n",
    "    \n",
    "    metrics = calculate_retrieval_metrics(retrieved_ids, relevant, k=3)\n",
    "    metrics['query'] = query[:40] + '...'\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nAVERAGE METRICS:\")\n",
    "for col in ['Precision@3', 'Recall@3', 'MRR', 'NDCG@3']:\n",
    "    print(f\"  {col}: {metrics_df[col].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Readiness Checklist\n",
    "\n",
    "```\n",
    "INDEX MANAGEMENT\n",
    "[ ] Inverted index persistence (save/load)\n",
    "[ ] Incremental index updates (add/remove docs)\n",
    "[ ] Index versioning and rollback\n",
    "[ ] Index size monitoring\n",
    "\n",
    "QUERY PROCESSING\n",
    "[ ] Query spell correction\n",
    "[ ] Query expansion (synonyms)\n",
    "[ ] Query logging for analytics\n",
    "[ ] Rate limiting\n",
    "\n",
    "RETRIEVAL QUALITY\n",
    "[ ] Relevance feedback loop\n",
    "[ ] Click-through rate tracking\n",
    "[ ] A/B testing framework\n",
    "[ ] Regular evaluation against benchmarks\n",
    "\n",
    "PERFORMANCE\n",
    "[ ] Sub-100ms latency for 1M docs\n",
    "[ ] Caching for frequent queries\n",
    "[ ] Load balancing for high traffic\n",
    "[ ] Timeout handling\n",
    "\n",
    "BANKING-SPECIFIC\n",
    "[ ] Access control (who can search what)\n",
    "[ ] Audit trail of searches\n",
    "[ ] Sensitive document handling\n",
    "[ ] Regulatory compliance tagging\n",
    "\n",
    "MONITORING\n",
    "[ ] Zero-result rate tracking\n",
    "[ ] Query latency percentiles\n",
    "[ ] Index freshness monitoring\n",
    "[ ] Error rate alerting\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Modern LLM-Based Approach\n",
    "\n",
    "### Dense Retrieval and RAG\n",
    "\n",
    "**Dense Retrieval** replaces sparse term matching with semantic embedding similarity:\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Index: Embed all documents\n",
    "doc_embeddings = model.encode([doc['content'] for doc in documents])\n",
    "\n",
    "# Search: Embed query and find similar\n",
    "query_embedding = model.encode(query)\n",
    "similarities = cosine_similarity([query_embedding], doc_embeddings)\n",
    "```\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** combines retrieval with LLM generation:\n",
    "\n",
    "```python\n",
    "# Step 1: Retrieve relevant documents\n",
    "docs = retriever.search(query, top_k=3)\n",
    "\n",
    "# Step 2: Create prompt with context\n",
    "context = \"\\n\".join([doc['content'] for doc in docs])\n",
    "prompt = f\"\"\"Based on the following policy documents, answer the question.\n",
    "\n",
    "Documents:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Step 3: Generate answer with LLM\n",
    "answer = llm.generate(prompt)\n",
    "```\n",
    "\n",
    "### Hybrid Search\n",
    "\n",
    "Modern systems combine sparse and dense:\n",
    "```python\n",
    "final_score = alpha * bm25_score + (1 - alpha) * dense_score\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for RAG pipeline\n",
    "def create_rag_prompt(query, retrieved_docs, max_context_length=2000):\n",
    "    \"\"\"\n",
    "    Create RAG prompt for question answering.\n",
    "    \n",
    "    Banking considerations:\n",
    "    - Include document IDs for attribution\n",
    "    - Instruct model to cite sources\n",
    "    - Handle case when no relevant docs found\n",
    "    \"\"\"\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        return \"\"\"I could not find any relevant documents to answer this question. \n",
    "        Please contact customer service for assistance.\"\"\"\n",
    "    \n",
    "    # Build context from retrieved docs\n",
    "    context_parts = []\n",
    "    total_length = 0\n",
    "    \n",
    "    for doc in retrieved_docs:\n",
    "        doc_text = f\"[{doc['doc_id']}] {doc['title']}\\n{doc['full_content']}\"\n",
    "        if total_length + len(doc_text) > max_context_length:\n",
    "            break\n",
    "        context_parts.append(doc_text)\n",
    "        total_length += len(doc_text)\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = f\"\"\"You are a banking customer service assistant. Answer the customer's question \n",
    "based ONLY on the information provided in the policy documents below.\n",
    "\n",
    "Rules:\n",
    "1. Only use information from the provided documents\n",
    "2. Cite the document ID (e.g., [POL-001]) when referencing specific information\n",
    "3. If the documents don't contain enough information, say so\n",
    "4. Be concise and direct\n",
    "\n",
    "Policy Documents:\n",
    "{context}\n",
    "\n",
    "Customer Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example\n",
    "query = \"What is the daily ATM limit for premium accounts?\"\n",
    "results = search_engine.search(query, top_k=2)['results']\n",
    "\n",
    "prompt = create_rag_prompt(query, results)\n",
    "\n",
    "print(\"RAG PROMPT EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "print(prompt[:1500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Traditional vs LLM Decision Matrix\n",
    "\n",
    "| Dimension | Sparse (BM25) | Dense (Embeddings) | Hybrid |\n",
    "|-----------|---------------|-------------------|--------|\n",
    "| **Exact Match** | Excellent | Poor | Good |\n",
    "| **Semantic Match** | Poor | Excellent | Good |\n",
    "| **Speed** | Very fast (<10ms) | Slower (50-100ms) | Medium |\n",
    "| **Index Size** | Small | Large (vectors) | Larger |\n",
    "| **Out-of-vocabulary** | Fails | Handles well | Handles well |\n",
    "| **Interpretability** | High (term overlap) | Low (embedding space) | Medium |\n",
    "| **Training Required** | No | Yes (or pretrained) | Partial |\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "**Use BM25 (Sparse)**:\n",
    "- Queries contain specific terms (\"Regulation E\", \"wire transfer\")\n",
    "- Need interpretable results (why was this doc retrieved?)\n",
    "- Index size is a concern\n",
    "- Building a baseline quickly\n",
    "\n",
    "**Use Dense Retrieval**:\n",
    "- Semantic matching needed (\"send money\" = \"transfer funds\")\n",
    "- User queries are natural language questions\n",
    "- Multilingual requirements\n",
    "\n",
    "**Use Hybrid**:\n",
    "- Best of both worlds needed\n",
    "- Production system where accuracy matters\n",
    "- Diverse query types (some keyword, some semantic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interview Soundbites\n",
    "\n",
    "### Ready-to-Say Statements\n",
    "\n",
    "**On BM25:**\n",
    "> \"BM25 is my go-to baseline for any retrieval task. It's fast, requires no training, and surprisingly hard to beat. The key improvements over TF-IDF are term frequency saturation - seeing a word 10 times isn't 10x better than seeing it once - and document length normalization.\"\n",
    "\n",
    "**On Sparse vs Dense:**\n",
    "> \"Dense retrieval excels at semantic matching, but sparse retrieval wins on exact terms. In banking, when someone searches 'Regulation E', they need documents containing those exact words. That's why I always start with BM25 and consider dense as an enhancement, not a replacement.\"\n",
    "\n",
    "**On Hybrid Search:**\n",
    "> \"In production, I combine BM25 and dense retrieval. BM25 handles keyword queries and specific terms; dense handles paraphrases and semantic similarity. The weighted combination typically outperforms either alone by 10-15%.\"\n",
    "\n",
    "**On Evaluation:**\n",
    "> \"MRR is my primary metric for single-answer queries like FAQ search - it tells me how quickly users find what they need. For comprehensive search where multiple docs are relevant, I use NDCG because it rewards getting relevant docs ranked higher.\"\n",
    "\n",
    "**On RAG Architecture:**\n",
    "> \"RAG decouples retrieval from generation. The retriever finds relevant context, the LLM synthesizes an answer. This gives us the best of both: factual grounding from retrieval and fluent answers from generation. But the retriever quality is critical - garbage in, garbage out.\"\n",
    "\n",
    "**On Production:**\n",
    "> \"Retrieval latency is critical for user experience. BM25 with inverted indices gives sub-10ms retrieval over millions of documents. Dense retrieval with vector search takes 50-100ms. For real-time applications, I often use BM25 for initial candidate generation, then re-rank with dense embeddings.\"\n",
    "\n",
    "**On Failure Modes:**\n",
    "> \"The biggest failure mode is vocabulary mismatch. If users say 'money transfer' but docs say 'wire transfer', BM25 misses it. Solutions: query expansion with synonyms, embedding-based retrieval, or training on user click data to learn the mapping.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Common Interview Questions\n",
    "\n",
    "**Q: What's the difference between TF-IDF and BM25?**\n",
    "> BM25 adds two improvements: (1) term frequency saturation via the k1 parameter - diminishing returns for repeated terms, and (2) document length normalization via the b parameter - don't unfairly penalize short documents. These make BM25 more robust in practice.\n",
    "\n",
    "**Q: How do you handle synonyms in sparse retrieval?**\n",
    "> Options: (1) Query expansion - add synonyms to query at search time, (2) Index expansion - add synonyms to documents at index time, (3) Stemming/lemmatization - normalize to common form, (4) Hybrid with dense retrieval which handles synonyms naturally.\n",
    "\n",
    "**Q: How does retrieval fit into RAG?**\n",
    "> RAG = Retrieval + Augmented Generation. Retrieval finds relevant documents, which become context for the LLM. The LLM generates an answer grounded in that context. Retrieval quality directly impacts answer quality - poor retrieval means the LLM lacks the information to answer correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                    NOTEBOOK SUMMARY                               ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║  Task: Information Retrieval / Search                            ║\n",
    "║  Approach: Traditional NLP (TF-IDF, BM25)                        ║\n",
    "║  Banking Use: Policy search, FAQ matching, document discovery    ║\n",
    "║                                                                  ║\n",
    "║  Key Takeaways:                                                  ║\n",
    "║  1. BM25 is the gold standard baseline for sparse retrieval      ║\n",
    "║  2. Term saturation + length normalization improve on TF-IDF     ║\n",
    "║  3. Hybrid (sparse + dense) often best in production             ║\n",
    "║  4. MRR/NDCG for evaluation, not just precision                  ║\n",
    "║  5. This is the foundation for RAG systems                       ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling & Clustering - Traditional NLP\n",
    "## Interview Preparation Notebook for Senior Applied AI Scientist (Retail Banking)\n",
    "\n",
    "---\n",
    "\n",
    "**Goal**: Demonstrate deep understanding of unsupervised text analysis techniques, production thinking, and when to choose traditional approaches over LLMs.\n",
    "\n",
    "**Interview Signal**: This notebook shows you can discover latent themes in unstructured text without labeled data - critical for understanding customer voice at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Context (Banking Lens)\n",
    "\n",
    "### Why Topic Modeling Exists in Retail Banking\n",
    "\n",
    "In retail banking, we generate massive volumes of unstructured text daily:\n",
    "\n",
    "| Source | Volume (Large Bank) | Business Need |\n",
    "|--------|---------------------|---------------|\n",
    "| Customer complaints | 50K-100K/month | Identify systemic issues before they escalate |\n",
    "| Call center transcripts | 500K-1M/month | Understand why customers call, reduce handle time |\n",
    "| Branch staff notes | 200K/month | Capture local market intelligence |\n",
    "| Social media mentions | 100K/month | Monitor brand sentiment and emerging issues |\n",
    "| Regulatory correspondence | 10K/month | Track compliance themes |\n",
    "\n",
    "### The Business Problem\n",
    "\n",
    "> \"We have 100,000 customer complaints this month. What are people actually complaining about?\"\n",
    "\n",
    "**Without topic modeling**: Manual sampling (5%), subjective categorization, missed patterns  \n",
    "**With topic modeling**: Automated discovery of ALL themes, trend detection, early warning signals\n",
    "\n",
    "### Real Banking Use Cases\n",
    "\n",
    "1. **Complaint Root Cause Analysis**: Discover that 23% of complaints mention \"mobile app crash\" + \"bill pay\" + \"Friday\" - reveals weekend deployment issue\n",
    "\n",
    "2. **Call Center Optimization**: Find that \"password reset\" accounts for 18% of calls - justify investment in self-service authentication\n",
    "\n",
    "3. **Regulatory Early Warning**: Detect emerging mentions of \"unauthorized charges\" before regulators notice the pattern\n",
    "\n",
    "4. **Product Feedback Synthesis**: Cluster 50K survey responses into actionable product themes\n",
    "\n",
    "### Interview Framing\n",
    "\n",
    "When asked \"How would you analyze customer feedback at scale?\"\n",
    "\n",
    "```\n",
    "\"I'd start with topic modeling to discover what customers are actually talking about, \n",
    "rather than forcing their feedback into predetermined categories. In banking, this is \n",
    "critical because we often don't know what we don't know - a new fraud pattern or \n",
    "product defect might emerge that doesn't fit our existing taxonomy. LDA or NMF gives \n",
    "us that discovery capability, and the topics become interpretable enough to share \n",
    "with business stakeholders and regulators.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem Definition\n",
    "\n",
    "### Task Type: Unsupervised Learning\n",
    "\n",
    "| Aspect | Description |\n",
    "|--------|-------------|\n",
    "| **Learning Type** | Unsupervised (no labels required) |\n",
    "| **Input** | Collection of text documents (corpus) |\n",
    "| **Output** | (1) Topic distributions per document, (2) Word distributions per topic |\n",
    "| **Core Assumption** | Documents are mixtures of latent topics |\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "**Latent Dirichlet Allocation (LDA)** generative process:\n",
    "\n",
    "1. For each topic $k$: draw word distribution $\\phi_k \\sim Dir(\\beta)$\n",
    "2. For each document $d$:\n",
    "   - Draw topic distribution $\\theta_d \\sim Dir(\\alpha)$\n",
    "   - For each word position $n$:\n",
    "     - Draw topic assignment $z_{d,n} \\sim Multinomial(\\theta_d)$\n",
    "     - Draw word $w_{d,n} \\sim Multinomial(\\phi_{z_{d,n}})$\n",
    "\n",
    "### Why This Approach Before LLMs\n",
    "\n",
    "1. **No labeled data required**: Banks rarely have pre-categorized complaints at scale\n",
    "2. **Interpretable**: Each topic is a distribution over words - explainable to regulators\n",
    "3. **Scalable**: LDA handles millions of documents with linear complexity\n",
    "4. **Deterministic** (with seed): Same input produces same output - auditability requirement\n",
    "5. **Low compute cost**: Runs on commodity hardware, no GPU required\n",
    "\n",
    "### Topic Modeling vs. Clustering\n",
    "\n",
    "| Approach | Assignment | Use Case |\n",
    "|----------|------------|----------|\n",
    "| **Topic Modeling (LDA)** | Soft (document can be 40% topic A, 60% topic B) | Customer feedback (mixed themes) |\n",
    "| **Clustering (K-Means)** | Hard (document belongs to one cluster) | Document routing, de-duplication |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset\n",
    "\n",
    "### Public Dataset: 20 Newsgroups\n",
    "\n",
    "We use the 20 Newsgroups dataset as a proxy for banking customer communications.\n",
    "\n",
    "**Why this dataset works for banking interview prep**:\n",
    "- Multi-topic documents (like customer complaints that mention multiple issues)\n",
    "- Noisy text (like real customer writing)\n",
    "- Clear ground truth for validation\n",
    "- Standard benchmark for topic modeling\n",
    "\n",
    "**Limitations vs. Real Banking Data**:\n",
    "- Banking text is shorter (avg 50 words vs 200+)\n",
    "- Banking has domain-specific vocabulary (\"ACH\", \"wire\", \"overdraft\")\n",
    "- Banking data has PII that requires masking\n",
    "- Banking complaints are more emotionally charged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install scikit-learn gensim pyLDAvis nltk matplotlib seaborn wordcloud pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility (CRITICAL for banking - auditability)\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset - using subset of categories for clearer topics\n",
    "categories = [\n",
    "    'comp.graphics',\n",
    "    'comp.sys.mac.hardware', \n",
    "    'rec.sport.baseball',\n",
    "    'rec.sport.hockey',\n",
    "    'sci.med',\n",
    "    'sci.space',\n",
    "    'talk.politics.guns',\n",
    "    'talk.religion.misc'\n",
    "]\n",
    "\n",
    "# Banking proxy: These categories simulate different complaint types\n",
    "# comp.* -> Technical/App issues\n",
    "# rec.* -> Service quality issues  \n",
    "# sci.* -> Product questions\n",
    "# talk.* -> Policy/fee complaints\n",
    "\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),  # Remove metadata noise\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "documents = newsgroups.data\n",
    "true_labels = newsgroups.target\n",
    "category_names = newsgroups.target_names\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents across {len(categories)} categories\")\n",
    "print(f\"\\nCategories: {category_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore sample documents\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE DOCUMENT (simulating a customer complaint)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Category: {category_names[true_labels[0]]}\")\n",
    "print(f\"\\nText (first 500 chars):\\n{documents[0][:500]}...\")\n",
    "print(f\"\\nDocument length: {len(documents[0].split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document length distribution - important for chunking decisions\n",
    "doc_lengths = [len(doc.split()) for doc in documents]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(doc_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(np.median(doc_lengths), color='red', linestyle='--', label=f'Median: {np.median(doc_lengths):.0f} words')\n",
    "ax.axvline(np.mean(doc_lengths), color='blue', linestyle='--', label=f'Mean: {np.mean(doc_lengths):.0f} words')\n",
    "ax.set_xlabel('Document Length (words)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Document Length Distribution')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Min: {min(doc_lengths)} words\")\n",
    "print(f\"  Max: {max(doc_lengths)} words\")\n",
    "print(f\"  Mean: {np.mean(doc_lengths):.1f} words\")\n",
    "print(f\"  Median: {np.median(doc_lengths):.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Traditional NLP Pipeline\n",
    "\n",
    "### 4.1 Text Cleaning\n",
    "\n",
    "Text preprocessing is critical for topic modeling quality. The goal is to normalize text while preserving semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Production-ready text preprocessor for topic modeling.\n",
    "    \n",
    "    Design decisions:\n",
    "    - Lowercase: Reduces vocabulary, improves topic coherence\n",
    "    - Remove punctuation: Not meaningful for topic discovery\n",
    "    - Remove numbers: Domain-specific decision (keep for banking amounts?)\n",
    "    - Remove stopwords: Critical for meaningful topics\n",
    "    - Lemmatize: Preserves meaning better than stemming for topic interpretation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 remove_numbers=True,\n",
    "                 min_word_length=3,\n",
    "                 custom_stopwords=None):\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.min_word_length = min_word_length\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Base stopwords + domain-specific additions\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Add common but uninformative words for topic modeling\n",
    "        additional_stops = {'would', 'could', 'also', 'one', 'two', 'get', 'got',\n",
    "                          'like', 'just', 'know', 'think', 'make', 'see', 'way'}\n",
    "        self.stop_words.update(additional_stops)\n",
    "        \n",
    "        if custom_stopwords:\n",
    "            self.stop_words.update(custom_stopwords)\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Apply all preprocessing steps.\"\"\"\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove email addresses (PII consideration)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        \n",
    "        # Remove numbers (configurable)\n",
    "        if self.remove_numbers:\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove punctuation and special characters\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize_and_lemmatize(self, text):\n",
    "        \"\"\"Tokenize and lemmatize text.\"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Filter and lemmatize\n",
    "        processed_tokens = [\n",
    "            self.lemmatizer.lemmatize(token)\n",
    "            for token in tokens\n",
    "            if token not in self.stop_words \n",
    "            and len(token) >= self.min_word_length\n",
    "        ]\n",
    "        \n",
    "        return ' '.join(processed_tokens)\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Full preprocessing pipeline.\"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        processed = self.tokenize_and_lemmatize(cleaned)\n",
    "        return processed\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(\n",
    "    remove_numbers=True,\n",
    "    min_word_length=3,\n",
    "    custom_stopwords={'subject', 'organization', 'lines'}  # Newsgroup-specific\n",
    ")\n",
    "\n",
    "print(\"TextPreprocessor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate preprocessing\n",
    "sample_text = documents[0]\n",
    "processed_text = preprocessor.preprocess(sample_text)\n",
    "\n",
    "print(\"ORIGINAL TEXT (first 300 chars):\")\n",
    "print(sample_text[:300])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"PROCESSED TEXT (first 300 chars):\")\n",
    "print(processed_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all documents\n",
    "print(\"Processing all documents...\")\n",
    "processed_documents = [preprocessor.preprocess(doc) for doc in documents]\n",
    "\n",
    "# Remove empty documents\n",
    "valid_indices = [i for i, doc in enumerate(processed_documents) if len(doc.split()) > 5]\n",
    "processed_documents = [processed_documents[i] for i in valid_indices]\n",
    "filtered_labels = [true_labels[i] for i in valid_indices]\n",
    "\n",
    "print(f\"Documents after preprocessing: {len(processed_documents)}\")\n",
    "print(f\"Documents removed (too short): {len(documents) - len(processed_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Stemming vs. Lemmatization\n",
    "\n",
    "**Critical Interview Question**: When do you use stemming vs. lemmatization?\n",
    "\n",
    "| Aspect | Stemming | Lemmatization |\n",
    "|--------|----------|---------------|\n",
    "| **Method** | Rule-based suffix removal | Dictionary lookup + POS tagging |\n",
    "| **Speed** | Faster | Slower |\n",
    "| **Output** | May not be real words (\"studi\") | Always real words (\"study\") |\n",
    "| **Use Case** | Search/retrieval, high volume | Topic modeling, when interpretability matters |\n",
    "\n",
    "**For Topic Modeling**: We use **lemmatization** because:\n",
    "1. Topics need to be interpretable to business stakeholders\n",
    "2. \"running\", \"ran\", \"runs\" → \"run\" (readable)\n",
    "3. Stemming: \"running\" → \"run\", but \"studies\" → \"studi\" (confusing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the difference\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "test_words = ['running', 'studies', 'better', 'organization', 'banking', 'complaints']\n",
    "\n",
    "print(f\"{'Word':<15} {'Stemmed':<15} {'Lemmatized':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for word in test_words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "    print(f\"{word:<15} {stemmed:<15} {lemmatized:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Feature Engineering\n",
    "\n",
    "For topic modeling, we use **Bag-of-Words** or **TF-IDF** representations.\n",
    "\n",
    "**Why not embeddings (Word2Vec, BERT) for traditional topic modeling?**\n",
    "1. LDA assumes word frequencies, not dense vectors\n",
    "2. Embeddings lose interpretability of individual words\n",
    "3. Pre-LLM era: embeddings were expensive to compute at scale\n",
    "\n",
    "**Interview Point**: \"Embeddings are great for semantic similarity, but for interpretable topic modeling, we still use count-based features because each dimension corresponds to a specific word.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-term matrix using CountVectorizer\n",
    "# This is the standard input for LDA\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_df=0.95,          # Ignore words appearing in >95% of docs (too common)\n",
    "    min_df=5,             # Ignore words appearing in <5 docs (too rare)\n",
    "    max_features=5000,    # Vocabulary size limit\n",
    "    ngram_range=(1, 1),   # Unigrams only for interpretability\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "doc_term_matrix = count_vectorizer.fit_transform(processed_documents)\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Document-Term Matrix Shape: {doc_term_matrix.shape}\")\n",
    "print(f\"  - {doc_term_matrix.shape[0]} documents\")\n",
    "print(f\"  - {doc_term_matrix.shape[1]} unique words (vocabulary)\")\n",
    "print(f\"\\nSparsity: {100 * (1 - doc_term_matrix.nnz / (doc_term_matrix.shape[0] * doc_term_matrix.shape[1])):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also create TF-IDF for NMF comparison\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=5,\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 1),\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_documents)\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Model Choice: LDA vs NMF vs K-Means\n",
    "\n",
    "| Model | Input | Assignment | Best For |\n",
    "|-------|-------|------------|----------|\n",
    "| **LDA** | Count matrix | Soft (probabilistic) | Mixed-topic documents |\n",
    "| **NMF** | TF-IDF matrix | Soft (non-negative weights) | Faster, good for short text |\n",
    "| **K-Means** | TF-IDF/embeddings | Hard (one cluster) | Document routing |\n",
    "\n",
    "**For Banking Complaints**: LDA is preferred because:\n",
    "- A single complaint often mentions multiple issues (app + fees + service)\n",
    "- Soft assignment reflects reality better\n",
    "- Probabilistic interpretation is useful for risk quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model\n",
    "N_TOPICS = 8  # We know there are 8 categories in our dataset\n",
    "\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=N_TOPICS,\n",
    "    max_iter=20,\n",
    "    learning_method='online',  # More scalable for large datasets\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    doc_topic_prior=0.1,       # Alpha - controls topic distribution sparsity\n",
    "    topic_word_prior=0.01,     # Beta - controls word distribution sparsity\n",
    ")\n",
    "\n",
    "print(\"Training LDA model...\")\n",
    "lda_model.fit(doc_term_matrix)\n",
    "print(f\"LDA training complete. Perplexity: {lda_model.perplexity(doc_term_matrix):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also train NMF for comparison\n",
    "nmf_model = NMF(\n",
    "    n_components=N_TOPICS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    max_iter=500,\n",
    "    init='nndsvd',  # Non-negative Double SVD - good initialization\n",
    ")\n",
    "\n",
    "print(\"Training NMF model...\")\n",
    "nmf_model.fit(tfidf_matrix)\n",
    "print(f\"NMF training complete. Reconstruction error: {nmf_model.reconstruction_err_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, n_top_words=10, title=\"Topics\"):\n",
    "    \"\"\"Display top words for each topic.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{title}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        print(f\"\\nTopic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "# Display LDA topics\n",
    "display_topics(lda_model, feature_names, n_top_words=10, title=\"LDA Topics\")\n",
    "\n",
    "# Display NMF topics\n",
    "display_topics(nmf_model, tfidf_vectorizer.get_feature_names_out(), n_top_words=10, title=\"NMF Topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Get topic distribution for a new document\n",
    "def predict_topics(text, model, vectorizer, top_n=3):\n",
    "    \"\"\"\n",
    "    Predict topic distribution for a new document.\n",
    "    This is the inference function you'd deploy in production.\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    processed = preprocessor.preprocess(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    vec = vectorizer.transform([processed])\n",
    "    \n",
    "    # Get topic distribution\n",
    "    topic_dist = model.transform(vec)[0]\n",
    "    \n",
    "    # Get top topics\n",
    "    top_topics = topic_dist.argsort()[::-1][:top_n]\n",
    "    \n",
    "    return {\n",
    "        'topic_distribution': topic_dist,\n",
    "        'top_topics': [(idx, topic_dist[idx]) for idx in top_topics]\n",
    "    }\n",
    "\n",
    "# Test inference\n",
    "test_doc = \"\"\"I'm having trouble with the mobile app crashing when I try to \n",
    "check my account balance. This has been happening for a week now and customer \n",
    "service hasn't been helpful. Very frustrated with the service quality.\"\"\"\n",
    "\n",
    "result = predict_topics(test_doc, lda_model, count_vectorizer)\n",
    "\n",
    "print(\"INFERENCE EXAMPLE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input: {test_doc[:100]}...\\n\")\n",
    "print(\"Top Topics:\")\n",
    "for topic_idx, prob in result['top_topics']:\n",
    "    print(f\"  Topic {topic_idx + 1}: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic distribution for sample documents\n",
    "doc_topics = lda_model.transform(doc_term_matrix)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    doc_idx = idx * 100  # Sample different documents\n",
    "    ax.bar(range(N_TOPICS), doc_topics[doc_idx])\n",
    "    ax.set_xlabel('Topic')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(f'Doc {doc_idx} - True Label: {category_names[filtered_labels[doc_idx]]}')\n",
    "    ax.set_xticks(range(N_TOPICS))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Strategy\n",
    "\n",
    "### Why Accuracy is NOT the Right Metric for Topic Modeling\n",
    "\n",
    "Topic modeling is **unsupervised** - there's no ground truth to measure accuracy against. Instead, we use:\n",
    "\n",
    "| Metric | What it Measures | Good Value |\n",
    "|--------|------------------|------------|\n",
    "| **Coherence** | Semantic similarity of top words in each topic | Higher is better (>0.4) |\n",
    "| **Perplexity** | How well model predicts held-out data | Lower is better |\n",
    "| **Human Evaluation** | Are topics interpretable? | Subjective |\n",
    "| **Downstream Task** | Does it improve classification? | Task-dependent |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence Score Calculation (simplified UMass coherence)\n",
    "def calculate_coherence(model, feature_names, doc_term_matrix, top_n=10):\n",
    "    \"\"\"\n",
    "    Calculate topic coherence using UMass method.\n",
    "    Coherence measures how often top words co-occur in the corpus.\n",
    "    \"\"\"\n",
    "    coherence_scores = []\n",
    "    \n",
    "    # Convert to dense for co-occurrence calculation\n",
    "    dtm_dense = doc_term_matrix.toarray()\n",
    "    \n",
    "    for topic in model.components_:\n",
    "        top_words_idx = topic.argsort()[:-top_n - 1:-1]\n",
    "        \n",
    "        # Calculate pairwise co-occurrence\n",
    "        score = 0\n",
    "        count = 0\n",
    "        \n",
    "        for i in range(len(top_words_idx)):\n",
    "            for j in range(i + 1, len(top_words_idx)):\n",
    "                w1, w2 = top_words_idx[i], top_words_idx[j]\n",
    "                \n",
    "                # Co-occurrence count\n",
    "                co_occur = np.sum((dtm_dense[:, w1] > 0) & (dtm_dense[:, w2] > 0))\n",
    "                w1_occur = np.sum(dtm_dense[:, w1] > 0)\n",
    "                \n",
    "                if w1_occur > 0:\n",
    "                    score += np.log((co_occur + 1) / w1_occur)\n",
    "                    count += 1\n",
    "        \n",
    "        if count > 0:\n",
    "            coherence_scores.append(score / count)\n",
    "    \n",
    "    return np.mean(coherence_scores)\n",
    "\n",
    "# Calculate coherence for LDA\n",
    "lda_coherence = calculate_coherence(lda_model, feature_names, doc_term_matrix)\n",
    "print(f\"LDA Coherence Score: {lda_coherence:.4f}\")\n",
    "\n",
    "# Calculate coherence for NMF\n",
    "nmf_coherence = calculate_coherence(nmf_model, tfidf_vectorizer.get_feature_names_out(), tfidf_matrix)\n",
    "print(f\"NMF Coherence Score: {nmf_coherence:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding optimal number of topics using coherence\n",
    "def find_optimal_topics(doc_term_matrix, feature_names, topic_range=(2, 15)):\n",
    "    \"\"\"\n",
    "    Find optimal number of topics using coherence score.\n",
    "    This is a key hyperparameter tuning step.\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    perplexity_values = []\n",
    "    \n",
    "    for n_topics in range(topic_range[0], topic_range[1] + 1):\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=n_topics,\n",
    "            max_iter=15,\n",
    "            learning_method='online',\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        lda.fit(doc_term_matrix)\n",
    "        \n",
    "        coherence = calculate_coherence(lda, feature_names, doc_term_matrix)\n",
    "        perplexity = lda.perplexity(doc_term_matrix)\n",
    "        \n",
    "        coherence_values.append(coherence)\n",
    "        perplexity_values.append(perplexity)\n",
    "        print(f\"Topics: {n_topics}, Coherence: {coherence:.4f}, Perplexity: {perplexity:.2f}\")\n",
    "    \n",
    "    return coherence_values, perplexity_values\n",
    "\n",
    "print(\"Finding optimal number of topics...\\n\")\n",
    "coherence_vals, perplexity_vals = find_optimal_topics(doc_term_matrix, feature_names, (4, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coherence vs number of topics\n",
    "topic_range = range(4, 13)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax1.set_xlabel('Number of Topics')\n",
    "ax1.set_ylabel('Coherence Score', color='blue')\n",
    "ax1.plot(topic_range, coherence_vals, 'b-o', label='Coherence')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Perplexity', color='red')\n",
    "ax2.plot(topic_range, perplexity_vals, 'r-s', label='Perplexity')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "plt.title('Coherence and Perplexity vs Number of Topics')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "optimal_topics = list(topic_range)[np.argmax(coherence_vals)]\n",
    "print(f\"\\nOptimal number of topics based on coherence: {optimal_topics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Readiness Checklist\n",
    "\n",
    "### Before deploying topic modeling in a banking environment:\n",
    "\n",
    "```\n",
    "DATA & PREPROCESSING\n",
    "[ ] PII detection and masking (account numbers, SSNs, names)\n",
    "[ ] Language detection (handle multilingual inputs)\n",
    "[ ] Document length validation (min/max thresholds)\n",
    "[ ] Encoding handling (UTF-8 normalization)\n",
    "[ ] Domain-specific stopwords list (banking jargon)\n",
    "\n",
    "MODEL ARTIFACTS\n",
    "[ ] Serialized model (joblib/pickle with version)\n",
    "[ ] Vectorizer saved with model (same vocabulary)\n",
    "[ ] Hyperparameters documented\n",
    "[ ] Training data statistics logged\n",
    "\n",
    "INFERENCE PIPELINE\n",
    "[ ] Batch inference support (nightly processing)\n",
    "[ ] Single-document API (real-time routing)\n",
    "[ ] Latency benchmarks (p50 < 100ms, p99 < 500ms)\n",
    "[ ] Error handling for empty/invalid documents\n",
    "\n",
    "MONITORING & DRIFT\n",
    "[ ] Topic distribution monitoring (detect new themes)\n",
    "[ ] Vocabulary drift detection (new words not in model)\n",
    "[ ] Document length distribution tracking\n",
    "[ ] Coherence score trending (model degradation)\n",
    "\n",
    "GOVERNANCE (BANKING-SPECIFIC)\n",
    "[ ] Model card with intended use\n",
    "[ ] Bias assessment (are certain demographics over-represented?)\n",
    "[ ] Audit trail for model decisions\n",
    "[ ] Version control with rollback capability\n",
    "[ ] Regulatory documentation (SR 11-7 compliance)\n",
    "\n",
    "FAILURE MODES\n",
    "[ ] What happens with very short documents (<10 words)?\n",
    "[ ] What if a new topic emerges not in training?\n",
    "[ ] How to handle documents in foreign languages?\n",
    "[ ] What's the fallback if model service is down?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready model serialization\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "def save_model_artifacts(model, vectorizer, preprocessor, model_name=\"topic_model\"):\n",
    "    \"\"\"\n",
    "    Save model artifacts for production deployment.\n",
    "    Includes metadata for audit trail.\n",
    "    \"\"\"\n",
    "    artifacts = {\n",
    "        'model': model,\n",
    "        'vectorizer': vectorizer,\n",
    "        'preprocessor_config': {\n",
    "            'remove_numbers': preprocessor.remove_numbers,\n",
    "            'min_word_length': preprocessor.min_word_length,\n",
    "            'stop_words': list(preprocessor.stop_words)\n",
    "        },\n",
    "        'metadata': {\n",
    "            'trained_at': datetime.now().isoformat(),\n",
    "            'n_topics': model.n_components,\n",
    "            'n_documents': doc_term_matrix.shape[0],\n",
    "            'vocabulary_size': len(vectorizer.get_feature_names_out()),\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'sklearn_version': '1.3.0'  # Track library version\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # In production, save to model registry (MLflow, SageMaker, etc.)\n",
    "    # joblib.dump(artifacts, f'{model_name}_v1.joblib')\n",
    "    \n",
    "    print(\"Model Artifacts Ready for Deployment:\")\n",
    "    print(f\"  - Trained: {artifacts['metadata']['trained_at']}\")\n",
    "    print(f\"  - Topics: {artifacts['metadata']['n_topics']}\")\n",
    "    print(f\"  - Documents: {artifacts['metadata']['n_documents']}\")\n",
    "    print(f\"  - Vocabulary: {artifacts['metadata']['vocabulary_size']} words\")\n",
    "    \n",
    "    return artifacts\n",
    "\n",
    "artifacts = save_model_artifacts(lda_model, count_vectorizer, preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Modern LLM-Based Approach\n",
    "\n",
    "### How would we solve this with LLMs today?\n",
    "\n",
    "**Option 1: BERTopic (Transformer-based Topic Modeling)**\n",
    "```python\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# BERTopic uses:\n",
    "# 1. Sentence embeddings (BERT/all-MiniLM)\n",
    "# 2. UMAP for dimensionality reduction\n",
    "# 3. HDBSCAN for clustering\n",
    "# 4. c-TF-IDF for topic representation\n",
    "\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(documents)\n",
    "```\n",
    "\n",
    "**Option 2: Zero-Shot Topic Classification**\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Pre-define candidate topics (requires domain knowledge)\n",
    "candidate_labels = [\"mobile app issues\", \"fee complaints\", \"service quality\", ...]\n",
    "\n",
    "result = classifier(document, candidate_labels)\n",
    "```\n",
    "\n",
    "**Option 3: LLM Topic Extraction (GPT-4/Claude)**\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "Analyze the following customer complaints and identify the main topics.\n",
    "Return a list of topics with descriptions.\n",
    "\n",
    "Complaints:\n",
    "{documents}\n",
    "\n",
    "Topics:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### When LLMs Win vs. When Traditional Wins\n",
    "\n",
    "| Scenario | Winner | Why |\n",
    "|----------|--------|-----|\n",
    "| Short documents (<50 words) | LLMs | Better semantic understanding |\n",
    "| Millions of documents | Traditional | Cost and latency |\n",
    "| Need interpretability | Traditional | Clear word distributions |\n",
    "| New domain, no labels | LLMs | Zero-shot capability |\n",
    "| Regulatory audit | Traditional | Deterministic, explainable |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for LLM-based topic extraction\n",
    "# (Would require API key in production)\n",
    "\n",
    "def llm_topic_extraction_prompt(documents, n_topics=8):\n",
    "    \"\"\"\n",
    "    Example prompt for LLM-based topic discovery.\n",
    "    \n",
    "    In production at JPMorgan, this would go through:\n",
    "    - Data Loss Prevention (DLP) scan\n",
    "    - PII masking before sending to external API\n",
    "    - Approved vendor (Azure OpenAI, not consumer API)\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are analyzing customer feedback for a retail bank.\n",
    "\n",
    "Task: Identify the {n_topics} main topics across these documents.\n",
    "\n",
    "For each topic, provide:\n",
    "1. A clear topic name (2-4 words)\n",
    "2. A brief description (1 sentence)\n",
    "3. 3 example keywords\n",
    "\n",
    "Documents (sample):\n",
    "---\n",
    "{chr(10).join(documents[:5])}\n",
    "---\n",
    "\n",
    "Output format:\n",
    "Topic 1: [Name]\n",
    "Description: [Brief description]\n",
    "Keywords: [word1, word2, word3]\n",
    "\n",
    "Topics:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "print(\"Example LLM Prompt:\")\n",
    "print(\"=\"*50)\n",
    "print(llm_topic_extraction_prompt(processed_documents[:5])[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Traditional vs LLM Decision Matrix\n",
    "\n",
    "| Dimension | Traditional (LDA/NMF) | LLM-Based (BERTopic/GPT) | Banking Consideration |\n",
    "|-----------|----------------------|--------------------------|----------------------|\n",
    "| **Accuracy** | Good for long docs | Better for short/noisy | Customer complaints vary |\n",
    "| **Cost** | ~$0 (local compute) | $0.001-$0.01 per doc | At 1M docs/month = $10K |\n",
    "| **Latency** | <100ms per doc | 200ms-2s per doc | Real-time routing needs speed |\n",
    "| **Explainability** | High (word distributions) | Medium (embeddings opaque) | Regulators require explanations |\n",
    "| **Compliance** | Easy (on-premise) | Complex (data residency) | PII cannot leave jurisdiction |\n",
    "| **Data Requirements** | Thousands of docs | Works with dozens | Cold start advantage for LLMs |\n",
    "| **Maintenance** | Retrain periodically | Prompt updates only | Lower operational burden for LLMs |\n",
    "| **Drift Handling** | Manual retraining | Continuous adaptation | LLMs better for evolving topics |\n",
    "\n",
    "### My Recommendation for Banking\n",
    "\n",
    "**Hybrid Approach**:\n",
    "1. Use **traditional LDA** for high-volume batch processing (nightly analysis)\n",
    "2. Use **LLM zero-shot** for real-time escalation detection (low volume, high stakes)\n",
    "3. Use **BERTopic** for quarterly deep-dives (exploratory analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interview Soundbites\n",
    "\n",
    "### Ready-to-Say Statements\n",
    "\n",
    "**On Model Choice:**\n",
    "> \"I would use LDA over K-means for customer complaints because a single complaint often touches multiple themes - someone might mention both the mobile app crashing AND poor customer service. LDA's soft assignment reflects this reality.\"\n",
    "\n",
    "**On Evaluation:**\n",
    "> \"For topic modeling, accuracy doesn't exist because we have no ground truth. Instead, I look at coherence scores to measure topic quality quantitatively, but ultimately the business test is: can a product manager look at these topics and take action?\"\n",
    "\n",
    "**On Hyperparameters:**\n",
    "> \"The number of topics is the hardest hyperparameter. I use coherence scores as a guide, but there's always a tradeoff - too few topics and you miss nuance, too many and topics become redundant. In practice, I iterate with stakeholders.\"\n",
    "\n",
    "**On When NOT to Use LLMs:**\n",
    "> \"I would not use GPT-4 for topic modeling on 1 million customer complaints. At $0.01 per document, that's $10,000 per run. LDA achieves 80% of the quality at 0.01% of the cost. I'd save the LLM budget for high-stakes, low-volume use cases.\"\n",
    "\n",
    "**On Production Failures:**\n",
    "> \"Topic models fail silently when vocabulary drifts. If customers start using 'Zelle' instead of 'transfer,' and 'Zelle' isn't in the vocabulary, the model just ignores it. We need vocabulary monitoring in production.\"\n",
    "\n",
    "**On Regulatory Considerations:**\n",
    "> \"In banking, I need to explain why a complaint was routed to a specific queue. LDA gives me that - I can say 'this document has 65% probability of being about fee disputes based on words like refund, charge, and statement.' That's audit-friendly.\"\n",
    "\n",
    "**On Short Text Problem:**\n",
    "> \"LDA struggles with tweets or short survey responses because there aren't enough words to estimate topic distributions reliably. For short text, I'd consider BERTopic which uses semantic embeddings, or aggregate documents before modeling.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Common Interview Questions\n",
    "\n",
    "**Q: How do you choose the number of topics?**\n",
    "> Use coherence scores + business intuition. Start with domain knowledge (how many complaint categories exist?), then validate with coherence. But remember: the optimal number mathematically may not be optimal for the business.\n",
    "\n",
    "**Q: What's the difference between LDA and NMF?**\n",
    "> LDA is probabilistic (topics are distributions over words), NMF is algebraic (matrix factorization). LDA has better theoretical grounding for text, NMF is faster and often better for short documents. I'd try both and compare coherence.\n",
    "\n",
    "**Q: How do you handle new topics emerging over time?**\n",
    "> This is the hardest problem. Options: (1) periodic retraining with recent data, (2) dynamic topic models that evolve, (3) monitoring topic distributions and alerting on outliers. In production, I'd combine option 1 with option 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                    NOTEBOOK SUMMARY                               ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║  Task: Topic Modeling / Clustering                               ║\n",
    "║  Approach: Traditional NLP (LDA, NMF)                            ║\n",
    "║  Banking Use: Customer complaint analysis                        ║\n",
    "║                                                                  ║\n",
    "║  Key Takeaways:                                                  ║\n",
    "║  1. LDA provides soft topic assignments - matches real feedback  ║\n",
    "║  2. Coherence > Perplexity for evaluation                        ║\n",
    "║  3. Traditional beats LLMs on cost at scale                      ║\n",
    "║  4. Explainability critical for banking compliance               ║\n",
    "║  5. Monitor vocabulary drift in production                       ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization - Traditional NLP\n",
    "## Interview Preparation Notebook for Senior Applied AI Scientist (Retail Banking)\n",
    "\n",
    "---\n",
    "\n",
    "**Goal**: Demonstrate mastery of extractive summarization techniques, with emphasis on faithfulness, production considerations, and banking document requirements.\n",
    "\n",
    "**Interview Signal**: This notebook shows you can condense long documents while maintaining accuracy - critical for compliance and executive reporting in banking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Context (Banking Lens)\n",
    "\n",
    "### Why Text Summarization Exists in Retail Banking\n",
    "\n",
    "Banking generates enormous volumes of long-form text that needs to be digested quickly by decision-makers, analysts, and regulators.\n",
    "\n",
    "| Use Case | Input | Output | Business Value |\n",
    "|----------|-------|--------|---------------|\n",
    "| **Earnings Call Digests** | 60-page transcript | 1-page summary | Analyst efficiency |\n",
    "| **Regulatory Filing Summaries** | 100+ page 10-K | Key risk highlights | Compliance monitoring |\n",
    "| **Customer Interaction Summaries** | 30-min call transcript | 3-sentence recap | CRM updates |\n",
    "| **Contract Clause Extraction** | 50-page agreement | Key terms summary | Risk assessment |\n",
    "| **Complaint Aggregation** | 1000 complaints | Theme summary | Executive dashboards |\n",
    "\n",
    "### The Business Problem\n",
    "\n",
    "> \"Our risk team reviews 500 regulatory filings per week. How do we help them focus on what matters?\"\n",
    "\n",
    "**Without summarization**: Analysts skim documents, miss critical information  \n",
    "**With summarization**: Key information surfaced, consistent coverage, audit trail\n",
    "\n",
    "### Real Banking Example\n",
    "\n",
    "**Input** (earnings call excerpt, 500 words):\n",
    "\"In Q3, we saw continued strength in our consumer banking segment with deposits growing 12% year-over-year. Net interest income increased by $340 million, driven primarily by higher interest rates. However, provision for credit losses increased to $1.2 billion, reflecting our conservative approach to the uncertain macroeconomic environment. On the commercial side, we saw some softening in loan demand, particularly in commercial real estate where we've maintained disciplined underwriting standards...\"\n",
    "\n",
    "**Summary** (50 words):\n",
    "\"Q3 showed strong consumer banking with 12% deposit growth and $340M NII increase from higher rates. Credit provisions rose to $1.2B due to macro uncertainty. Commercial lending softened, especially in CRE, with disciplined underwriting maintained.\"\n",
    "\n",
    "### Interview Framing\n",
    "\n",
    "```\n",
    "\"Summarization in banking requires a different mindset than news summarization. We can't \n",
    "afford hallucinations - if a summary says the provision for credit losses is $1.5B when \n",
    "the actual number is $1.2B, that's a compliance failure. That's why I prefer extractive \n",
    "methods for factual documents and reserve abstractive approaches for less critical use cases.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem Definition\n",
    "\n",
    "### Task Type: Document Compression\n",
    "\n",
    "| Aspect | Description |\n",
    "|--------|-------------|\n",
    "| **Input** | Long document (100-10,000+ words) |\n",
    "| **Output** | Shorter summary (10-500 words) |\n",
    "| **Key Metric** | Compression ratio + information retention |\n",
    "| **Core Challenge** | What to keep vs. what to drop |\n",
    "\n",
    "### Extractive vs. Abstractive Summarization\n",
    "\n",
    "| Approach | Method | Faithfulness | Fluency | Banking Use |\n",
    "|----------|--------|--------------|---------|-------------|\n",
    "| **Extractive** | Select existing sentences | High (verbatim) | Lower | Regulatory docs, contracts |\n",
    "| **Abstractive** | Generate new text | Risk of errors | Higher | Executive briefings |\n",
    "\n",
    "### Why Extractive Before LLMs\n",
    "\n",
    "1. **No hallucination risk**: Selected sentences exist in source\n",
    "2. **Attributable**: Can point to exact source location\n",
    "3. **Deterministic**: Same input → same output\n",
    "4. **Fast**: No generation, just selection\n",
    "5. **Works without training data**: Unsupervised methods available\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "**Sentence Selection Problem**:\n",
    "Given document $D = \\{s_1, s_2, ..., s_n\\}$ (sentences), select subset $S \\subset D$ that:\n",
    "- Maximizes coverage: $\\text{Coverage}(S, D)$\n",
    "- Minimizes redundancy: $\\text{Redundancy}(S)$\n",
    "- Satisfies length constraint: $|S| \\leq k$ sentences or $\\sum_{s \\in S} |s| \\leq L$ words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset\n",
    "\n",
    "### Dataset Selection\n",
    "\n",
    "For this demo, we'll create sample financial documents. In practice, you'd use:\n",
    "- **CNN/DailyMail**: News articles with highlights\n",
    "- **XSum**: Extreme summarization (single sentence)\n",
    "- **Financial reports**: SEC filings, earnings transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install scikit-learn nltk networkx numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# For TextRank\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample banking documents for summarization\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"title\": \"Q3 2024 Earnings Call Transcript (Excerpt)\",\n",
    "        \"text\": \"\"\"Good morning and welcome to our third quarter 2024 earnings call. \n",
    "        \n",
    "I'm pleased to report that we delivered strong results this quarter with revenue of $32.4 billion, up 8% year-over-year. Our consumer banking division showed particular strength with deposits growing 12% compared to the same period last year. This growth was driven by successful marketing campaigns and competitive interest rates on savings products.\n",
    "\n",
    "Net interest income increased by $340 million to reach $14.2 billion, primarily due to higher interest rates. Our net interest margin expanded by 15 basis points to 2.85%. We continue to benefit from the rate environment while managing our funding costs carefully.\n",
    "\n",
    "However, we did increase our provision for credit losses to $1.2 billion this quarter, up from $800 million in Q2. This reflects our conservative approach to the uncertain macroeconomic environment. We are seeing some early signs of stress in our credit card portfolio, particularly in the subprime segment, but overall credit quality remains strong.\n",
    "\n",
    "On the commercial banking side, we saw some softening in loan demand, especially in commercial real estate. CRE loans declined by 3% as we maintained our disciplined underwriting standards. We remain cautious about office space lending given the ongoing shift to hybrid work arrangements.\n",
    "\n",
    "Our wealth management division delivered record revenue of $5.8 billion, driven by strong asset inflows and higher transaction volumes. Assets under management grew to $4.2 trillion, up 15% year-over-year.\n",
    "\n",
    "Looking ahead, we expect continued pressure on net interest margin as deposit competition intensifies. We are investing heavily in digital capabilities to improve customer experience and reduce operating costs. Our technology spend will increase by 12% next year.\n",
    "\n",
    "We remain committed to returning capital to shareholders. This quarter, we repurchased $2.5 billion of common stock and paid dividends of $1.1 billion. Our CET1 ratio stands at 12.8%, well above regulatory requirements.\n",
    "\n",
    "In summary, we delivered solid results despite a challenging environment. Our diversified business model continues to serve us well, and we are well positioned for the quarters ahead.\"\"\",\n",
    "        \"type\": \"earnings_call\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Risk Management Policy Update\",\n",
    "        \"text\": \"\"\"This document outlines updates to our enterprise risk management framework effective January 1, 2025.\n",
    "\n",
    "The Board of Directors has approved several enhancements to our risk appetite statement. Our credit risk appetite for commercial real estate has been reduced from 15% to 12% of total loans. This change reflects current market conditions and our assessment of sector-specific risks.\n",
    "\n",
    "Market risk limits have been updated to reflect increased volatility in interest rate markets. The Value-at-Risk limit for the trading book has been increased from $50 million to $65 million to accommodate larger hedging positions. However, stress testing requirements have been enhanced with three additional scenarios.\n",
    "\n",
    "Operational risk management procedures have been strengthened following recent industry incidents. All critical systems must now have recovery time objectives of less than four hours, reduced from the previous eight-hour standard. Cyber security investments will increase by 25% in the coming year.\n",
    "\n",
    "Model risk governance has been enhanced with the creation of a dedicated Model Validation team reporting directly to the Chief Risk Officer. All pricing and credit risk models must be independently validated annually, with challenger models required for Tier 1 applications.\n",
    "\n",
    "Liquidity risk management now requires maintaining a liquidity coverage ratio of 120%, up from the regulatory minimum of 100%. Additional stress scenarios for deposit outflows have been implemented based on recent regional bank experiences.\n",
    "\n",
    "Compliance risk procedures have been updated to address new regulatory requirements. Anti-money laundering transaction monitoring thresholds have been lowered, and suspicious activity report filing timelines have been shortened from 30 to 15 days.\n",
    "\n",
    "The Risk Committee will review these policies quarterly and report to the Board on implementation progress.\"\"\",\n",
    "        \"type\": \"policy\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(sample_documents)} sample documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze documents\n",
    "for doc in sample_documents:\n",
    "    sentences = sent_tokenize(doc['text'])\n",
    "    words = word_tokenize(doc['text'])\n",
    "    print(f\"\\nDocument: {doc['title']}\")\n",
    "    print(f\"  Sentences: {len(sentences)}\")\n",
    "    print(f\"  Words: {len(words)}\")\n",
    "    print(f\"  Avg words/sentence: {len(words)/len(sentences):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Traditional NLP Pipeline\n",
    "\n",
    "### 4.1 Text Preprocessing for Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocessor for extractive summarization.\n",
    "    \n",
    "    Key considerations:\n",
    "    - Sentence boundaries must be preserved (we select sentences)\n",
    "    - Keep structure for ranking, but normalize for comparison\n",
    "    - Handle financial text patterns (numbers, percentages)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def split_sentences(self, text):\n",
    "        \"\"\"Split text into sentences while handling edge cases.\"\"\"\n",
    "        # Clean up whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        # Use NLTK sentence tokenizer\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        # Filter very short sentences\n",
    "        sentences = [s.strip() for s in sentences if len(s.split()) >= 5]\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def normalize_sentence(self, sentence):\n",
    "        \"\"\"Normalize sentence for comparison (lowercase, remove stopwords).\"\"\"\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        words = [w for w in words if w.isalnum() and w not in self.stop_words]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def preprocess_document(self, text):\n",
    "        \"\"\"Preprocess document for summarization.\"\"\"\n",
    "        sentences = self.split_sentences(text)\n",
    "        normalized = [self.normalize_sentence(s) for s in sentences]\n",
    "        \n",
    "        return {\n",
    "            'original_sentences': sentences,\n",
    "            'normalized_sentences': normalized,\n",
    "            'num_sentences': len(sentences)\n",
    "        }\n",
    "\n",
    "preprocessor = SummarizationPreprocessor()\n",
    "\n",
    "# Demo\n",
    "doc = sample_documents[0]\n",
    "processed = preprocessor.preprocess_document(doc['text'])\n",
    "\n",
    "print(f\"PREPROCESSING DEMO\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Original sentences: {processed['num_sentences']}\")\n",
    "print(f\"\\nFirst sentence (original):\")\n",
    "print(f\"  {processed['original_sentences'][0]}\")\n",
    "print(f\"\\nFirst sentence (normalized):\")\n",
    "print(f\"  {processed['normalized_sentences'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Extractive Summarization Methods\n",
    "\n",
    "#### Method 1: TF-IDF Based Scoring\n",
    "\n",
    "**Intuition**: Important sentences contain important words (high TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFSummarizer:\n",
    "    \"\"\"\n",
    "    TF-IDF based extractive summarizer.\n",
    "    \n",
    "    Sentence importance = sum of TF-IDF scores of words in sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        self.preprocessor = SummarizationPreprocessor()\n",
    "    \n",
    "    def summarize(self, text, num_sentences=3):\n",
    "        \"\"\"Extract top sentences based on TF-IDF scores.\"\"\"\n",
    "        processed = self.preprocessor.preprocess_document(text)\n",
    "        original_sentences = processed['original_sentences']\n",
    "        \n",
    "        if len(original_sentences) <= num_sentences:\n",
    "            return ' '.join(original_sentences)\n",
    "        \n",
    "        # Calculate TF-IDF matrix\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(original_sentences)\n",
    "        \n",
    "        # Score each sentence by sum of TF-IDF values\n",
    "        sentence_scores = np.asarray(tfidf_matrix.sum(axis=1)).flatten()\n",
    "        \n",
    "        # Get top sentence indices (maintain original order)\n",
    "        top_indices = sentence_scores.argsort()[-num_sentences:][::-1]\n",
    "        top_indices = sorted(top_indices)  # Maintain document order\n",
    "        \n",
    "        # Extract sentences\n",
    "        summary_sentences = [original_sentences[i] for i in top_indices]\n",
    "        \n",
    "        return {\n",
    "            'summary': ' '.join(summary_sentences),\n",
    "            'selected_indices': top_indices,\n",
    "            'scores': [(i, sentence_scores[i]) for i in top_indices]\n",
    "        }\n",
    "\n",
    "# Test TF-IDF summarizer\n",
    "tfidf_summarizer = TFIDFSummarizer()\n",
    "\n",
    "result = tfidf_summarizer.summarize(sample_documents[0]['text'], num_sentences=4)\n",
    "\n",
    "print(\"TF-IDF SUMMARIZATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSummary ({len(result['summary'].split())} words):\")\n",
    "print(result['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: TextRank (Graph-Based)\n",
    "\n",
    "**Intuition**: Important sentences are similar to many other important sentences (like PageRank for web pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRankSummarizer:\n",
    "    \"\"\"\n",
    "    TextRank algorithm for extractive summarization.\n",
    "    \n",
    "    Based on the PageRank algorithm:\n",
    "    1. Build similarity graph between sentences\n",
    "    2. Run PageRank to find important sentences\n",
    "    3. Select top-ranked sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, similarity_threshold=0.1):\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        self.preprocessor = SummarizationPreprocessor()\n",
    "    \n",
    "    def build_similarity_matrix(self, sentences):\n",
    "        \"\"\"Build sentence similarity matrix using cosine similarity.\"\"\"\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(sentences)\n",
    "        similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "        \n",
    "        # Apply threshold to create sparse graph\n",
    "        similarity_matrix[similarity_matrix < self.similarity_threshold] = 0\n",
    "        \n",
    "        # Remove self-loops\n",
    "        np.fill_diagonal(similarity_matrix, 0)\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def summarize(self, text, num_sentences=3):\n",
    "        \"\"\"Extract top sentences using TextRank.\"\"\"\n",
    "        processed = self.preprocessor.preprocess_document(text)\n",
    "        original_sentences = processed['original_sentences']\n",
    "        \n",
    "        if len(original_sentences) <= num_sentences:\n",
    "            return ' '.join(original_sentences)\n",
    "        \n",
    "        # Build similarity matrix\n",
    "        similarity_matrix = self.build_similarity_matrix(original_sentences)\n",
    "        \n",
    "        # Create graph and run PageRank\n",
    "        graph = nx.from_numpy_array(similarity_matrix)\n",
    "        scores = nx.pagerank(graph, max_iter=100)\n",
    "        \n",
    "        # Convert to array\n",
    "        sentence_scores = np.array([scores.get(i, 0) for i in range(len(original_sentences))])\n",
    "        \n",
    "        # Get top sentence indices (maintain original order)\n",
    "        top_indices = sentence_scores.argsort()[-num_sentences:][::-1]\n",
    "        top_indices = sorted(top_indices)\n",
    "        \n",
    "        # Extract sentences\n",
    "        summary_sentences = [original_sentences[i] for i in top_indices]\n",
    "        \n",
    "        return {\n",
    "            'summary': ' '.join(summary_sentences),\n",
    "            'selected_indices': top_indices,\n",
    "            'scores': [(i, sentence_scores[i]) for i in top_indices]\n",
    "        }\n",
    "\n",
    "# Test TextRank summarizer\n",
    "textrank_summarizer = TextRankSummarizer(similarity_threshold=0.1)\n",
    "\n",
    "result = textrank_summarizer.summarize(sample_documents[0]['text'], num_sentences=4)\n",
    "\n",
    "print(\"TEXTRANK SUMMARIZATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSummary ({len(result['summary'].split())} words):\")\n",
    "print(result['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3: Lead Bias (Position-Based)\n",
    "\n",
    "**Intuition**: Important information often appears at the beginning (news articles, executive summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeadSummarizer:\n",
    "    \"\"\"\n",
    "    Lead-based summarizer with position decay.\n",
    "    \n",
    "    Combines content importance with position bias:\n",
    "    Score = content_score * position_weight\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, position_decay=0.9):\n",
    "        self.position_decay = position_decay\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        self.preprocessor = SummarizationPreprocessor()\n",
    "    \n",
    "    def summarize(self, text, num_sentences=3):\n",
    "        \"\"\"Extract sentences with position bias.\"\"\"\n",
    "        processed = self.preprocessor.preprocess_document(text)\n",
    "        original_sentences = processed['original_sentences']\n",
    "        \n",
    "        if len(original_sentences) <= num_sentences:\n",
    "            return ' '.join(original_sentences)\n",
    "        \n",
    "        # Calculate content scores (TF-IDF)\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(original_sentences)\n",
    "        content_scores = np.asarray(tfidf_matrix.sum(axis=1)).flatten()\n",
    "        content_scores = content_scores / content_scores.max()  # Normalize\n",
    "        \n",
    "        # Calculate position weights (exponential decay)\n",
    "        position_weights = np.array([\n",
    "            self.position_decay ** i \n",
    "            for i in range(len(original_sentences))\n",
    "        ])\n",
    "        \n",
    "        # Combined scores\n",
    "        combined_scores = content_scores * position_weights\n",
    "        \n",
    "        # Get top sentence indices\n",
    "        top_indices = combined_scores.argsort()[-num_sentences:][::-1]\n",
    "        top_indices = sorted(top_indices)\n",
    "        \n",
    "        summary_sentences = [original_sentences[i] for i in top_indices]\n",
    "        \n",
    "        return {\n",
    "            'summary': ' '.join(summary_sentences),\n",
    "            'selected_indices': top_indices,\n",
    "            'scores': [(i, combined_scores[i]) for i in top_indices]\n",
    "        }\n",
    "\n",
    "# Test Lead summarizer\n",
    "lead_summarizer = LeadSummarizer(position_decay=0.95)\n",
    "\n",
    "result = lead_summarizer.summarize(sample_documents[0]['text'], num_sentences=4)\n",
    "\n",
    "print(\"LEAD-BIASED SUMMARIZATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSummary ({len(result['summary'].split())} words):\")\n",
    "print(result['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 4: MMR (Maximal Marginal Relevance)\n",
    "\n",
    "**Intuition**: Balance relevance with diversity - don't select redundant sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMRSummarizer:\n",
    "    \"\"\"\n",
    "    Maximal Marginal Relevance summarizer.\n",
    "    \n",
    "    MMR = λ * Sim(s, query) - (1-λ) * max(Sim(s, s_selected))\n",
    "    \n",
    "    Balances:\n",
    "    - Relevance to document (coverage)\n",
    "    - Novelty compared to already selected sentences (diversity)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambda_param=0.7):\n",
    "        self.lambda_param = lambda_param\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        self.preprocessor = SummarizationPreprocessor()\n",
    "    \n",
    "    def summarize(self, text, num_sentences=3):\n",
    "        \"\"\"Extract sentences using MMR.\"\"\"\n",
    "        processed = self.preprocessor.preprocess_document(text)\n",
    "        original_sentences = processed['original_sentences']\n",
    "        \n",
    "        if len(original_sentences) <= num_sentences:\n",
    "            return ' '.join(original_sentences)\n",
    "        \n",
    "        # Calculate TF-IDF representations\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(original_sentences)\n",
    "        \n",
    "        # Document centroid (query proxy)\n",
    "        doc_centroid = tfidf_matrix.mean(axis=0)\n",
    "        \n",
    "        # Similarity to document\n",
    "        doc_similarity = cosine_similarity(tfidf_matrix, doc_centroid).flatten()\n",
    "        \n",
    "        # Pairwise sentence similarity\n",
    "        sentence_similarity = cosine_similarity(tfidf_matrix)\n",
    "        \n",
    "        # Greedy MMR selection\n",
    "        selected_indices = []\n",
    "        remaining_indices = list(range(len(original_sentences)))\n",
    "        \n",
    "        for _ in range(num_sentences):\n",
    "            best_idx = None\n",
    "            best_score = float('-inf')\n",
    "            \n",
    "            for idx in remaining_indices:\n",
    "                # Relevance to document\n",
    "                relevance = doc_similarity[idx]\n",
    "                \n",
    "                # Maximum similarity to already selected\n",
    "                if selected_indices:\n",
    "                    redundancy = max(sentence_similarity[idx][j] for j in selected_indices)\n",
    "                else:\n",
    "                    redundancy = 0\n",
    "                \n",
    "                # MMR score\n",
    "                mmr_score = self.lambda_param * relevance - (1 - self.lambda_param) * redundancy\n",
    "                \n",
    "                if mmr_score > best_score:\n",
    "                    best_score = mmr_score\n",
    "                    best_idx = idx\n",
    "            \n",
    "            if best_idx is not None:\n",
    "                selected_indices.append(best_idx)\n",
    "                remaining_indices.remove(best_idx)\n",
    "        \n",
    "        # Sort to maintain document order\n",
    "        selected_indices = sorted(selected_indices)\n",
    "        summary_sentences = [original_sentences[i] for i in selected_indices]\n",
    "        \n",
    "        return {\n",
    "            'summary': ' '.join(summary_sentences),\n",
    "            'selected_indices': selected_indices\n",
    "        }\n",
    "\n",
    "# Test MMR summarizer\n",
    "mmr_summarizer = MMRSummarizer(lambda_param=0.7)\n",
    "\n",
    "result = mmr_summarizer.summarize(sample_documents[0]['text'], num_sentences=4)\n",
    "\n",
    "print(\"MMR SUMMARIZATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSummary ({len(result['summary'].split())} words):\")\n",
    "print(result['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods on the same document\n",
    "def compare_summarizers(text, num_sentences=4):\n",
    "    \"\"\"Compare all summarization methods.\"\"\"\n",
    "    \n",
    "    summarizers = {\n",
    "        'TF-IDF': TFIDFSummarizer(),\n",
    "        'TextRank': TextRankSummarizer(similarity_threshold=0.1),\n",
    "        'Lead-Biased': LeadSummarizer(position_decay=0.95),\n",
    "        'MMR': MMRSummarizer(lambda_param=0.7)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, summarizer in summarizers.items():\n",
    "        result = summarizer.summarize(text, num_sentences)\n",
    "        results[name] = {\n",
    "            'summary': result['summary'],\n",
    "            'word_count': len(result['summary'].split()),\n",
    "            'selected_indices': result['selected_indices']\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare on earnings call\n",
    "doc = sample_documents[0]\n",
    "original_word_count = len(doc['text'].split())\n",
    "\n",
    "print(f\"SUMMARIZATION COMPARISON\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Document: {doc['title']}\")\n",
    "print(f\"Original length: {original_word_count} words\")\n",
    "print(f\"Target: 4 sentences\\n\")\n",
    "\n",
    "results = compare_summarizers(doc['text'], num_sentences=4)\n",
    "\n",
    "for name, result in results.items():\n",
    "    compression = 100 * (1 - result['word_count'] / original_word_count)\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(f\"Sentences selected: {result['selected_indices']}\")\n",
    "    print(f\"Words: {result['word_count']} (compression: {compression:.1f}%)\")\n",
    "    print(f\"Summary: {result['summary'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production summarization pipeline\n",
    "class ProductionSummarizer:\n",
    "    \"\"\"\n",
    "    Production-ready summarizer with multiple strategy options.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, default_method='mmr'):\n",
    "        self.methods = {\n",
    "            'tfidf': TFIDFSummarizer(),\n",
    "            'textrank': TextRankSummarizer(),\n",
    "            'lead': LeadSummarizer(),\n",
    "            'mmr': MMRSummarizer()\n",
    "        }\n",
    "        self.default_method = default_method\n",
    "        self.preprocessor = SummarizationPreprocessor()\n",
    "    \n",
    "    def summarize(self, text, target_words=None, target_sentences=None, \n",
    "                  method=None, min_sentence_length=10):\n",
    "        \"\"\"\n",
    "        Generate summary with specified constraints.\n",
    "        \n",
    "        Args:\n",
    "            text: Input document\n",
    "            target_words: Approximate word count target\n",
    "            target_sentences: Number of sentences to select\n",
    "            method: Summarization method to use\n",
    "            min_sentence_length: Minimum words per sentence\n",
    "        \"\"\"\n",
    "        method = method or self.default_method\n",
    "        \n",
    "        # Validate input\n",
    "        if not text or len(text.strip()) < 50:\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'message': 'Input text too short'\n",
    "            }\n",
    "        \n",
    "        # Preprocess to get sentence count\n",
    "        processed = self.preprocessor.preprocess_document(text)\n",
    "        total_sentences = processed['num_sentences']\n",
    "        total_words = len(text.split())\n",
    "        \n",
    "        # Determine number of sentences to extract\n",
    "        if target_words:\n",
    "            # Estimate based on average sentence length\n",
    "            avg_sentence_length = total_words / total_sentences\n",
    "            num_sentences = max(1, int(target_words / avg_sentence_length))\n",
    "        elif target_sentences:\n",
    "            num_sentences = target_sentences\n",
    "        else:\n",
    "            # Default: ~30% compression\n",
    "            num_sentences = max(1, int(total_sentences * 0.3))\n",
    "        \n",
    "        # Cap at reasonable limits\n",
    "        num_sentences = min(num_sentences, total_sentences - 1)\n",
    "        num_sentences = max(1, num_sentences)\n",
    "        \n",
    "        # Generate summary\n",
    "        summarizer = self.methods[method]\n",
    "        result = summarizer.summarize(text, num_sentences)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        summary_words = len(result['summary'].split())\n",
    "        compression_ratio = 1 - (summary_words / total_words)\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'summary': result['summary'],\n",
    "            'method': method,\n",
    "            'metrics': {\n",
    "                'original_words': total_words,\n",
    "                'summary_words': summary_words,\n",
    "                'compression_ratio': compression_ratio,\n",
    "                'sentences_selected': len(result['selected_indices']),\n",
    "                'sentences_total': total_sentences\n",
    "            },\n",
    "            'source_indices': result['selected_indices']\n",
    "        }\n",
    "\n",
    "# Test production summarizer\n",
    "prod_summarizer = ProductionSummarizer(default_method='mmr')\n",
    "\n",
    "result = prod_summarizer.summarize(\n",
    "    sample_documents[0]['text'],\n",
    "    target_words=100,\n",
    "    method='mmr'\n",
    ")\n",
    "\n",
    "print(\"PRODUCTION SUMMARIZER OUTPUT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Status: {result['status']}\")\n",
    "print(f\"Method: {result['method']}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "for key, value in result['metrics'].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2%}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "print(f\"\\nSummary:\\n{result['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Strategy\n",
    "\n",
    "### Why Standard Metrics Are Tricky for Summarization\n",
    "\n",
    "Unlike classification, there's no single \"correct\" summary. Multiple valid summaries exist.\n",
    "\n",
    "### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "\n",
    "| Metric | Measures | Formula |\n",
    "|--------|----------|---------|\n",
    "| **ROUGE-1** | Unigram overlap | overlap / reference_length |\n",
    "| **ROUGE-2** | Bigram overlap | Better for fluency |\n",
    "| **ROUGE-L** | Longest common subsequence | Sentence-level structure |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(summary, reference):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores (simplified implementation).\n",
    "    \n",
    "    In production, use the 'rouge-score' library.\n",
    "    \"\"\"\n",
    "    def get_ngrams(text, n):\n",
    "        words = word_tokenize(text.lower())\n",
    "        return set(tuple(words[i:i+n]) for i in range(len(words)-n+1))\n",
    "    \n",
    "    def lcs_length(s1, s2):\n",
    "        \"\"\"Longest common subsequence.\"\"\"\n",
    "        words1 = word_tokenize(s1.lower())\n",
    "        words2 = word_tokenize(s2.lower())\n",
    "        m, n = len(words1), len(words2)\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        \n",
    "        for i in range(1, m + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                if words1[i-1] == words2[j-1]:\n",
    "                    dp[i][j] = dp[i-1][j-1] + 1\n",
    "                else:\n",
    "                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "        \n",
    "        return dp[m][n]\n",
    "    \n",
    "    # ROUGE-1\n",
    "    summary_unigrams = get_ngrams(summary, 1)\n",
    "    reference_unigrams = get_ngrams(reference, 1)\n",
    "    overlap_1 = len(summary_unigrams & reference_unigrams)\n",
    "    rouge1_precision = overlap_1 / max(1, len(summary_unigrams))\n",
    "    rouge1_recall = overlap_1 / max(1, len(reference_unigrams))\n",
    "    rouge1_f1 = 2 * rouge1_precision * rouge1_recall / max(0.001, rouge1_precision + rouge1_recall)\n",
    "    \n",
    "    # ROUGE-2\n",
    "    summary_bigrams = get_ngrams(summary, 2)\n",
    "    reference_bigrams = get_ngrams(reference, 2)\n",
    "    overlap_2 = len(summary_bigrams & reference_bigrams)\n",
    "    rouge2_precision = overlap_2 / max(1, len(summary_bigrams))\n",
    "    rouge2_recall = overlap_2 / max(1, len(reference_bigrams))\n",
    "    rouge2_f1 = 2 * rouge2_precision * rouge2_recall / max(0.001, rouge2_precision + rouge2_recall)\n",
    "    \n",
    "    # ROUGE-L\n",
    "    lcs = lcs_length(summary, reference)\n",
    "    summary_words = len(word_tokenize(summary))\n",
    "    reference_words = len(word_tokenize(reference))\n",
    "    rougel_precision = lcs / max(1, summary_words)\n",
    "    rougel_recall = lcs / max(1, reference_words)\n",
    "    rougel_f1 = 2 * rougel_precision * rougel_recall / max(0.001, rougel_precision + rougel_recall)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': {'precision': rouge1_precision, 'recall': rouge1_recall, 'f1': rouge1_f1},\n",
    "        'rouge2': {'precision': rouge2_precision, 'recall': rouge2_recall, 'f1': rouge2_f1},\n",
    "        'rougeL': {'precision': rougel_precision, 'recall': rougel_recall, 'f1': rougel_f1}\n",
    "    }\n",
    "\n",
    "# Example evaluation\n",
    "reference_summary = \"\"\"Q3 showed strong revenue growth of 8% to $32.4 billion. Consumer banking \n",
    "deposits grew 12% and net interest income increased $340 million. Credit provisions rose to \n",
    "$1.2 billion due to economic uncertainty. Commercial real estate lending softened with \n",
    "disciplined underwriting maintained.\"\"\"\n",
    "\n",
    "generated_summary = result['summary']\n",
    "\n",
    "scores = calculate_rouge_scores(generated_summary, reference_summary)\n",
    "\n",
    "print(\"ROUGE EVALUATION\")\n",
    "print(\"=\"*40)\n",
    "for metric, values in scores.items():\n",
    "    print(f\"\\n{metric.upper()}:\")\n",
    "    print(f\"  Precision: {values['precision']:.4f}\")\n",
    "    print(f\"  Recall: {values['recall']:.4f}\")\n",
    "    print(f\"  F1: {values['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Banking-Specific Evaluation Criteria\n",
    "\n",
    "| Criterion | Description | How to Measure |\n",
    "|-----------|-------------|----------------|\n",
    "| **Factual Accuracy** | Numbers, names, dates correct | Manual review / NER comparison |\n",
    "| **Key Information** | Critical facts included | Checklist coverage |\n",
    "| **No Hallucination** | Everything in summary exists in source | Attribution check |\n",
    "| **Coherence** | Summary reads naturally | Human rating |\n",
    "| **Compression** | Significant length reduction | Word count ratio |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Readiness Checklist\n",
    "\n",
    "```\n",
    "INPUT HANDLING\n",
    "[ ] Document length limits (min/max)\n",
    "[ ] Character encoding normalization\n",
    "[ ] Language detection\n",
    "[ ] Multi-document support (if needed)\n",
    "\n",
    "SUMMARIZATION QUALITY\n",
    "[ ] Factual consistency checking\n",
    "[ ] Key entity preservation (names, numbers)\n",
    "[ ] Attribution to source sentences\n",
    "[ ] Readability scoring\n",
    "\n",
    "OUTPUT REQUIREMENTS\n",
    "[ ] Configurable length (words/sentences)\n",
    "[ ] Sentence boundary preservation\n",
    "[ ] Source attribution (which sentences selected)\n",
    "[ ] Confidence/quality score\n",
    "\n",
    "BANKING-SPECIFIC\n",
    "[ ] Numerical accuracy verification\n",
    "[ ] Key metric extraction (revenue, profit, etc.)\n",
    "[ ] Sentiment preservation\n",
    "[ ] Regulatory term handling\n",
    "\n",
    "MONITORING\n",
    "[ ] Compression ratio tracking\n",
    "[ ] Processing time monitoring\n",
    "[ ] Error rate tracking\n",
    "[ ] Human feedback loop\n",
    "\n",
    "GOVERNANCE\n",
    "[ ] Audit trail (input → output mapping)\n",
    "[ ] Version control of algorithm\n",
    "[ ] Human review for critical documents\n",
    "[ ] Disclosure that summary is machine-generated\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Modern LLM-Based Approach\n",
    "\n",
    "### Abstractive Summarization with LLMs\n",
    "\n",
    "**Option 1: Fine-tuned T5/BART**\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summary = summarizer(text, max_length=150, min_length=50)\n",
    "```\n",
    "\n",
    "**Option 2: GPT/Claude Prompting**\n",
    "```python\n",
    "prompt = f\"\"\"\n",
    "Summarize the following earnings call transcript in 3-4 sentences.\n",
    "Focus on: revenue, profit metrics, key business highlights, and forward guidance.\n",
    "Use exact numbers from the text.\n",
    "\n",
    "Transcript:\n",
    "{text}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Option 3: Hybrid (Extract then Abstract)**\n",
    "```python\n",
    "# Step 1: Extract key sentences (traditional)\n",
    "key_sentences = extractive_summarize(text, n=10)\n",
    "\n",
    "# Step 2: Rewrite for fluency (LLM)\n",
    "prompt = f\"Rewrite these key points as a coherent paragraph:\\n{key_sentences}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt for LLM summarization\n",
    "def create_summarization_prompt(text, doc_type='earnings_call', max_words=100):\n",
    "    \"\"\"\n",
    "    Create prompt for LLM-based summarization.\n",
    "    \n",
    "    Banking considerations:\n",
    "    - Emphasize factual accuracy\n",
    "    - Request specific metrics\n",
    "    - Avoid speculation/inference\n",
    "    \"\"\"\n",
    "    \n",
    "    type_instructions = {\n",
    "        'earnings_call': \"\"\"Focus on:\n",
    "- Revenue and profit figures (use exact numbers)\n",
    "- Key business segment performance\n",
    "- Forward guidance and outlook\n",
    "- Material risks or concerns mentioned\"\"\",\n",
    "        'regulatory': \"\"\"Focus on:\n",
    "- New requirements or changes\n",
    "- Compliance deadlines\n",
    "- Impact on operations\n",
    "- Required actions\"\"\",\n",
    "        'contract': \"\"\"Focus on:\n",
    "- Parties involved\n",
    "- Key terms and conditions\n",
    "- Financial obligations\n",
    "- Important dates and deadlines\"\"\"\n",
    "    }\n",
    "    \n",
    "    instructions = type_instructions.get(doc_type, \"Identify the key points.\")\n",
    "    \n",
    "    prompt = f\"\"\"You are summarizing a banking document. Accuracy is critical.\n",
    "\n",
    "Document Type: {doc_type}\n",
    "\n",
    "{instructions}\n",
    "\n",
    "Rules:\n",
    "1. Use EXACT numbers from the document - do not round or approximate\n",
    "2. Do not add information not in the document\n",
    "3. Do not speculate or make inferences\n",
    "4. Keep summary under {max_words} words\n",
    "5. Maintain neutral, factual tone\n",
    "\n",
    "Document:\n",
    "---\n",
    "{text[:3000]}  # Truncate for context window\n",
    "---\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example\n",
    "prompt = create_summarization_prompt(\n",
    "    sample_documents[0]['text'],\n",
    "    doc_type='earnings_call',\n",
    "    max_words=100\n",
    ")\n",
    "\n",
    "print(\"LLM SUMMARIZATION PROMPT\")\n",
    "print(\"=\"*60)\n",
    "print(prompt[:1500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Traditional vs LLM Decision Matrix\n",
    "\n",
    "| Dimension | Extractive (TextRank/MMR) | Abstractive (T5/BART) | LLM (GPT-4/Claude) |\n",
    "|-----------|--------------------------|----------------------|--------------------|\n",
    "| **Faithfulness** | 100% (verbatim) | 90-95% | 85-95% |\n",
    "| **Fluency** | Lower (sentence fragments) | High | Very high |\n",
    "| **Hallucination Risk** | None | Low | Medium |\n",
    "| **Latency** | <100ms | 500ms-2s | 1-5s |\n",
    "| **Cost** | ~$0 | $0.001-0.01 | $0.01-0.05 |\n",
    "| **Long docs** | Excellent | Limited (context) | Limited (context) |\n",
    "| **Training data** | None needed | Required | Few examples |\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "**Use Extractive**:\n",
    "- Legal/regulatory documents where verbatim accuracy required\n",
    "- Audit trail needed (which exact sentences were selected)\n",
    "- Very long documents (hundreds of pages)\n",
    "- High volume, low latency requirements\n",
    "\n",
    "**Use Abstractive (fine-tuned)**:\n",
    "- Customer-facing summaries needing polish\n",
    "- Consistent output format needed\n",
    "- Moderate accuracy requirements\n",
    "\n",
    "**Use LLM**:\n",
    "- Executive briefings where fluency matters\n",
    "- Low volume, high quality requirements\n",
    "- Complex synthesis across document sections\n",
    "- **Always with human review for banking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interview Soundbites\n",
    "\n",
    "### Ready-to-Say Statements\n",
    "\n",
    "**On Extractive vs Abstractive:**\n",
    "> \"For regulatory documents and financial statements, I default to extractive summarization. When a summary says 'revenue grew 12%' I need to be able to point to the exact sentence that says that. Abstractive models can subtly change numbers or merge facts incorrectly - that's a compliance risk.\"\n",
    "\n",
    "**On TextRank:**\n",
    "> \"TextRank is elegant because it treats sentences like web pages. Important sentences are connected to many other important sentences - it's PageRank for documents. No training data needed, works across domains, and gives surprisingly good results.\"\n",
    "\n",
    "**On MMR:**\n",
    "> \"I prefer MMR over pure relevance ranking because it explicitly penalizes redundancy. In a 100-sentence document, the top 5 by relevance might all say the same thing differently. MMR's diversity term ensures we cover more ground.\"\n",
    "\n",
    "**On Evaluation:**\n",
    "> \"ROUGE scores are useful but limited. A summary could have perfect ROUGE but miss the most important fact. For banking, I complement ROUGE with key fact checklists - did we capture the revenue number, the provision change, the forward guidance? That's what matters.\"\n",
    "\n",
    "**On Hallucination:**\n",
    "> \"Hallucination in summarization is a dealbreaker for banking. If an LLM summarizes '12% growth' as '15% growth' that's not a rounding error - that's material misrepresentation. For high-stakes documents, I use extractive as a safety layer even if the output is less polished.\"\n",
    "\n",
    "**On Long Documents:**\n",
    "> \"LLMs have context limits. For a 100-page 10-K filing, I use a two-stage approach: extractive to select key passages within context limits, then optionally LLM to synthesize. This combines the coverage of extractive with the fluency of generative.\"\n",
    "\n",
    "**On Production:**\n",
    "> \"Every machine-generated summary in banking needs attribution. I include the source sentence indices so reviewers can verify. And we're transparent with users that it's AI-generated - no pretending summaries are human-written.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Common Interview Questions\n",
    "\n",
    "**Q: How do you handle multi-document summarization?**\n",
    "> Aggregate sentences from all documents, run similarity-based de-duplication, then apply standard extractive methods. For MMR, the redundancy penalty naturally handles cross-document overlap.\n",
    "\n",
    "**Q: What if the document has multiple distinct topics?**\n",
    "> First segment by topic (using topic modeling), summarize each segment, then combine. Or use query-focused summarization where each topic becomes a query and we extract relevant sentences per topic.\n",
    "\n",
    "**Q: How do you ensure numbers are preserved correctly?**\n",
    "> Extract numerical entities first, track their source sentences, and ensure at least one source sentence for each key number is selected. Post-process to verify number presence in final summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                    NOTEBOOK SUMMARY                               ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║  Task: Text Summarization                                        ║\n",
    "║  Approach: Traditional NLP (Extractive Methods)                  ║\n",
    "║  Banking Use: Document digests, compliance summaries             ║\n",
    "║                                                                  ║\n",
    "║  Key Takeaways:                                                  ║\n",
    "║  1. Extractive = no hallucination (verbatim sentences)           ║\n",
    "║  2. TextRank (graph-based) works without training                ║\n",
    "║  3. MMR balances relevance and diversity                         ║\n",
    "║  4. ROUGE + fact checklist for evaluation                        ║\n",
    "║  5. Attribution required for banking compliance                  ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

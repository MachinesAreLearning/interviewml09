{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification - Traditional NLP\n",
    "## Interview Preparation Notebook for Senior Applied AI Scientist (Retail Banking)\n",
    "\n",
    "---\n",
    "\n",
    "**Goal**: Demonstrate mastery of supervised text classification using traditional ML approaches, with emphasis on production deployment and banking-specific considerations.\n",
    "\n",
    "**Interview Signal**: This notebook shows you can build interpretable, scalable classification systems that meet regulatory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Context (Banking Lens)\n",
    "\n",
    "### Why Text Classification Exists in Retail Banking\n",
    "\n",
    "Text classification is arguably the most deployed NLP task in banking. Every piece of unstructured text needs to be routed, prioritized, or categorized.\n",
    "\n",
    "| Use Case | Input | Output | Business Impact |\n",
    "|----------|-------|--------|----------------|\n",
    "| **Complaint Routing** | Customer complaint | Department (fraud, billing, service) | Reduce resolution time by 40% |\n",
    "| **Fraud Alert Triage** | Transaction alert text | Priority (high/medium/low) | Focus analysts on real threats |\n",
    "| **Sentiment Analysis** | Survey response | Positive/Negative/Neutral | Track NPS drivers |\n",
    "| **Email Intent Detection** | Inbound email | Intent (inquiry, complaint, request) | Auto-response eligibility |\n",
    "| **Document Classification** | Uploaded document | Type (ID, statement, proof of address) | Automate KYC workflows |\n",
    "\n",
    "### The Business Problem\n",
    "\n",
    "> \"We receive 50,000 customer emails per day. How do we route them to the right team without making customers wait?\"\n",
    "\n",
    "**Without classification**: Manual triage by generalist agents → 24-48 hour routing delay  \n",
    "**With classification**: Instant routing to specialist queues → <1 hour first response\n",
    "\n",
    "### Real Banking Example: Complaint Classification\n",
    "\n",
    "**Input**: \"I was charged $35 for an overdraft but I had money in my savings account. This is unfair and I want a refund immediately.\"\n",
    "\n",
    "**Classification Task**: \n",
    "- Primary Category: `Fee Dispute`\n",
    "- Product: `Checking Account`\n",
    "- Urgency: `High` (refund request)\n",
    "- Sentiment: `Negative`\n",
    "\n",
    "### Interview Framing\n",
    "\n",
    "```\n",
    "\"Text classification in banking isn't just about accuracy - it's about building trust. \n",
    "When we misroute a fraud complaint to the billing team, we're not just creating \n",
    "inefficiency - we're potentially leaving a customer vulnerable. That's why I focus \n",
    "on high recall for high-stakes categories, even if it means more manual review.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem Definition\n",
    "\n",
    "### Task Type: Supervised Learning (Classification)\n",
    "\n",
    "| Aspect | Description |\n",
    "|--------|-------------|\n",
    "| **Learning Type** | Supervised (requires labeled training data) |\n",
    "| **Input** | Single text document |\n",
    "| **Output** | Class label + probability score |\n",
    "| **Variants** | Binary, Multi-class, Multi-label |\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given document $d$ represented as feature vector $\\mathbf{x}$, predict class $y$:\n",
    "\n",
    "$$P(y|\\mathbf{x}) = \\frac{P(\\mathbf{x}|y)P(y)}{P(\\mathbf{x})}$$\n",
    "\n",
    "(Naive Bayes) or find $\\mathbf{w}$ such that:\n",
    "\n",
    "$$\\hat{y} = \\text{argmax}_y \\ \\mathbf{w}^T \\phi(\\mathbf{x}, y)$$\n",
    "\n",
    "(Linear models like Logistic Regression, SVM)\n",
    "\n",
    "### Why Traditional Approaches Before LLMs\n",
    "\n",
    "1. **Interpretable**: Logistic regression coefficients show which words drive predictions\n",
    "2. **Fast inference**: <10ms per document vs 100ms+ for transformers\n",
    "3. **Low resource**: Runs on CPUs, no GPU required\n",
    "4. **Audit-friendly**: Deterministic, reproducible results\n",
    "5. **Works with limited data**: 1,000 examples often sufficient\n",
    "\n",
    "### Classification Types in Banking\n",
    "\n",
    "| Type | Example | Challenge |\n",
    "|------|---------|----------|\n",
    "| **Binary** | Fraud vs Not Fraud | Extreme class imbalance (99.9% not fraud) |\n",
    "| **Multi-class** | Route to 1 of 10 departments | Overlapping categories |\n",
    "| **Multi-label** | Tag with multiple products | One complaint → multiple issues |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset\n",
    "\n",
    "### Public Dataset: Consumer Finance Complaints (CFPB)\n",
    "\n",
    "We'll use a subset of the CFPB Consumer Complaint Database - actual banking complaints filed with the US government.\n",
    "\n",
    "**Why this is the ideal banking dataset**:\n",
    "- Real customer language about financial products\n",
    "- Pre-labeled with product categories\n",
    "- Contains complaint narratives (not just metadata)\n",
    "- Public and frequently updated\n",
    "\n",
    "For this demo, we'll simulate with 20 Newsgroups as a fallback, but production would use CFPB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install scikit-learn nltk pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset - Using 20 Newsgroups with banking-relevant categories\n",
    "# Mapping to banking context:\n",
    "# comp.* -> Technical Support (app/website issues)\n",
    "# talk.politics.* -> Policy Complaints (regulatory, fees)\n",
    "# rec.* -> General Service Issues\n",
    "# sci.* -> Product Inquiries\n",
    "\n",
    "categories = [\n",
    "    'comp.sys.mac.hardware',  # Tech support\n",
    "    'comp.windows.x',         # Tech support\n",
    "    'rec.autos',              # Service issues\n",
    "    'rec.sport.baseball',     # Service issues\n",
    "    'sci.electronics',        # Product inquiries\n",
    "    'sci.med',                # Product inquiries\n",
    "    'talk.politics.misc',     # Policy complaints\n",
    "    'talk.religion.misc',     # Policy complaints\n",
    "]\n",
    "\n",
    "# Simplified to 4 categories for banking simulation\n",
    "category_mapping = {\n",
    "    'comp.sys.mac.hardware': 'Tech_Support',\n",
    "    'comp.windows.x': 'Tech_Support',\n",
    "    'rec.autos': 'Service_Issues',\n",
    "    'rec.sport.baseball': 'Service_Issues',\n",
    "    'sci.electronics': 'Product_Inquiry',\n",
    "    'sci.med': 'Product_Inquiry',\n",
    "    'talk.politics.misc': 'Policy_Complaint',\n",
    "    'talk.religion.misc': 'Policy_Complaint',\n",
    "}\n",
    "\n",
    "# Load data\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Map to banking categories\n",
    "banking_categories = ['Tech_Support', 'Service_Issues', 'Product_Inquiry', 'Policy_Complaint']\n",
    "\n",
    "def map_to_banking_label(original_label, target_names):\n",
    "    original_category = target_names[original_label]\n",
    "    banking_label = category_mapping[original_category]\n",
    "    return banking_categories.index(banking_label)\n",
    "\n",
    "X_train = newsgroups_train.data\n",
    "y_train = np.array([map_to_banking_label(y, newsgroups_train.target_names) \n",
    "                    for y in newsgroups_train.target])\n",
    "\n",
    "X_test = newsgroups_test.data\n",
    "y_test = np.array([map_to_banking_label(y, newsgroups_test.target_names) \n",
    "                   for y in newsgroups_test.target])\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"\\nBanking Categories: {banking_categories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution - critical for banking (often imbalanced)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for ax, (y, title) in zip(axes, [(y_train, 'Training'), (y_test, 'Test')]):\n",
    "    counts = np.bincount(y)\n",
    "    ax.bar(banking_categories, counts)\n",
    "    ax.set_title(f'{title} Set Class Distribution')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, count in enumerate(counts):\n",
    "        ax.text(i, count + 10, str(count), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for imbalance\n",
    "train_counts = np.bincount(y_train)\n",
    "imbalance_ratio = max(train_counts) / min(train_counts)\n",
    "print(f\"\\nImbalance ratio (max/min): {imbalance_ratio:.2f}\")\n",
    "print(\"Note: In real banking, fraud detection has 100:1 or worse imbalance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "print(\"SAMPLE DOCUMENTS BY CATEGORY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for cat_idx, cat_name in enumerate(banking_categories):\n",
    "    sample_idx = np.where(y_train == cat_idx)[0][0]\n",
    "    print(f\"\\n[{cat_name}]\")\n",
    "    print(f\"{X_train[sample_idx][:300]}...\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Traditional NLP Pipeline\n",
    "\n",
    "### 4.1 Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "class TextClassificationPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocessor optimized for text classification.\n",
    "    \n",
    "    Key differences from topic modeling:\n",
    "    - Keep some stopwords (negations matter: \"not satisfied\" vs \"satisfied\")\n",
    "    - Preserve case for proper nouns optionally\n",
    "    - Handle banking-specific patterns (account numbers, amounts)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 remove_numbers=True,\n",
    "                 preserve_negations=True,\n",
    "                 min_word_length=2):\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.preserve_negations = preserve_negations\n",
    "        self.min_word_length = min_word_length\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Modified stopwords - keep negations for sentiment/classification\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        if preserve_negations:\n",
    "            negations = {'not', 'no', 'never', 'neither', 'nobody', 'nothing', \n",
    "                        'nowhere', 'cannot', \"can't\", \"won't\", \"don't\", \"doesn't\",\n",
    "                        \"didn't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\"}\n",
    "            self.stop_words -= negations\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Full preprocessing pipeline.\"\"\"\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove emails\n",
    "        text = re.sub(r'\\S+@\\S+', ' EMAIL ', text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+', ' URL ', text)\n",
    "        \n",
    "        # Handle money amounts (banking-specific)\n",
    "        text = re.sub(r'\\$[\\d,]+\\.?\\d*', ' MONEY ', text)\n",
    "        \n",
    "        # Handle account-like numbers\n",
    "        text = re.sub(r'\\b\\d{4,}\\b', ' ACCTNUM ', text)\n",
    "        \n",
    "        # Remove remaining numbers\n",
    "        if self.remove_numbers:\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove punctuation (keep apostrophes for contractions)\n",
    "        text = re.sub(r\"[^a-zA-Z'\\s]\", ' ', text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Filter and lemmatize\n",
    "        processed = [\n",
    "            self.lemmatizer.lemmatize(token)\n",
    "            for token in tokens\n",
    "            if token not in self.stop_words\n",
    "            and len(token) >= self.min_word_length\n",
    "        ]\n",
    "        \n",
    "        return ' '.join(processed)\n",
    "\n",
    "preprocessor = TextClassificationPreprocessor(preserve_negations=True)\n",
    "\n",
    "# Demonstrate\n",
    "sample = \"I was charged $35.00 for overdraft on account 123456789. I am NOT satisfied with this!\"\n",
    "print(f\"Original: {sample}\")\n",
    "print(f\"Processed: {preprocessor.preprocess(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all data\n",
    "print(\"Preprocessing training data...\")\n",
    "X_train_processed = [preprocessor.preprocess(doc) for doc in X_train]\n",
    "\n",
    "print(\"Preprocessing test data...\")\n",
    "X_test_processed = [preprocessor.preprocess(doc) for doc in X_test]\n",
    "\n",
    "print(f\"\\nProcessed {len(X_train_processed)} training documents\")\n",
    "print(f\"Processed {len(X_test_processed)} test documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Stemming vs. Lemmatization for Classification\n",
    "\n",
    "**For Classification**: Either can work, but consider:\n",
    "\n",
    "| Aspect | Stemming | Lemmatization |\n",
    "|--------|----------|---------------|\n",
    "| **Speed** | Faster | Slower |\n",
    "| **Feature interpretability** | Lower (\"studi\") | Higher (\"study\") |\n",
    "| **Vocabulary reduction** | More aggressive | Less aggressive |\n",
    "| **Best for** | High-volume, less interpretability needed | When explaining to stakeholders |\n",
    "\n",
    "**My choice for banking**: Lemmatization, because:\n",
    "1. We often need to explain why a complaint was classified a certain way\n",
    "2. Feature coefficients should be readable to compliance teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different vectorization approaches\n",
    "\n",
    "# 1. Bag of Words (CountVectorizer)\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 1),  # Unigrams only\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# 2. TF-IDF (standard for classification)\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),  # Unigrams + bigrams\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,  # Apply log scaling to term frequencies\n",
    ")\n",
    "\n",
    "# 3. TF-IDF with more aggressive bigrams\n",
    "tfidf_bigram_vectorizer = TfidfVectorizer(\n",
    "    max_features=15000,\n",
    "    ngram_range=(1, 3),  # Up to trigrams\n",
    "    min_df=3,\n",
    "    max_df=0.90,\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "\n",
    "# Fit and compare\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train_processed)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_processed)\n",
    "X_train_tfidf_bigram = tfidf_bigram_vectorizer.fit_transform(X_train_processed)\n",
    "\n",
    "print(\"Feature Matrix Shapes:\")\n",
    "print(f\"  BoW (unigrams): {X_train_bow.shape}\")\n",
    "print(f\"  TF-IDF (uni+bi): {X_train_tfidf.shape}\")\n",
    "print(f\"  TF-IDF (uni+bi+tri): {X_train_tfidf_bigram.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why TF-IDF over BoW for classification?\n",
    "print(\"\"\"\n",
    "TF-IDF vs Bag-of-Words for Classification:\n",
    "\n",
    "BoW: Raw word counts\n",
    "  - \"the\" appears 10 times → feature value = 10\n",
    "  - Doesn't account for word importance\n",
    "\n",
    "TF-IDF: Term Frequency × Inverse Document Frequency  \n",
    "  - \"the\" appears in every doc → low IDF → low weight\n",
    "  - \"overdraft\" appears in fee complaints → high IDF → high weight\n",
    "  \n",
    "For classification, TF-IDF typically performs better because\n",
    "discriminative words are upweighted automatically.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Model Choice\n",
    "\n",
    "Traditional classifiers for text:\n",
    "\n",
    "| Model | Strengths | Weaknesses | Best For |\n",
    "|-------|-----------|------------|----------|\n",
    "| **Naive Bayes** | Fast, works well with small data | Assumes feature independence | Baseline, quick iteration |\n",
    "| **Logistic Regression** | Interpretable, probabilistic | Linear decision boundary | Production, when explainability needed |\n",
    "| **SVM (Linear)** | Excellent for high-dim sparse data | Less interpretable | Maximum accuracy |\n",
    "| **Random Forest** | Handles non-linearity | Slower, harder to interpret | When features have interactions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_processed)\n",
    "\n",
    "# Train multiple models for comparison\n",
    "models = {\n",
    "    'Naive Bayes (Multinomial)': MultinomialNB(alpha=0.1),\n",
    "    'Naive Bayes (Complement)': ComplementNB(alpha=0.1),  # Better for imbalanced data\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000, \n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight='balanced',  # Handle imbalance\n",
    "        C=1.0\n",
    "    ),\n",
    "    'Linear SVM': LinearSVC(\n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight='balanced',\n",
    "        max_iter=2000\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'model': model,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  F1 (weighted): {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "results_df = pd.DataFrame({\n",
    "    name: {'Accuracy': r['accuracy'], 'Precision': r['precision'], \n",
    "           'Recall': r['recall'], 'F1': r['f1']}\n",
    "    for name, r in results.items()\n",
    "}).T\n",
    "\n",
    "print(\"\\nMODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "results_df.plot(kind='bar', ax=ax)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([0.7, 1.0])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of best model (Logistic Regression for interpretability)\n",
    "best_model_name = 'Logistic Regression'\n",
    "best_model = results[best_model_name]['model']\n",
    "y_pred_best = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"\\nDETAILED CLASSIFICATION REPORT: {best_model_name}\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred_best, target_names=banking_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=banking_categories,\n",
    "            yticklabels=banking_categories)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Normalize for better interpretation\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=banking_categories,\n",
    "            yticklabels=banking_categories)\n",
    "plt.title(f'Normalized Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance - what words drive each class?\n",
    "def get_top_features(model, vectorizer, class_names, top_n=10):\n",
    "    \"\"\"Extract most important features per class from logistic regression.\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        if hasattr(model, 'coef_'):\n",
    "            # Get coefficients for this class\n",
    "            coefficients = model.coef_[i]\n",
    "            \n",
    "            # Top positive features (indicate this class)\n",
    "            top_positive_idx = coefficients.argsort()[-top_n:][::-1]\n",
    "            top_positive = [(feature_names[j], coefficients[j]) for j in top_positive_idx]\n",
    "            \n",
    "            # Top negative features (indicate NOT this class)\n",
    "            top_negative_idx = coefficients.argsort()[:top_n]\n",
    "            top_negative = [(feature_names[j], coefficients[j]) for j in top_negative_idx]\n",
    "            \n",
    "            print(f\"\\n[{class_name}]\")\n",
    "            print(f\"  Top POSITIVE indicators: {[f[0] for f in top_positive]}\")\n",
    "            print(f\"  Top NEGATIVE indicators: {[f[0] for f in top_negative]}\")\n",
    "\n",
    "print(\"FEATURE IMPORTANCE BY CLASS\")\n",
    "print(\"=\" * 60)\n",
    "get_top_features(best_model, tfidf_vectorizer, banking_categories, top_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function for production\n",
    "def classify_complaint(text, model, vectorizer, preprocessor, class_names, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Classify a customer complaint with confidence score.\n",
    "    \n",
    "    Returns:\n",
    "        dict with predicted class, confidence, and all class probabilities\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    processed = preprocessor.preprocess(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    features = vectorizer.transform([processed])\n",
    "    \n",
    "    # Predict\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probabilities = model.predict_proba(features)[0]\n",
    "    else:\n",
    "        # For SVM, use decision function\n",
    "        decision = model.decision_function(features)[0]\n",
    "        # Softmax approximation\n",
    "        probabilities = np.exp(decision) / np.sum(np.exp(decision))\n",
    "    \n",
    "    predicted_class = np.argmax(probabilities)\n",
    "    confidence = probabilities[predicted_class]\n",
    "    \n",
    "    # Flag low confidence predictions\n",
    "    needs_review = confidence < threshold\n",
    "    \n",
    "    return {\n",
    "        'predicted_class': class_names[predicted_class],\n",
    "        'confidence': confidence,\n",
    "        'needs_human_review': needs_review,\n",
    "        'all_probabilities': dict(zip(class_names, probabilities))\n",
    "    }\n",
    "\n",
    "# Test inference\n",
    "test_complaints = [\n",
    "    \"The mobile app keeps crashing when I try to check my balance. Very frustrating!\",\n",
    "    \"I was charged a $35 fee that I don't understand. Please explain your fee policy.\",\n",
    "    \"How do I set up automatic bill pay for my credit card?\",\n",
    "    \"The customer service representative was very rude to me on the phone.\"\n",
    "]\n",
    "\n",
    "print(\"INFERENCE EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for complaint in test_complaints:\n",
    "    result = classify_complaint(\n",
    "        complaint, best_model, tfidf_vectorizer, \n",
    "        preprocessor, banking_categories, threshold=0.6\n",
    "    )\n",
    "    print(f\"\\nInput: {complaint[:80]}...\")\n",
    "    print(f\"Predicted: {result['predicted_class']} (confidence: {result['confidence']:.2%})\")\n",
    "    if result['needs_human_review']:\n",
    "        print(\"  ⚠️  LOW CONFIDENCE - Route to human review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Strategy\n",
    "\n",
    "### Why Accuracy is NOT Always the Right Metric\n",
    "\n",
    "**Scenario**: Fraud detection with 99.9% non-fraud transactions\n",
    "- Model predicts \"not fraud\" for everything → 99.9% accuracy\n",
    "- But catches 0% of actual fraud → useless\n",
    "\n",
    "### Choosing the Right Metric\n",
    "\n",
    "| Metric | Formula | Use When |\n",
    "|--------|---------|----------|\n",
    "| **Accuracy** | (TP+TN)/(TP+TN+FP+FN) | Balanced classes |\n",
    "| **Precision** | TP/(TP+FP) | Cost of false positives is high (fraud alerts) |\n",
    "| **Recall** | TP/(TP+FN) | Cost of false negatives is high (missing fraud) |\n",
    "| **F1** | 2×(P×R)/(P+R) | Balance precision and recall |\n",
    "| **AUC-ROC** | Area under ROC curve | Ranking quality, threshold-agnostic |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics - critical for banking\n",
    "print(\"PER-CLASS METRICS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_test, y_pred_best, average=None\n",
    ")\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Category': banking_categories,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1': f1,\n",
    "    'Support': support\n",
    "}).set_index('Category')\n",
    "\n",
    "print(metrics_df.round(4))\n",
    "\n",
    "# Highlight potential issues\n",
    "print(\"\\nPOTENTIAL ISSUES:\")\n",
    "for cat, row in metrics_df.iterrows():\n",
    "    if row['Recall'] < 0.8:\n",
    "        print(f\"  ⚠️  {cat}: Low recall ({row['Recall']:.2%}) - missing {100-row['Recall']*100:.1f}% of actual cases\")\n",
    "    if row['Precision'] < 0.8:\n",
    "        print(f\"  ⚠️  {cat}: Low precision ({row['Precision']:.2%}) - {100-row['Precision']*100:.1f}% false positives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for robust estimates\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create pipeline for cross-validation\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2))),\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# 5-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_scores = cross_val_score(pipeline, X_train_processed, y_train, cv=cv, scoring='f1_weighted')\n",
    "\n",
    "print(\"CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"F1 Scores: {cv_scores.round(4)}\")\n",
    "print(f\"Mean: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# This gives us confidence interval for production performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Readiness Checklist\n",
    "\n",
    "```\n",
    "DATA QUALITY\n",
    "[ ] PII detection and masking before classification\n",
    "[ ] Input validation (min/max length, language detection)\n",
    "[ ] Handle encoding issues (UTF-8 normalization)\n",
    "[ ] Logging of raw input for debugging (separate PII-safe store)\n",
    "\n",
    "MODEL ARTIFACTS\n",
    "[ ] Serialized model with version number\n",
    "[ ] Vectorizer saved (vocabulary must match)\n",
    "[ ] Preprocessing pipeline saved (identical transforms)\n",
    "[ ] Model card documenting training data, metrics, limitations\n",
    "\n",
    "INFERENCE PIPELINE\n",
    "[ ] Latency benchmarks (p50 < 50ms, p99 < 200ms for real-time)\n",
    "[ ] Batch inference for nightly processing\n",
    "[ ] Confidence threshold for human escalation\n",
    "[ ] Fallback behavior when model is unavailable\n",
    "\n",
    "HANDLING CLASS IMBALANCE\n",
    "[ ] Class weights in training\n",
    "[ ] Threshold tuning per class\n",
    "[ ] Separate high-recall model for critical classes (fraud)\n",
    "[ ] Stratified sampling in train/test splits\n",
    "\n",
    "MONITORING & DRIFT\n",
    "[ ] Prediction distribution monitoring (shift detection)\n",
    "[ ] Confidence score distribution tracking\n",
    "[ ] Human override rate tracking\n",
    "[ ] Feature drift detection (new words appearing)\n",
    "\n",
    "GOVERNANCE (BANKING-SPECIFIC)\n",
    "[ ] Model risk assessment (SR 11-7)\n",
    "[ ] Bias testing across demographic segments\n",
    "[ ] Audit trail for all predictions\n",
    "[ ] Explainability for individual predictions\n",
    "[ ] Periodic revalidation schedule\n",
    "\n",
    "FAILURE MODES\n",
    "[ ] What if input is empty or very short?\n",
    "[ ] What if input is in wrong language?\n",
    "[ ] What if input contains adversarial content?\n",
    "[ ] What happens with previously unseen vocabulary?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production model wrapper with safety checks\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "class ProductionClassifier:\n",
    "    \"\"\"\n",
    "    Production-ready text classifier with safety checks and logging.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, vectorizer, preprocessor, class_names,\n",
    "                 confidence_threshold=0.6, min_text_length=10):\n",
    "        self.model = model\n",
    "        self.vectorizer = vectorizer\n",
    "        self.preprocessor = preprocessor\n",
    "        self.class_names = class_names\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.min_text_length = min_text_length\n",
    "        self.prediction_count = 0\n",
    "        self.low_confidence_count = 0\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"Make prediction with safety checks.\"\"\"\n",
    "        self.prediction_count += 1\n",
    "        \n",
    "        # Input validation\n",
    "        if not text or len(text.strip()) < self.min_text_length:\n",
    "            return {\n",
    "                'status': 'ERROR',\n",
    "                'error': 'Input too short',\n",
    "                'predicted_class': None,\n",
    "                'route_to_human': True\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Preprocess\n",
    "            processed = self.preprocessor.preprocess(text)\n",
    "            \n",
    "            if len(processed.split()) < 3:\n",
    "                return {\n",
    "                    'status': 'ERROR',\n",
    "                    'error': 'Insufficient content after preprocessing',\n",
    "                    'predicted_class': None,\n",
    "                    'route_to_human': True\n",
    "                }\n",
    "            \n",
    "            # Vectorize and predict\n",
    "            features = self.vectorizer.transform([processed])\n",
    "            probabilities = self.model.predict_proba(features)[0]\n",
    "            \n",
    "            predicted_idx = np.argmax(probabilities)\n",
    "            confidence = probabilities[predicted_idx]\n",
    "            \n",
    "            # Check confidence\n",
    "            route_to_human = confidence < self.confidence_threshold\n",
    "            if route_to_human:\n",
    "                self.low_confidence_count += 1\n",
    "            \n",
    "            return {\n",
    "                'status': 'SUCCESS',\n",
    "                'predicted_class': self.class_names[predicted_idx],\n",
    "                'confidence': float(confidence),\n",
    "                'route_to_human': route_to_human,\n",
    "                'all_probabilities': {name: float(prob) \n",
    "                                     for name, prob in zip(self.class_names, probabilities)},\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'status': 'ERROR',\n",
    "                'error': str(e),\n",
    "                'predicted_class': None,\n",
    "                'route_to_human': True\n",
    "            }\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Return operational metrics.\"\"\"\n",
    "        return {\n",
    "            'total_predictions': self.prediction_count,\n",
    "            'low_confidence_rate': self.low_confidence_count / max(1, self.prediction_count),\n",
    "            'human_escalation_rate': self.low_confidence_count / max(1, self.prediction_count)\n",
    "        }\n",
    "\n",
    "# Initialize production classifier\n",
    "prod_classifier = ProductionClassifier(\n",
    "    model=best_model,\n",
    "    vectorizer=tfidf_vectorizer,\n",
    "    preprocessor=preprocessor,\n",
    "    class_names=banking_categories,\n",
    "    confidence_threshold=0.6\n",
    ")\n",
    "\n",
    "# Test production classifier\n",
    "print(\"PRODUCTION CLASSIFIER TEST\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_inputs = [\n",
    "    \"The ATM ate my card and I need it back immediately!\",\n",
    "    \"\",  # Empty input\n",
    "    \"Hi\",  # Too short\n",
    "    \"I have a question about the new savings account interest rates and how they compare to competitors.\"\n",
    "]\n",
    "\n",
    "for text in test_inputs:\n",
    "    result = prod_classifier.predict(text)\n",
    "    print(f\"\\nInput: '{text[:50]}{'...' if len(text) > 50 else ''}'\")\n",
    "    print(f\"Status: {result['status']}\")\n",
    "    if result['status'] == 'SUCCESS':\n",
    "        print(f\"Prediction: {result['predicted_class']} ({result['confidence']:.2%})\")\n",
    "        print(f\"Route to human: {result['route_to_human']}\")\n",
    "    else:\n",
    "        print(f\"Error: {result.get('error', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Modern LLM-Based Approach\n",
    "\n",
    "### How would we solve text classification with LLMs today?\n",
    "\n",
    "**Option 1: Zero-Shot Classification**\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "result = classifier(\n",
    "    \"The mobile app crashes when I check my balance\",\n",
    "    candidate_labels=[\"Tech Support\", \"Service Issues\", \"Product Inquiry\", \"Policy Complaint\"]\n",
    ")\n",
    "```\n",
    "\n",
    "**Option 2: Few-Shot with GPT**\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "Classify the following customer complaint into one of these categories:\n",
    "- Tech_Support: Issues with mobile app, website, or digital banking\n",
    "- Service_Issues: Problems with customer service or branch experience\n",
    "- Product_Inquiry: Questions about products, rates, or features\n",
    "- Policy_Complaint: Concerns about fees, policies, or terms\n",
    "\n",
    "Examples:\n",
    "\"The app keeps logging me out\" -> Tech_Support\n",
    "\"The teller was rude\" -> Service_Issues\n",
    "\"What's the interest rate?\" -> Product_Inquiry\n",
    "\"Why was I charged $35?\" -> Policy_Complaint\n",
    "\n",
    "Complaint: {user_input}\n",
    "Category:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Option 3: Fine-Tuned BERT**\n",
    "```python\n",
    "from transformers import BertForSequenceClassification, Trainer\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    num_labels=4\n",
    ")\n",
    "\n",
    "# Fine-tune on labeled banking data\n",
    "trainer = Trainer(model=model, train_dataset=train_dataset, ...)\n",
    "trainer.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for LLM classification (would require API key)\n",
    "\n",
    "def create_classification_prompt(text, categories, examples=None):\n",
    "    \"\"\"\n",
    "    Create a prompt for LLM-based classification.\n",
    "    \n",
    "    In banking production:\n",
    "    - PII must be masked before sending to external API\n",
    "    - Use Azure OpenAI (approved vendor) not consumer API\n",
    "    - Log prompts for audit trail\n",
    "    \"\"\"\n",
    "    \n",
    "    category_descriptions = {\n",
    "        'Tech_Support': 'Issues with mobile app, website, ATM, or digital banking platforms',\n",
    "        'Service_Issues': 'Problems with customer service quality, wait times, or staff behavior',\n",
    "        'Product_Inquiry': 'Questions about account features, interest rates, or product offerings',\n",
    "        'Policy_Complaint': 'Concerns about fees, terms and conditions, or bank policies'\n",
    "    }\n",
    "    \n",
    "    prompt = f\"\"\"You are a customer service routing assistant for a retail bank.\n",
    "\n",
    "Task: Classify the following customer message into exactly one category.\n",
    "\n",
    "Categories:\n",
    "\"\"\"\n",
    "    \n",
    "    for cat in categories:\n",
    "        prompt += f\"- {cat}: {category_descriptions.get(cat, 'No description')}\\n\"\n",
    "    \n",
    "    if examples:\n",
    "        prompt += \"\\nExamples:\\n\"\n",
    "        for ex_text, ex_label in examples:\n",
    "            prompt += f'\"{ex_text}\" -> {ex_label}\\n'\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "Customer Message: \"{text}\"\n",
    "\n",
    "Respond with only the category name, nothing else.\n",
    "\n",
    "Category:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example\n",
    "example_prompt = create_classification_prompt(\n",
    "    \"The mobile app crashes every time I try to deposit a check\",\n",
    "    banking_categories,\n",
    "    examples=[\n",
    "        (\"Website won't load\", \"Tech_Support\"),\n",
    "        (\"Charged wrong fee\", \"Policy_Complaint\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"EXAMPLE LLM CLASSIFICATION PROMPT\")\n",
    "print(\"=\" * 50)\n",
    "print(example_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Traditional vs LLM Decision Matrix\n",
    "\n",
    "| Dimension | Traditional (LR/SVM) | LLM (Fine-tuned BERT) | LLM (Zero-shot GPT) |\n",
    "|-----------|---------------------|----------------------|--------------------|\n",
    "| **Accuracy** | 85-92% | 92-96% | 80-90% |\n",
    "| **Latency** | <10ms | 50-200ms | 500ms-2s |\n",
    "| **Cost per prediction** | ~$0 | $0.0001-0.001 | $0.001-0.01 |\n",
    "| **Training data needed** | 1,000+ per class | 100+ per class | 0-10 examples |\n",
    "| **Explainability** | High (coefficients) | Low (black box) | Medium (can ask why) |\n",
    "| **Cold start** | Needs labels | Needs labels | Works immediately |\n",
    "| **New classes** | Retrain required | Retrain required | Just update prompt |\n",
    "| **Compliance** | Easy (on-premise) | Medium (self-hosted) | Complex (API/data residency) |\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "**Use Traditional (Logistic Regression/SVM)**:\n",
    "- High volume (millions of predictions/day)\n",
    "- Strict latency requirements (<50ms)\n",
    "- Need full explainability for regulators\n",
    "- Stable categories that don't change often\n",
    "- Sufficient labeled data available\n",
    "\n",
    "**Use Fine-tuned BERT**:\n",
    "- Accuracy is critical\n",
    "- Have GPU infrastructure\n",
    "- Medium volume with latency budget\n",
    "- Can self-host (data privacy concerns)\n",
    "\n",
    "**Use Zero-shot LLM**:\n",
    "- New category needs to be added quickly\n",
    "- No labeled data available\n",
    "- Low volume, high-value decisions\n",
    "- Exploratory/prototyping phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interview Soundbites\n",
    "\n",
    "### Ready-to-Say Statements\n",
    "\n",
    "**On Algorithm Choice:**\n",
    "> \"For complaint classification at scale, I'd start with Logistic Regression on TF-IDF features. It's not sexy, but it gives me interpretable coefficients I can show to compliance, sub-10ms latency, and 90%+ accuracy. I'd only move to BERT if that 5% accuracy gain justifies the 10x latency and infrastructure cost.\"\n",
    "\n",
    "**On Evaluation:**\n",
    "> \"Accuracy can be misleading with imbalanced classes. For fraud detection with 0.1% fraud rate, a model that predicts 'not fraud' for everything gets 99.9% accuracy but is useless. I focus on recall for the minority class and use precision-recall curves, not ROC curves, for imbalanced problems.\"\n",
    "\n",
    "**On Feature Engineering:**\n",
    "> \"TF-IDF with n-grams is still competitive with transformers for many classification tasks, especially when you have clean, well-structured text. The key is good preprocessing - keeping negations for sentiment, handling domain-specific patterns like account numbers, and aggressive stopword removal.\"\n",
    "\n",
    "**On Class Imbalance:**\n",
    "> \"For imbalanced classes in banking, I use a combination of class weights during training, stratified cross-validation, and threshold tuning per class at inference. For critical categories like fraud, I might train a separate high-recall model that errs on the side of caution.\"\n",
    "\n",
    "**On When NOT to Use LLMs:**\n",
    "> \"Zero-shot classification sounds appealing, but at 1 million complaints per month, the API costs alone would be $10K-100K. And I can't send PII to external APIs without masking, which might remove important context. Traditional models give me 95% of the performance at 0.1% of the cost.\"\n",
    "\n",
    "**On Production Failures:**\n",
    "> \"Text classifiers fail silently when the vocabulary drifts. When a new product launches with new terminology, the model sees unknown words and falls back to generic predictions. We need vocabulary monitoring and regular retraining cycles - I recommend monthly for fast-moving domains like banking.\"\n",
    "\n",
    "**On Explainability:**\n",
    "> \"In banking, I need to explain every routing decision. With Logistic Regression, I can say 'this complaint was routed to Tech Support because it contains the words app, crash, and login with high positive coefficients for that class.' With BERT, I can only show attention weights which don't satisfy auditors.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Common Interview Questions\n",
    "\n",
    "**Q: How do you handle class imbalance?**\n",
    "> Multiple strategies: class weights in the loss function, oversampling minority class (SMOTE for tabular, not recommended for text), undersampling majority, threshold adjustment at inference, or ensemble of models with different class balances.\n",
    "\n",
    "**Q: Why Logistic Regression over Naive Bayes?**\n",
    "> Naive Bayes assumes feature independence, which is violated in text (word co-occurrences matter). Logistic Regression is discriminative - it directly models P(y|x) without the independence assumption. In practice, LR usually outperforms NB by 3-5% on text classification.\n",
    "\n",
    "**Q: How do you choose between multi-class and multi-label?**\n",
    "> Multi-class: exactly one label per document (complaint routing - goes to one queue). Multi-label: multiple labels possible (complaint tagging - can be about both 'fees' AND 'service'). The latter requires different loss functions (binary cross-entropy per label) and evaluation (subset accuracy, hamming loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                    NOTEBOOK SUMMARY                               ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║  Task: Text Classification                                       ║\n",
    "║  Approach: Traditional NLP (Logistic Regression, TF-IDF)         ║\n",
    "║  Banking Use: Complaint routing, fraud triage                    ║\n",
    "║                                                                  ║\n",
    "║  Key Takeaways:                                                  ║\n",
    "║  1. TF-IDF + Logistic Regression is production-ready baseline    ║\n",
    "║  2. Class imbalance requires careful metric selection            ║\n",
    "║  3. Explainability (coefficients) crucial for banking            ║\n",
    "║  4. Confidence thresholds for human escalation                   ║\n",
    "║  5. Traditional beats LLMs on cost at scale                      ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with LLMs - Modern Approaches\n",
    "## Interview Preparation Notebook for Senior Applied AI Scientist (Retail Banking)\n",
    "\n",
    "---\n",
    "\n",
    "**Goal**: Demonstrate mastery of LLM-based text classification including fine-tuning, zero-shot, few-shot, and prompt engineering approaches.\n",
    "\n",
    "**Interview Signal**: This notebook shows you understand the full spectrum of LLM classification approaches and can make informed decisions about when to use each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Context (Banking Lens)\n",
    "\n",
    "### Why LLMs for Classification Now?\n",
    "\n",
    "| Challenge | Traditional Solution | LLM Solution |\n",
    "|-----------|---------------------|---------------|\n",
    "| Cold start (no labels) | Can't train | Zero-shot works |\n",
    "| New categories | Retrain entire model | Update prompt |\n",
    "| Nuanced decisions | Rule-based fallback | Handles nuance |\n",
    "| Multi-lingual | Separate models | One model |\n",
    "\n",
    "### Banking Use Cases\n",
    "\n",
    "1. **Zero-shot complaint routing** - No labeled data? LLM can route based on category descriptions\n",
    "2. **Few-shot fraud detection** - 10 examples of new fraud patterns → instant classifier\n",
    "3. **Fine-tuned sentiment** - BERT fine-tuned on banking language for customer satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem Definition\n",
    "\n",
    "### LLM Classification Approaches\n",
    "\n",
    "| Approach | Training Data | Latency | Accuracy | Cost |\n",
    "|----------|--------------|---------|----------|------|\n",
    "| **Fine-tuned BERT** | 1K+ examples | 50-200ms | 92-96% | $0.0001/doc |\n",
    "| **Zero-shot (BART-MNLI)** | 0 examples | 200-500ms | 75-85% | $0.001/doc |\n",
    "| **Few-shot (GPT)** | 5-20 examples | 500ms-2s | 80-90% | $0.005-0.02/doc |\n",
    "| **SetFit** | 8-64 examples | 50-100ms | 85-92% | $0.0001/doc |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install transformers torch scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Sample banking complaints for classification\n",
    "sample_complaints = [\n",
    "    {\"text\": \"The mobile app crashes every time I try to deposit a check\", \"label\": \"Tech_Support\"},\n",
    "    {\"text\": \"I was charged a $35 overdraft fee even though I had money in savings\", \"label\": \"Fee_Dispute\"},\n",
    "    {\"text\": \"The customer service representative was extremely rude to me\", \"label\": \"Service_Quality\"},\n",
    "    {\"text\": \"Someone made unauthorized purchases on my credit card\", \"label\": \"Fraud\"},\n",
    "    {\"text\": \"What is the interest rate on your high-yield savings account?\", \"label\": \"Product_Inquiry\"},\n",
    "]\n",
    "\n",
    "print(f\"Sample complaints: {len(sample_complaints)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-4. LLM Approach Selection & Implementation\n",
    "\n",
    "### 4.1 Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot classification (pseudocode)\n",
    "'''\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "labels = [\"Technical Support\", \"Fee Dispute\", \"Service Quality\", \"Fraud\", \"Product Inquiry\"]\n",
    "\n",
    "text = \"The mobile app crashes every time I try to deposit a check\"\n",
    "result = classifier(text, labels)\n",
    "\n",
    "print(f\"Predicted: {result['labels'][0]} ({result['scores'][0]:.2%})\")\n",
    "'''\n",
    "\n",
    "def simulate_zero_shot(text, labels):\n",
    "    \"\"\"Simulated zero-shot for demonstration.\"\"\"\n",
    "    # Simple keyword-based simulation\n",
    "    keywords = {\n",
    "        \"Technical Support\": [\"app\", \"crash\", \"login\", \"error\", \"website\"],\n",
    "        \"Fee Dispute\": [\"fee\", \"charge\", \"overdraft\", \"refund\"],\n",
    "        \"Service Quality\": [\"rude\", \"wait\", \"service\", \"representative\"],\n",
    "        \"Fraud\": [\"unauthorized\", \"fraud\", \"stolen\", \"scam\"],\n",
    "        \"Product Inquiry\": [\"rate\", \"interest\", \"account\", \"what is\"]\n",
    "    }\n",
    "    \n",
    "    scores = {}\n",
    "    text_lower = text.lower()\n",
    "    for label in labels:\n",
    "        matches = sum(1 for kw in keywords.get(label, []) if kw in text_lower)\n",
    "        scores[label] = matches\n",
    "    \n",
    "    total = sum(scores.values()) + 0.001\n",
    "    return {k: v/total for k, v in sorted(scores.items(), key=lambda x: -x[1])}\n",
    "\n",
    "# Test\n",
    "labels = [\"Technical Support\", \"Fee Dispute\", \"Service Quality\", \"Fraud\", \"Product Inquiry\"]\n",
    "for complaint in sample_complaints[:3]:\n",
    "    result = simulate_zero_shot(complaint['text'], labels)\n",
    "    top_label = list(result.keys())[0]\n",
    "    print(f\"Text: {complaint['text'][:50]}...\")\n",
    "    print(f\"Predicted: {top_label}, True: {complaint['label']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Few-Shot with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_few_shot_prompt(text, examples, categories):\n",
    "    \"\"\"Create few-shot classification prompt.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Classify the following customer complaint into one of these categories:\n",
    "{', '.join(categories)}\n",
    "\n",
    "Examples:\n",
    "\"\"\"\n",
    "    \n",
    "    for ex in examples:\n",
    "        prompt += f'Complaint: \"{ex[\"text\"]}\"\\nCategory: {ex[\"label\"]}\\n\\n'\n",
    "    \n",
    "    prompt += f'\"\"\"Complaint: \"{text}\"\\nCategory:'\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example prompt\n",
    "few_shot_examples = sample_complaints[:3]\n",
    "new_complaint = \"I can't access my account online and the password reset isn't working\"\n",
    "\n",
    "prompt = create_few_shot_prompt(\n",
    "    new_complaint, \n",
    "    few_shot_examples,\n",
    "    [\"Tech_Support\", \"Fee_Dispute\", \"Service_Quality\", \"Fraud\", \"Product_Inquiry\"]\n",
    ")\n",
    "\n",
    "print(\"FEW-SHOT PROMPT\")\n",
    "print(\"=\" * 50)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Fine-tuned BERT (Pseudocode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Fine-tuning BERT for classification\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained BERT\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=5  # Number of categories\n",
    ")\n",
    "\n",
    "# Prepare training data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "'''\n",
    "\n",
    "print(\"Fine-tuning pseudocode shown above.\")\n",
    "print(\"Requires: transformers, torch, GPU recommended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-6. Evaluation & Production\n",
    "\n",
    "### Same Metrics as Traditional\n",
    "- Accuracy, Precision, Recall, F1\n",
    "- Class imbalance handling\n",
    "\n",
    "### Additional LLM Considerations\n",
    "- **Prompt sensitivity**: Same model, different prompts → different results\n",
    "- **Calibration**: LLM confidence vs actual accuracy\n",
    "- **Consistency**: Same input, different runs → same output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Readiness Checklist\n",
    "\n",
    "```\n",
    "API-BASED (GPT-4, Claude)\n",
    "[ ] Rate limiting and retry logic\n",
    "[ ] Cost monitoring and alerts\n",
    "[ ] PII masking before API call\n",
    "[ ] Fallback to local model if API down\n",
    "[ ] Prompt versioning and A/B testing\n",
    "[ ] Response validation (is output a valid category?)\n",
    "\n",
    "SELF-HOSTED (Fine-tuned BERT)\n",
    "[ ] Model serving infrastructure (TorchServe, Triton)\n",
    "[ ] Batch processing for throughput\n",
    "[ ] Model versioning and rollback\n",
    "[ ] Latency monitoring (p50, p99)\n",
    "[ ] GPU utilization optimization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Traditional vs LLM Comparison\n",
    "\n",
    "| Dimension | Traditional (LR/SVM) | Fine-tuned BERT | Zero-shot | Few-shot GPT |\n",
    "|-----------|---------------------|-----------------|-----------|---------------|\n",
    "| **Accuracy** | 85-92% | 92-96% | 75-85% | 80-90% |\n",
    "| **Training data** | 1000+ | 100+ | 0 | 5-20 |\n",
    "| **Latency** | <10ms | 50-200ms | 200-500ms | 500ms-2s |\n",
    "| **Cost/prediction** | ~$0 | $0.0001 | $0.001 | $0.01-0.02 |\n",
    "| **New categories** | Retrain | Retrain | Update labels | Update prompt |\n",
    "| **Explainability** | High | Low | Medium | Medium |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Techniques\n",
    "\n",
    "### Chain-of-Thought Classification\n",
    "```python\n",
    "prompt = \"\"\"Classify this complaint step by step:\n",
    "1. What is the customer's main issue?\n",
    "2. What banking product is involved?\n",
    "3. What emotion is expressed?\n",
    "4. Based on the above, the category is:\"\"\"\n",
    "```\n",
    "\n",
    "### SetFit (Few-shot with sentence transformers)\n",
    "- 8-64 examples needed\n",
    "- Fine-tunes embedding model\n",
    "- No prompting, fast inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interview Soundbites\n",
    "\n",
    "**On Approach Selection:**\n",
    "> \"My decision tree: Have 1000+ labeled examples? Fine-tune BERT. Have 10-50 examples? Try SetFit. Zero examples but know the categories? Zero-shot. Need to add new categories dynamically? Few-shot prompting.\"\n",
    "\n",
    "**On Cost vs Accuracy:**\n",
    "> \"At 1M classifications/month, GPT-4 costs ~$15K. Fine-tuned BERT costs ~$100 in GPU time. For a 5% accuracy gain, is that worth 150x the cost? Usually not in banking where we have enough data to fine-tune.\"\n",
    "\n",
    "**On Zero-shot Limitations:**\n",
    "> \"Zero-shot sounds magical but it's really pattern matching on label names. 'Technical Support' works because BART learned what those words mean. If I use 'Category A' and 'Category B', it fails.\"\n",
    "\n",
    "**On Production:**\n",
    "> \"I never deploy raw LLM output to production. Always validate: Is the output one of my valid categories? If not, route to human review. LLMs can generate creative but invalid responses.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Q: When would you NOT use an LLM for classification?**\n",
    "> High volume (millions/day), need full explainability, have sufficient training data, or strict latency requirements (<50ms). Traditional models win on all these dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                    NOTEBOOK SUMMARY                               ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║  Task: Text Classification with LLMs                             ║\n",
    "║  Approaches: Fine-tuned BERT, Zero-shot, Few-shot                ║\n",
    "║  Banking Use: Cold start classification, dynamic categories      ║\n",
    "║                                                                  ║\n",
    "║  Key Takeaways:                                                  ║\n",
    "║  1. Zero-shot: No data needed, but lower accuracy                ║\n",
    "║  2. Few-shot: 5-20 examples, quick to deploy                     ║\n",
    "║  3. Fine-tuned: Best accuracy, needs data + GPU                  ║\n",
    "║  4. SetFit: Sweet spot (few examples, fast inference)            ║\n",
    "║  5. Always validate LLM output is a valid category               ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

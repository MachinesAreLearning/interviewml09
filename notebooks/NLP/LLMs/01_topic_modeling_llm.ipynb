{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with LLMs - Modern Approaches\n",
    "## Interview Preparation Notebook for Senior Applied AI Scientist (Retail Banking)\n",
    "\n",
    "---\n",
    "\n",
    "**Goal**: Demonstrate mastery of modern transformer-based topic discovery including BERTopic, zero-shot classification, and LLM-based topic extraction.\n",
    "\n",
    "**Interview Signal**: This notebook shows you understand when and how to leverage LLMs for topic discovery, with production considerations for banking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Context (Banking Lens)\n",
    "\n",
    "### Why LLMs for Topic Modeling Now?\n",
    "\n",
    "Traditional topic modeling (LDA) struggled with:\n",
    "- Short text (tweets, chat messages, complaints under 50 words)\n",
    "- Semantic understanding (\"send money\" ≠ \"transfer funds\" for LDA)\n",
    "- Dynamic topics (new issues emerge faster than retraining cycles)\n",
    "\n",
    "LLM-based approaches solve these by leveraging:\n",
    "- Pre-trained semantic understanding\n",
    "- Zero-shot capability for new topics\n",
    "- Better representation of short documents\n",
    "\n",
    "### Banking Use Cases for LLM Topic Modeling\n",
    "\n",
    "| Use Case | Why LLM > Traditional | Example |\n",
    "|----------|----------------------|----------|\n",
    "| **Chat Analysis** | Short messages need semantic understanding | \"app crashed\" = \"mobile banking issue\" |\n",
    "| **Social Media** | Slang, abbreviations, evolving language | \"this bank is trash\" = negative sentiment |\n",
    "| **Emerging Issue Detection** | Zero-shot finds new topics without retraining | Detect new fraud pattern mentions |\n",
    "| **Multi-lingual** | Single model handles multiple languages | Complaints in Spanish + English |\n",
    "\n",
    "### Cost/Benefit Analysis\n",
    "\n",
    "| Approach | Cost at 100K docs/month | Best For |\n",
    "|----------|------------------------|----------|\n",
    "| LDA | ~$0 | High volume, long docs |\n",
    "| BERTopic | ~$50 (GPU compute) | Medium volume, better quality |\n",
    "| Zero-shot (API) | ~$1,000-5,000 | Low volume, new topics |\n",
    "| GPT-4 extraction | ~$5,000-20,000 | Ad-hoc analysis, quality critical |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem Definition\n",
    "\n",
    "### LLM Approaches to Topic Discovery\n",
    "\n",
    "| Approach | Method | Supervision | Strengths |\n",
    "|----------|--------|-------------|----------|\n",
    "| **BERTopic** | Embedding → Clustering → Topic naming | Unsupervised | Scalable, interpretable |\n",
    "| **Zero-shot Classification** | Match docs to predefined topics | Semi-supervised | No training, flexible |\n",
    "| **LLM Extraction** | Prompt LLM to identify topics | Unsupervised | Most flexible, handles nuance |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset\n",
    "\n",
    "We'll use the same dataset as the traditional notebook for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install sentence-transformers umap-learn hdbscan bertopic transformers torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Load same dataset as traditional notebook\n",
    "categories = [\n",
    "    'comp.graphics', 'comp.sys.mac.hardware', \n",
    "    'rec.sport.baseball', 'rec.sport.hockey',\n",
    "    'sci.med', 'sci.space',\n",
    "    'talk.politics.guns', 'talk.religion.misc'\n",
    "]\n",
    "\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Sample for faster processing\n",
    "sample_size = 500\n",
    "indices = np.random.choice(len(newsgroups.data), sample_size, replace=False)\n",
    "documents = [newsgroups.data[i] for i in indices]\n",
    "true_labels = [newsgroups.target[i] for i in indices]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM Approach Selection\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "**BERTopic (Recommended for Most Cases)**:\n",
    "- Volume: 1K - 1M documents\n",
    "- Need interpretable topic words\n",
    "- Can run locally (privacy)\n",
    "\n",
    "**Zero-shot Classification**:\n",
    "- Have predefined topic categories\n",
    "- Need consistent categorization\n",
    "- Lower volume\n",
    "\n",
    "**LLM Extraction (GPT-4)**:\n",
    "- Ad-hoc analysis\n",
    "- Need nuanced understanding\n",
    "- Budget allows API costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementation\n",
    "\n",
    "### 5.1 BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTopic implementation (pseudocode - requires GPU)\n",
    "# Uncomment to run if you have the dependencies\n",
    "\n",
    "'''\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize with banking-friendly embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create BERTopic model\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    nr_topics=\"auto\",  # Let model determine\n",
    "    top_n_words=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "topics, probs = topic_model.fit_transform(documents)\n",
    "\n",
    "# View topics\n",
    "topic_model.get_topic_info()\n",
    "'''\n",
    "\n",
    "print(\"BERTopic pseudocode shown above.\")\n",
    "print(\"Requires: sentence-transformers, umap-learn, hdbscan, bertopic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated BERTopic-like workflow using available libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class SimplifiedBERTopic:\n",
    "    \"\"\"\n",
    "    Simplified BERTopic-like implementation.\n",
    "    \n",
    "    Real BERTopic uses:\n",
    "    1. Sentence-BERT embeddings (we use TF-IDF as proxy)\n",
    "    2. UMAP for dimensionality reduction\n",
    "    3. HDBSCAN for clustering\n",
    "    4. c-TF-IDF for topic representation\n",
    "    \n",
    "    This simplified version demonstrates the workflow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_topics=8):\n",
    "        self.n_topics = n_topics\n",
    "        self.vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "        self.clusterer = KMeans(n_clusters=n_topics, random_state=RANDOM_STATE)\n",
    "        self.pca = PCA(n_components=50, random_state=RANDOM_STATE)\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        # Step 1: Embed documents (TF-IDF as proxy for BERT embeddings)\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(documents)\n",
    "        \n",
    "        # Step 2: Reduce dimensions (PCA as proxy for UMAP)\n",
    "        reduced = self.pca.fit_transform(tfidf_matrix.toarray())\n",
    "        \n",
    "        # Step 3: Cluster (K-Means as proxy for HDBSCAN)\n",
    "        topics = self.clusterer.fit_predict(reduced)\n",
    "        \n",
    "        return topics\n",
    "    \n",
    "    def get_topic_words(self, documents, topics, top_n=10):\n",
    "        \"\"\"Extract representative words for each topic (c-TF-IDF style).\"\"\"\n",
    "        topic_words = {}\n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        \n",
    "        for topic_id in range(self.n_topics):\n",
    "            # Get documents in this topic\n",
    "            topic_docs = [documents[i] for i, t in enumerate(topics) if t == topic_id]\n",
    "            \n",
    "            if not topic_docs:\n",
    "                continue\n",
    "            \n",
    "            # Get TF-IDF for topic documents\n",
    "            topic_tfidf = self.vectorizer.transform(topic_docs)\n",
    "            \n",
    "            # Sum and normalize\n",
    "            word_scores = np.asarray(topic_tfidf.sum(axis=0)).flatten()\n",
    "            top_indices = word_scores.argsort()[-top_n:][::-1]\n",
    "            \n",
    "            topic_words[topic_id] = [feature_names[i] for i in top_indices]\n",
    "        \n",
    "        return topic_words\n",
    "\n",
    "# Run simplified BERTopic\n",
    "simplified_model = SimplifiedBERTopic(n_topics=8)\n",
    "topics = simplified_model.fit_transform(documents)\n",
    "topic_words = simplified_model.get_topic_words(documents, topics)\n",
    "\n",
    "print(\"SIMPLIFIED BERTOPIC RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "for topic_id, words in topic_words.items():\n",
    "    print(f\"\\nTopic {topic_id}: {', '.join(words[:8])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Zero-Shot Topic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot classification (pseudocode - requires transformers)\n",
    "'''\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Define candidate topics (banking example)\n",
    "banking_topics = [\n",
    "    \"Mobile app technical issues\",\n",
    "    \"Fee and charge complaints\",\n",
    "    \"Customer service quality\",\n",
    "    \"Account access problems\",\n",
    "    \"Fraud and security concerns\",\n",
    "    \"Product feature requests\"\n",
    "]\n",
    "\n",
    "# Classify documents\n",
    "for doc in documents[:5]:\n",
    "    result = classifier(doc[:500], banking_topics, multi_label=True)\n",
    "    print(f\"Top topic: {result['labels'][0]} ({result['scores'][0]:.2%})\")\n",
    "'''\n",
    "\n",
    "print(\"Zero-shot classification pseudocode shown above.\")\n",
    "print(\"Requires: transformers, torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated zero-shot using keyword matching\n",
    "class SimulatedZeroShot:\n",
    "    \"\"\"\n",
    "    Simulated zero-shot classification using keyword matching.\n",
    "    \n",
    "    Real zero-shot uses:\n",
    "    - Pre-trained NLI model (BART, DeBERTa)\n",
    "    - Semantic similarity between doc and label\n",
    "    \n",
    "    This demonstrates the interface.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Keywords associated with each topic\n",
    "        self.topic_keywords = {\n",
    "            \"Technology/Computing\": ['computer', 'software', 'hardware', 'system', 'graphics', 'mac', 'windows'],\n",
    "            \"Sports\": ['game', 'team', 'player', 'season', 'hockey', 'baseball', 'score'],\n",
    "            \"Science\": ['research', 'study', 'medical', 'space', 'disease', 'treatment', 'nasa'],\n",
    "            \"Politics/Society\": ['government', 'law', 'rights', 'policy', 'gun', 'religion', 'political']\n",
    "        }\n",
    "    \n",
    "    def classify(self, text, candidate_labels=None):\n",
    "        labels = candidate_labels or list(self.topic_keywords.keys())\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        scores = {}\n",
    "        for label in labels:\n",
    "            keywords = self.topic_keywords.get(label, [])\n",
    "            matches = sum(1 for kw in keywords if kw in text_lower)\n",
    "            scores[label] = matches / max(1, len(keywords))\n",
    "        \n",
    "        # Normalize\n",
    "        total = sum(scores.values()) + 0.001\n",
    "        scores = {k: v/total for k, v in scores.items()}\n",
    "        \n",
    "        sorted_labels = sorted(scores.items(), key=lambda x: -x[1])\n",
    "        \n",
    "        return {\n",
    "            'labels': [l for l, s in sorted_labels],\n",
    "            'scores': [s for l, s in sorted_labels]\n",
    "        }\n",
    "\n",
    "# Test simulated zero-shot\n",
    "zs_classifier = SimulatedZeroShot()\n",
    "\n",
    "print(\"SIMULATED ZERO-SHOT CLASSIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(3):\n",
    "    result = zs_classifier.classify(documents[i][:500])\n",
    "    print(f\"\\nDoc {i+1}: {documents[i][:100]}...\")\n",
    "    print(f\"Top topic: {result['labels'][0]} ({result['scores'][0]:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 LLM-Based Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_extraction_prompt(documents, n_topics=8):\n",
    "    \"\"\"\n",
    "    Create prompt for LLM topic extraction.\n",
    "    \n",
    "    This prompt asks the LLM to:\n",
    "    1. Read sample documents\n",
    "    2. Identify common themes\n",
    "    3. Name and describe each topic\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample documents for the prompt\n",
    "    sample_docs = \"\\n\\n---\\n\\n\".join([doc[:300] + \"...\" for doc in documents[:10]])\n",
    "    \n",
    "    prompt = f\"\"\"You are analyzing customer communications for a retail bank.\n",
    "\n",
    "Below are sample documents from a larger corpus. Identify the {n_topics} main topics \n",
    "present across these documents.\n",
    "\n",
    "For each topic, provide:\n",
    "1. A short name (2-4 words)\n",
    "2. A one-sentence description\n",
    "3. 5 keywords commonly associated with this topic\n",
    "\n",
    "Sample Documents:\n",
    "---\n",
    "{sample_docs}\n",
    "---\n",
    "\n",
    "Output in this exact format:\n",
    "\n",
    "Topic 1: [Name]\n",
    "Description: [One sentence]\n",
    "Keywords: [word1, word2, word3, word4, word5]\n",
    "\n",
    "Topic 2: [Name]\n",
    "...\n",
    "\n",
    "Topics:\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example prompt\n",
    "prompt = create_topic_extraction_prompt(documents)\n",
    "print(\"LLM TOPIC EXTRACTION PROMPT\")\n",
    "print(\"=\" * 50)\n",
    "print(prompt[:1500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Strategy\n",
    "\n",
    "### Comparing LLM vs Traditional\n",
    "\n",
    "| Metric | Traditional (LDA) | LLM (BERTopic) | Notes |\n",
    "|--------|------------------|----------------|-------|\n",
    "| **Coherence** | 0.4-0.5 | 0.5-0.7 | LLM typically higher |\n",
    "| **Human Interpretability** | Medium | High | LLM topics more readable |\n",
    "| **Short Text Performance** | Poor | Good | LLM's key advantage |\n",
    "| **Compute Cost** | Low | Medium-High | GPU for BERTopic |\n",
    "| **Latency** | Fast | Slower | Embedding generation time |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Readiness Checklist\n",
    "\n",
    "```\n",
    "MODEL SELECTION\n",
    "[ ] Choose embedding model (MiniLM for speed, MPNet for quality)\n",
    "[ ] Evaluate on domain-specific data\n",
    "[ ] Consider fine-tuning for banking vocabulary\n",
    "\n",
    "API vs LOCAL\n",
    "[ ] Data privacy assessment (can data leave premises?)\n",
    "[ ] Cost projection at production volume\n",
    "[ ] Latency requirements\n",
    "[ ] Fallback strategy if API unavailable\n",
    "\n",
    "SCALING\n",
    "[ ] Batch processing for large corpora\n",
    "[ ] Embedding caching\n",
    "[ ] Incremental updates (new docs)\n",
    "\n",
    "BANKING-SPECIFIC\n",
    "[ ] PII handling before embedding\n",
    "[ ] Audit trail for topic assignments\n",
    "[ ] Human validation of discovered topics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Traditional vs LLM Comparison\n",
    "\n",
    "| Dimension | Traditional (LDA) | LLM (BERTopic) | Winner |\n",
    "|-----------|------------------|----------------|--------|\n",
    "| **Short text (<50 words)** | Poor | Good | LLM |\n",
    "| **Semantic understanding** | None | Strong | LLM |\n",
    "| **Interpretability** | Word lists | Word lists + embeddings | Tie |\n",
    "| **Compute cost** | CPU only | GPU preferred | Traditional |\n",
    "| **No training needed** | Yes | Yes | Tie |\n",
    "| **Deterministic** | Yes (with seed) | Less so | Traditional |\n",
    "| **Compliance/Audit** | Easy | Harder | Traditional |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Techniques\n",
    "\n",
    "### Dynamic Topic Modeling\n",
    "Track how topics evolve over time:\n",
    "```python\n",
    "# BERTopic with timestamps\n",
    "topics_over_time = topic_model.topics_over_time(documents, timestamps)\n",
    "```\n",
    "\n",
    "### Guided Topic Modeling\n",
    "Seed with known topics:\n",
    "```python\n",
    "seed_topics = [[\"fraud\", \"scam\", \"unauthorized\"], [\"fee\", \"charge\", \"overdraft\"]]\n",
    "topic_model = BERTopic(seed_topic_list=seed_topics)\n",
    "```\n",
    "\n",
    "### Hierarchical Topics\n",
    "```python\n",
    "hierarchical_topics = topic_model.hierarchical_topics(documents)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interview Soundbites\n",
    "\n",
    "### LLM-Specific Talking Points\n",
    "\n",
    "**On BERTopic:**\n",
    "> \"BERTopic solves LDA's biggest weakness - short text. By using sentence embeddings, even a 10-word customer complaint gets a meaningful representation. The UMAP + HDBSCAN combination finds natural clusters without forcing a fixed number of topics.\"\n",
    "\n",
    "**On Zero-shot for Banking:**\n",
    "> \"Zero-shot classification is powerful for banking because we often know the categories we care about - fraud, fees, service complaints. We don't need training data, and when a new category emerges, we just add it to the label list.\"\n",
    "\n",
    "**On Cost Considerations:**\n",
    "> \"BERTopic is the sweet spot for production. It's 10-100x cheaper than GPT-4 for topic discovery, runs locally for data privacy, and produces interpretable results. I'd only use GPT-4 for ad-hoc analysis or when I need to explain topics in natural language.\"\n",
    "\n",
    "**On When Traditional Wins:**\n",
    "> \"LDA still wins when you need deterministic, auditable results. For regulatory reporting where the same input must always produce the same output, LDA's seeded randomness is easier to control than BERTopic's clustering.\"\n",
    "\n",
    "**On Hybrid Approach:**\n",
    "> \"In production, I'd use BERTopic for discovery - find what's actually in the data. Then create a fixed taxonomy and use zero-shot classification for ongoing categorization. This gives us the flexibility of discovery with the consistency of classification.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Common Interview Questions\n",
    "\n",
    "**Q: When would you use BERTopic over LDA?**\n",
    "> Short documents, need semantic understanding, or dealing with synonyms/paraphrases. BERTopic's embeddings capture meaning that LDA's bag-of-words misses.\n",
    "\n",
    "**Q: How do you handle the non-determinism of embedding-based methods?**\n",
    "> Set random seeds everywhere (embedding model, UMAP, HDBSCAN). For critical applications, run multiple times and take the consensus. Or train a classifier on discovered topics for deterministic inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                    NOTEBOOK SUMMARY                               ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║  Task: Topic Modeling with LLMs                                  ║\n",
    "║  Approaches: BERTopic, Zero-shot, LLM Extraction                 ║\n",
    "║  Banking Use: Short text analysis, emerging issue detection      ║\n",
    "║                                                                  ║\n",
    "║  Key Takeaways:                                                  ║\n",
    "║  1. BERTopic excels at short text (LDA's weakness)               ║\n",
    "║  2. Zero-shot for known categories without training              ║\n",
    "║  3. LLM extraction for ad-hoc, nuanced analysis                  ║\n",
    "║  4. Cost: BERTopic << Zero-shot API << GPT-4                     ║\n",
    "║  5. Traditional still wins on determinism/audit                  ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

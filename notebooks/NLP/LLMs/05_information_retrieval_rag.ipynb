{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval with RAG - Modern Approaches\n",
    "## Interview Preparation Notebook for Senior Applied AI Scientist (Retail Banking)\n",
    "\n",
    "---\n",
    "\n",
    "**Goal**: Demonstrate mastery of dense retrieval, hybrid search, and Retrieval-Augmented Generation (RAG) pipelines for knowledge-intensive banking applications.\n",
    "\n",
    "**Interview Signal**: This notebook shows you understand the full RAG stack from embedding models to reranking to generation, with production considerations for banking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Context (Banking Lens)\n",
    "\n",
    "### Why RAG Now?\n",
    "\n",
    "| Traditional IR Limitation | RAG Solution |\n",
    "|--------------------------|---------------|\n",
    "| Keyword mismatch (\"loan\" vs \"credit\") | Semantic understanding |\n",
    "| Returns documents, not answers | Synthesized responses |\n",
    "| Can't reason across documents | Multi-hop reasoning |\n",
    "| No conversational context | Maintains dialogue state |\n",
    "\n",
    "### Banking Use Cases\n",
    "\n",
    "1. **Policy Q&A**: \"What's our policy on foreign wire transfers over $10K?\"\n",
    "2. **Compliance Assistant**: Search regulations and explain applicability\n",
    "3. **Customer Service Bot**: Answer product questions from knowledge base\n",
    "4. **Internal Knowledge Search**: Find procedures across 10K+ internal documents\n",
    "5. **Research Assistant**: Query earnings calls, SEC filings, analyst reports\n",
    "\n",
    "### Why Not Just Use LLM Knowledge?\n",
    "\n",
    "**Banking Requires**:\n",
    "- **Recency**: LLM training data is stale; policies change weekly\n",
    "- **Accuracy**: Can't hallucinate compliance rules\n",
    "- **Auditability**: Must cite sources for regulators\n",
    "- **Proprietary data**: Internal docs not in LLM training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem Definition\n",
    "\n",
    "### RAG Architecture Overview\n",
    "\n",
    "```\n",
    "Query → [Retriever] → Top-K Documents → [Reranker] → Top-N Documents → [Generator] → Answer\n",
    "         ↑                                                              ↑\n",
    "    Embedding Model                                                  LLM + Context\n",
    "```\n",
    "\n",
    "### Retrieval Approaches\n",
    "\n",
    "| Approach | How It Works | Strengths | Weaknesses |\n",
    "|----------|-------------|-----------|------------|\n",
    "| **Sparse (BM25)** | Term frequency matching | Fast, interpretable | Keyword mismatch |\n",
    "| **Dense (DPR/SBERT)** | Semantic embedding similarity | Understands meaning | Misses exact matches |\n",
    "| **Hybrid** | Combine sparse + dense | Best of both | More complex |\n",
    "| **Learned Sparse (SPLADE)** | Neural term weighting | Interpretable + semantic | Newer, less tooling |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install sentence-transformers faiss-cpu numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sample banking knowledge base\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": \"policy_001\",\n",
    "        \"title\": \"Wire Transfer Policy\",\n",
    "        \"content\": \"International wire transfers exceeding $10,000 require additional verification. The customer must provide government-issued ID and state the purpose of the transfer. Transfers to high-risk countries require branch manager approval and a 24-hour hold period.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"policy_002\", \n",
    "        \"title\": \"Account Opening Requirements\",\n",
    "        \"content\": \"New account opening requires two forms of identification: one government-issued photo ID and one proof of address dated within 60 days. For business accounts, articles of incorporation and EIN documentation are also required.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"policy_003\",\n",
    "        \"title\": \"Overdraft Protection\",\n",
    "        \"content\": \"Overdraft protection links checking to savings account. When checking balance is insufficient, funds are automatically transferred from savings. A $12 transfer fee applies per occurrence. Daily transfer limit is $1,000.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"policy_004\",\n",
    "        \"title\": \"Fraud Reporting Procedures\",\n",
    "        \"content\": \"Suspected fraud must be reported within 60 days of statement date. Customer liability is limited to $50 if reported within 2 business days, up to $500 if reported within 60 days, and unlimited thereafter. Immediately freeze the affected account.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"policy_005\",\n",
    "        \"title\": \"Large Cash Transaction Reporting\",\n",
    "        \"content\": \"Cash transactions over $10,000 require Currency Transaction Report (CTR) filing. Structuring transactions to avoid reporting is illegal. Suspicious patterns below threshold trigger Suspicious Activity Report (SAR).\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"faq_001\",\n",
    "        \"title\": \"Interest Rate FAQ\",\n",
    "        \"content\": \"Savings account interest is calculated daily and paid monthly. Current rate is 4.5% APY for balances over $10,000 and 3.2% APY for balances under $10,000. Rates are variable and may change.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"faq_002\",\n",
    "        \"title\": \"Mobile Deposit Limits\",\n",
    "        \"content\": \"Mobile check deposit limits are $5,000 per check and $10,000 per day for standard accounts. Premium accounts have $10,000 per check and $25,000 daily limits. Funds availability is typically next business day.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"reg_001\",\n",
    "        \"title\": \"Regulation E - Electronic Transfers\",\n",
    "        \"content\": \"Under Regulation E, consumers must report unauthorized electronic transfers within 60 days. Banks must investigate within 10 business days and provisionally credit the account. Final resolution required within 45 days.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base size: {len(knowledge_base)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-4. Implementation\n",
    "\n",
    "### 4.1 Dense Retrieval with Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense retrieval pseudocode\n",
    "'''\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dim, fast\n",
    "# Or: 'BAAI/bge-base-en-v1.5' for better quality\n",
    "\n",
    "# Embed documents\n",
    "doc_texts = [d['title'] + \" \" + d['content'] for d in knowledge_base]\n",
    "doc_embeddings = model.encode(doc_texts, normalize_embeddings=True)\n",
    "\n",
    "# Build FAISS index\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# Query\n",
    "query = \"How do I report fraud on my account?\"\n",
    "query_embedding = model.encode([query], normalize_embeddings=True)\n",
    "\n",
    "# Search\n",
    "k = 3\n",
    "scores, indices = index.search(query_embedding, k)\n",
    "\n",
    "for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "    print(f\"{i+1}. [{score:.3f}] {knowledge_base[idx]['title']}\")\n",
    "'''\n",
    "\n",
    "print(\"Dense retrieval pseudocode shown above.\")\n",
    "print(\"Key models: all-MiniLM-L6-v2 (fast), BGE (quality), E5 (versatile)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated dense retrieval for demonstration\n",
    "def simulate_dense_retrieval(query, documents, top_k=3):\n",
    "    \"\"\"\n",
    "    Simulate semantic similarity with simple word overlap + synonyms.\n",
    "    In production, use actual embeddings!\n",
    "    \"\"\"\n",
    "    # Simple synonym expansion\n",
    "    synonyms = {\n",
    "        'fraud': ['unauthorized', 'stolen', 'suspicious'],\n",
    "        'wire': ['transfer', 'send', 'remittance'],\n",
    "        'account': ['checking', 'savings', 'deposit'],\n",
    "        'report': ['file', 'notify', 'alert'],\n",
    "        'money': ['funds', 'cash', 'balance'],\n",
    "        'interest': ['rate', 'apy', 'yield']\n",
    "    }\n",
    "    \n",
    "    query_words = set(query.lower().split())\n",
    "    expanded_query = query_words.copy()\n",
    "    for word in query_words:\n",
    "        if word in synonyms:\n",
    "            expanded_query.update(synonyms[word])\n",
    "    \n",
    "    scores = []\n",
    "    for doc in documents:\n",
    "        doc_text = (doc['title'] + ' ' + doc['content']).lower()\n",
    "        doc_words = set(doc_text.split())\n",
    "        \n",
    "        # Jaccard-like similarity with expansion\n",
    "        overlap = len(expanded_query & doc_words)\n",
    "        score = overlap / (len(expanded_query) + 0.1)\n",
    "        scores.append((score, doc))\n",
    "    \n",
    "    # Sort by score descending\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    return scores[:top_k]\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"How do I report fraud on my account?\",\n",
    "    \"What are the wire transfer limits for international payments?\",\n",
    "    \"What interest rate do I get on savings?\"\n",
    "]\n",
    "\n",
    "print(\"SIMULATED DENSE RETRIEVAL\")\n",
    "print(\"=\" * 60)\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    results = simulate_dense_retrieval(query, knowledge_base, top_k=2)\n",
    "    for score, doc in results:\n",
    "        print(f\"  [{score:.3f}] {doc['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hybrid Search (BM25 + Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_score(query, document, doc_lengths, avg_doc_length, k1=1.5, b=0.75):\n",
    "    \"\"\"Calculate BM25 score for a single document.\"\"\"\n",
    "    query_terms = query.lower().split()\n",
    "    doc_terms = document.lower().split()\n",
    "    doc_length = len(doc_terms)\n",
    "    term_freqs = Counter(doc_terms)\n",
    "    \n",
    "    score = 0\n",
    "    N = len(doc_lengths)\n",
    "    \n",
    "    for term in query_terms:\n",
    "        if term in term_freqs:\n",
    "            tf = term_freqs[term]\n",
    "            # Simplified IDF\n",
    "            df = sum(1 for d in doc_lengths if term in d.lower())\n",
    "            idf = math.log((N - df + 0.5) / (df + 0.5) + 1)\n",
    "            \n",
    "            # BM25 term score\n",
    "            numerator = tf * (k1 + 1)\n",
    "            denominator = tf + k1 * (1 - b + b * doc_length / avg_doc_length)\n",
    "            score += idf * numerator / denominator\n",
    "    \n",
    "    return score\n",
    "\n",
    "def hybrid_search(query, documents, alpha=0.5, top_k=3):\n",
    "    \"\"\"\n",
    "    Combine BM25 (sparse) and simulated dense scores.\n",
    "    alpha: weight for dense (1-alpha for sparse)\n",
    "    \"\"\"\n",
    "    # Prepare documents\n",
    "    doc_texts = [d['title'] + ' ' + d['content'] for d in documents]\n",
    "    avg_length = np.mean([len(t.split()) for t in doc_texts])\n",
    "    \n",
    "    # Get sparse scores (BM25)\n",
    "    sparse_scores = []\n",
    "    for doc, text in zip(documents, doc_texts):\n",
    "        score = bm25_score(query, text, doc_texts, avg_length)\n",
    "        sparse_scores.append(score)\n",
    "    \n",
    "    # Get dense scores (simulated)\n",
    "    dense_results = simulate_dense_retrieval(query, documents, top_k=len(documents))\n",
    "    dense_scores = {r[1]['id']: r[0] for r in dense_results}\n",
    "    \n",
    "    # Normalize scores to [0, 1]\n",
    "    sparse_max = max(sparse_scores) if max(sparse_scores) > 0 else 1\n",
    "    dense_max = max(dense_scores.values()) if max(dense_scores.values()) > 0 else 1\n",
    "    \n",
    "    # Combine\n",
    "    combined = []\n",
    "    for doc, sparse_score in zip(documents, sparse_scores):\n",
    "        norm_sparse = sparse_score / sparse_max\n",
    "        norm_dense = dense_scores.get(doc['id'], 0) / dense_max\n",
    "        hybrid_score = alpha * norm_dense + (1 - alpha) * norm_sparse\n",
    "        combined.append((hybrid_score, doc))\n",
    "    \n",
    "    combined.sort(key=lambda x: -x[0])\n",
    "    return combined[:top_k]\n",
    "\n",
    "# Test hybrid search\n",
    "print(\"HYBRID SEARCH (BM25 + Dense)\")\n",
    "print(\"=\" * 60)\n",
    "query = \"What are the rules for large cash deposits?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "for alpha in [0.0, 0.5, 1.0]:\n",
    "    print(f\"Alpha={alpha} ({'BM25 only' if alpha==0 else 'Dense only' if alpha==1 else 'Hybrid'}):\")\n",
    "    results = hybrid_search(query, knowledge_base, alpha=alpha, top_k=2)\n",
    "    for score, doc in results:\n",
    "        print(f\"  [{score:.3f}] {doc['title']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranking pseudocode\n",
    "'''\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load cross-encoder reranker\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "# Or: 'BAAI/bge-reranker-base' for better quality\n",
    "\n",
    "# Initial retrieval (fast, recall-focused)\n",
    "candidates = hybrid_search(query, documents, top_k=20)\n",
    "\n",
    "# Rerank (slow, precision-focused)\n",
    "pairs = [(query, doc['content']) for _, doc in candidates]\n",
    "rerank_scores = reranker.predict(pairs)\n",
    "\n",
    "# Sort by reranker score\n",
    "reranked = sorted(zip(rerank_scores, candidates), reverse=True)\n",
    "top_docs = [doc for _, (_, doc) in reranked[:5]]\n",
    "'''\n",
    "\n",
    "print(\"Reranking improves precision after initial recall-focused retrieval.\")\n",
    "print(\"\")\n",
    "print(\"Two-stage retrieval pattern:\")\n",
    "print(\"1. Retrieve top-100 with fast bi-encoder\")\n",
    "print(\"2. Rerank to top-5 with slow cross-encoder\")\n",
    "print(\"\")\n",
    "print(\"Cross-encoders are 10-100x slower but much more accurate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Full RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_prompt(query, retrieved_docs, max_context_length=2000):\n",
    "    \"\"\"\n",
    "    Create RAG prompt with retrieved context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build context from retrieved documents\n",
    "    context_parts = []\n",
    "    total_length = 0\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        doc_text = f\"[Document {i+1}: {doc['title']}]\\n{doc['content']}\"\n",
    "        if total_length + len(doc_text) < max_context_length:\n",
    "            context_parts.append(doc_text)\n",
    "            total_length += len(doc_text)\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = f\"\"\"You are a helpful banking assistant. Answer the customer's question using ONLY the information provided in the documents below.\n",
    "\n",
    "IMPORTANT RULES:\n",
    "1. Only use information from the provided documents\n",
    "2. If the documents don't contain the answer, say \"I don't have information about that in my knowledge base\"\n",
    "3. Cite the document title when providing information\n",
    "4. Be concise and direct\n",
    "5. For compliance questions, always recommend consulting with compliance team\n",
    "\n",
    "DOCUMENTS:\n",
    "{context}\n",
    "\n",
    "CUSTOMER QUESTION: {query}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example RAG pipeline\n",
    "query = \"I think someone made unauthorized transactions on my account. What should I do?\"\n",
    "\n",
    "# Step 1: Retrieve\n",
    "retrieved = hybrid_search(query, knowledge_base, alpha=0.5, top_k=3)\n",
    "retrieved_docs = [doc for _, doc in retrieved]\n",
    "\n",
    "# Step 2: Generate prompt\n",
    "prompt = create_rag_prompt(query, retrieved_docs)\n",
    "\n",
    "print(\"RAG PROMPT\")\n",
    "print(\"=\" * 60)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated RAG response\n",
    "def simulate_rag_response(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    Simulate what an LLM would generate based on retrieved docs.\n",
    "    In production, this calls the actual LLM API.\n",
    "    \"\"\"\n",
    "    # Find relevant doc\n",
    "    for doc in retrieved_docs:\n",
    "        if 'fraud' in doc['content'].lower() or 'unauthorized' in doc['content'].lower():\n",
    "            return f\"\"\"Based on our Fraud Reporting Procedures:\n",
    "\n",
    "1. **Report immediately**: You should report suspected fraud within 2 business days to limit your liability to $50.\n",
    "\n",
    "2. **Time limits**: \n",
    "   - Within 2 days: max $50 liability\n",
    "   - Within 60 days: max $500 liability\n",
    "   - After 60 days: unlimited liability\n",
    "\n",
    "3. **Next steps**: Contact us immediately to freeze the affected account.\n",
    "\n",
    "[Source: {doc['title']}]\n",
    "\n",
    "I recommend calling our fraud hotline at 1-800-XXX-XXXX right away.\"\"\"\n",
    "    \n",
    "    return \"I don't have specific information about that in my knowledge base. Please contact customer service.\"\n",
    "\n",
    "# Test\n",
    "print(\"SIMULATED RAG RESPONSE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: {query}\\n\")\n",
    "response = simulate_rag_response(query, retrieved_docs)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Advanced: Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_document(text, chunk_size=500, overlap=100):\n",
    "    \"\"\"\n",
    "    Split document into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text: Document text\n",
    "        chunk_size: Target characters per chunk\n",
    "        overlap: Characters to overlap between chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        # Try to break at sentence boundary\n",
    "        if end < len(text):\n",
    "            # Look for sentence end in last 100 chars\n",
    "            for i in range(min(100, end - start)):\n",
    "                if text[end - i] in '.!?':\n",
    "                    end = end - i + 1\n",
    "                    break\n",
    "        \n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append({\n",
    "                'text': chunk,\n",
    "                'start': start,\n",
    "                'end': end\n",
    "            })\n",
    "        \n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example: chunking a longer document\n",
    "long_doc = \"\"\"Wire Transfer Policy and Procedures\n",
    "\n",
    "Section 1: Domestic Transfers\n",
    "Domestic wire transfers can be initiated through online banking or at any branch. \n",
    "Standard processing time is same-day for requests before 4 PM ET. Fees are $25 for \n",
    "online transfers and $35 for branch-initiated transfers.\n",
    "\n",
    "Section 2: International Transfers  \n",
    "International wire transfers exceeding $10,000 require additional verification. The \n",
    "customer must provide government-issued ID and state the purpose of the transfer. \n",
    "Transfers to high-risk countries require branch manager approval and a 24-hour hold.\n",
    "\n",
    "Section 3: Compliance Requirements\n",
    "All wire transfers are subject to OFAC screening. Transfers flagged by our compliance \n",
    "system require manual review. Currency Transaction Reports (CTR) are filed for cash \n",
    "transactions exceeding $10,000.\"\"\"\n",
    "\n",
    "chunks = chunk_document(long_doc, chunk_size=300, overlap=50)\n",
    "\n",
    "print(\"CHUNKING EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original document: {len(long_doc)} characters\")\n",
    "print(f\"Number of chunks: {len(chunks)}\\n\")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1} [{chunk['start']}:{chunk['end']}]:\")\n",
    "    print(f\"  {chunk['text'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-6. Evaluation\n",
    "\n",
    "### Retrieval Metrics\n",
    "- **Recall@K**: % of relevant docs in top-K\n",
    "- **MRR**: Mean Reciprocal Rank\n",
    "- **NDCG**: Normalized Discounted Cumulative Gain\n",
    "\n",
    "### End-to-End RAG Metrics\n",
    "- **Answer Correctness**: Does answer match ground truth?\n",
    "- **Faithfulness**: Is answer grounded in retrieved docs?\n",
    "- **Relevance**: Are retrieved docs relevant to query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(retrieved_ids, relevant_ids, k=5):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval quality.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_ids: List of retrieved document IDs (ranked)\n",
    "        relevant_ids: Set of relevant document IDs\n",
    "        k: Cutoff for metrics\n",
    "    \"\"\"\n",
    "    retrieved_at_k = retrieved_ids[:k]\n",
    "    \n",
    "    # Recall@K\n",
    "    recall = len(set(retrieved_at_k) & relevant_ids) / len(relevant_ids) if relevant_ids else 0\n",
    "    \n",
    "    # Precision@K\n",
    "    precision = len(set(retrieved_at_k) & relevant_ids) / k\n",
    "    \n",
    "    # MRR\n",
    "    mrr = 0\n",
    "    for i, doc_id in enumerate(retrieved_ids):\n",
    "        if doc_id in relevant_ids:\n",
    "            mrr = 1 / (i + 1)\n",
    "            break\n",
    "    \n",
    "    # NDCG@K\n",
    "    dcg = sum(1 / np.log2(i + 2) for i, doc_id in enumerate(retrieved_at_k) if doc_id in relevant_ids)\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(min(len(relevant_ids), k)))\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'recall@k': recall,\n",
    "        'precision@k': precision,\n",
    "        'mrr': mrr,\n",
    "        'ndcg@k': ndcg\n",
    "    }\n",
    "\n",
    "# Example evaluation\n",
    "retrieved = ['policy_004', 'reg_001', 'policy_001', 'faq_001']  # Our retrieval\n",
    "relevant = {'policy_004', 'reg_001'}  # Ground truth for fraud query\n",
    "\n",
    "metrics = evaluate_retrieval(retrieved, relevant, k=3)\n",
    "\n",
    "print(\"RETRIEVAL EVALUATION\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Query: 'How do I report fraud?'\")\n",
    "print(f\"Retrieved: {retrieved[:3]}\")\n",
    "print(f\"Relevant: {relevant}\")\n",
    "print(f\"\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_faithfulness(answer, source_docs):\n",
    "    \"\"\"\n",
    "    Check if claims in answer are grounded in source documents.\n",
    "    Simplified version - in production use NLI models.\n",
    "    \"\"\"\n",
    "    # Combine source content\n",
    "    source_text = ' '.join(d['content'].lower() for d in source_docs)\n",
    "    \n",
    "    # Extract numbers from answer\n",
    "    import re\n",
    "    answer_numbers = re.findall(r'\\$[\\d,]+|\\d+(?:\\.\\d+)?%?|\\d+ (?:days?|hours?)', answer.lower())\n",
    "    \n",
    "    # Check if numbers appear in source\n",
    "    grounded_numbers = []\n",
    "    hallucinated_numbers = []\n",
    "    \n",
    "    for num in answer_numbers:\n",
    "        if num in source_text or num.replace('$', '') in source_text:\n",
    "            grounded_numbers.append(num)\n",
    "        else:\n",
    "            hallucinated_numbers.append(num)\n",
    "    \n",
    "    return {\n",
    "        'grounded': grounded_numbers,\n",
    "        'potentially_hallucinated': hallucinated_numbers,\n",
    "        'faithfulness_score': len(grounded_numbers) / (len(answer_numbers) + 0.001)\n",
    "    }\n",
    "\n",
    "# Test\n",
    "answer = \"\"\"Report fraud within 2 business days to limit liability to $50. \n",
    "After 60 days, you could face $500 or unlimited liability.\"\"\"\n",
    "\n",
    "source = [knowledge_base[3]]  # Fraud policy\n",
    "\n",
    "result = check_faithfulness(answer, source)\n",
    "print(\"FAITHFULNESS CHECK\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Grounded claims: {result['grounded']}\")\n",
    "print(f\"Potentially hallucinated: {result['potentially_hallucinated']}\")\n",
    "print(f\"Faithfulness score: {result['faithfulness_score']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Readiness Checklist\n",
    "\n",
    "```\n",
    "INDEXING PIPELINE\n",
    "[ ] Document chunking strategy (size, overlap, semantic boundaries)\n",
    "[ ] Metadata extraction (titles, dates, categories)\n",
    "[ ] Incremental index updates (not full rebuild)\n",
    "[ ] Version control for document corpus\n",
    "[ ] PII masking before embedding\n",
    "\n",
    "RETRIEVAL\n",
    "[ ] Hybrid search (sparse + dense)\n",
    "[ ] Reranking for precision\n",
    "[ ] Query expansion/rewriting\n",
    "[ ] Relevance threshold (don't return irrelevant docs)\n",
    "[ ] Latency monitoring (p50, p99)\n",
    "\n",
    "GENERATION\n",
    "[ ] Prompt versioning\n",
    "[ ] Output validation (format, length)\n",
    "[ ] Faithfulness checking\n",
    "[ ] Source citation in response\n",
    "[ ] Fallback for low-confidence answers\n",
    "\n",
    "BANKING-SPECIFIC\n",
    "[ ] Compliance review of knowledge base\n",
    "[ ] Audit trail for queries and responses\n",
    "[ ] Human escalation path\n",
    "[ ] Disclaimer on AI-generated content\n",
    "[ ] Regular accuracy audits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Traditional vs RAG Comparison\n",
    "\n",
    "| Dimension | BM25 | Dense Retrieval | Hybrid + RAG |\n",
    "|-----------|------|-----------------|---------------|\n",
    "| **Semantic Understanding** | None | High | High |\n",
    "| **Exact Match** | Excellent | Poor | Good |\n",
    "| **Answer Synthesis** | None (returns docs) | None | Yes |\n",
    "| **Latency** | <50ms | 100-200ms | 500ms-2s |\n",
    "| **Cost/query** | ~$0 | $0.001 | $0.01-0.05 |\n",
    "| **Explainability** | High | Medium | Medium |\n",
    "| **Hallucination Risk** | None | None | Medium |\n",
    "| **Setup Complexity** | Low | Medium | High |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Techniques\n",
    "\n",
    "### Query Expansion\n",
    "```python\n",
    "# Use LLM to expand query\n",
    "prompt = \"\"\"Generate 3 alternative phrasings for this search query:\n",
    "Query: \"wire transfer limit\"\n",
    "Alternatives:\n",
    "1. maximum wire transfer amount\n",
    "2. how much can I wire transfer\n",
    "3. wire transaction threshold\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### Multi-hop Retrieval\n",
    "```python\n",
    "# For complex questions requiring multiple docs\n",
    "# Step 1: Retrieve docs for sub-question 1\n",
    "# Step 2: Use findings to formulate sub-question 2\n",
    "# Step 3: Retrieve more docs\n",
    "# Step 4: Synthesize across all retrieved docs\n",
    "```\n",
    "\n",
    "### ColBERT (Late Interaction)\n",
    "```python\n",
    "# Token-level matching instead of document-level\n",
    "# Better accuracy than bi-encoder, faster than cross-encoder\n",
    "# Good for production reranking\n",
    "```\n",
    "\n",
    "### Hypothetical Document Embedding (HyDE)\n",
    "```python\n",
    "# Generate hypothetical answer first, then retrieve\n",
    "query = \"What is the wire transfer limit?\"\n",
    "hypothetical = LLM(\"Answer this banking question: \" + query)\n",
    "# Embed hypothetical answer and search with it\n",
    "results = retrieve(embed(hypothetical))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interview Soundbites\n",
    "\n",
    "**On Hybrid Search:**\n",
    "> \"I always use hybrid search in production - BM25 for exact matches (policy numbers, account codes) and dense for semantic (customer paraphrasing). The combination is strictly better than either alone, typically 10-15% improvement in recall.\"\n",
    "\n",
    "**On Chunking:**\n",
    "> \"Chunking is the most underrated part of RAG. I chunk at semantic boundaries (section headers, paragraphs), not fixed character counts. And I always preserve metadata - a chunk without its document title loses context.\"\n",
    "\n",
    "**On Reranking:**\n",
    "> \"Two-stage retrieval is standard now: retrieve top-100 with fast bi-encoder, rerank to top-5 with cross-encoder. The cross-encoder is 100x slower but catches the semantic matches that bi-encoders miss.\"\n",
    "\n",
    "**On Faithfulness:**\n",
    "> \"For banking RAG, faithfulness isn't optional. I run NLI-based fact checking on every response, verify numbers match source documents, and always include citations. A hallucinated compliance answer is worse than no answer.\"\n",
    "\n",
    "**On Evaluation:**\n",
    "> \"I evaluate retrieval and generation separately. You can have perfect retrieval and terrible generation, or vice versa. MRR for retrieval, faithfulness scores for generation, and end-to-end answer correctness for the full pipeline.\"\n",
    "\n",
    "**On When NOT to Use RAG:**\n",
    "> \"RAG adds latency and complexity. If the answer is always in one document type with consistent structure, just retrieve the document - don't generate. RAG shines when you need to synthesize across documents or explain in natural language.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Q: How do you handle documents that update frequently?**\n",
    "> \"Incremental indexing, not full rebuilds. I track document hashes and only re-embed changed docs. For time-sensitive content like policies, I add effective dates to metadata and filter at query time. Old versions stay indexed for audit purposes.\"\n",
    "\n",
    "**Q: How do you prevent hallucination in RAG?**\n",
    "> \"Three defenses: (1) Strong retrieval - if the right docs aren't retrieved, hallucination is inevitable. (2) Grounding instructions in the prompt - 'only use information from the documents'. (3) Post-generation validation - check that claims can be traced to sources. If validation fails, return 'I don't have that information' instead.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║                    NOTEBOOK SUMMARY                               ║\n",
    "╠══════════════════════════════════════════════════════════════════╣\n",
    "║  Task: Information Retrieval with RAG                            ║\n",
    "║  Approaches: Dense retrieval, Hybrid search, Full RAG            ║\n",
    "║  Banking Use: Policy Q&A, compliance search, knowledge base      ║\n",
    "║                                                                  ║\n",
    "║  Key Takeaways:                                                  ║\n",
    "║  1. Hybrid search (BM25 + dense) beats either alone              ║\n",
    "║  2. Reranking with cross-encoder improves precision              ║\n",
    "║  3. Chunking strategy matters more than embedding model          ║\n",
    "║  4. Faithfulness checking is mandatory for banking               ║\n",
    "║  5. Always cite sources in generated responses                   ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
